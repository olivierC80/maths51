<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cours - Probabilités sur un Univers Fini</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"  crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"  crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"  crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <style>
        body { font-family: sans-serif; line-height: 1.6; padding: 20px; }
        h1, h2, h3 { color: #0056b3; }
        .definition, .theorem, .proposition, .proof, .example, .corollary, .remark {
            margin: 15px 0;
            padding: 15px;
            border-left: 4px solid;
        }
        .definition { border-color: #17a2b8; background-color: #e1f5fe; }
        .theorem { border-color: #28a745; background-color: #e8f5e9; }
        .proposition { border-color: #ffc107; background-color: #fff8e1; }
        .proof { border-color: #6c757d; background-color: #f8f9fa; font-style: italic; }
        .example { border-color: #fd7e14; background-color: #fff3e0; }
        .corollary { border-color: #007bff; background-color: #e7f3ff; }
        .remark { border-color: #adb5bd; background-color: #e9ecef; }
        strong { color: #0056b3; }
        code { background-color: #eee; padding: 2px 4px; border-radius: 3px; }
        .katex-display { overflow-x: auto; overflow-y: hidden; } /* Allow scrolling for wide formulas */
        .video-link { font-style: italic; color: #6c757d; margin-left: 10px; }
    </style>
</head>
<body>

<h1>Probabilités sur un Univers Fini</h1>

<h2>I. Expérience aléatoire - Événement</h2>

<div class="definition">
    <strong>Expérience aléatoire, Issues, Univers</strong><br>
    <ul>
        <li>Une <strong>expérience aléatoire</strong> est une expérience dont le résultat ne peut pas être prédit avec certitude, même si elle est répétée dans des conditions identiques.</li>
        <li>Les résultats possibles d'une expérience aléatoire sont appelés les <strong>issues</strong> ou résultats élémentaires.</li>
        <li>L'ensemble de toutes les issues possibles est appelé l'<strong>univers</strong> de l'expérience aléatoire, généralement noté $\Omega$.</li>
    </ul>
    Dans tout ce cours, nous supposerons que l'univers $\Omega$ est <strong>fini</strong>.
</div>

<div class="example">
    <ul>
        <li>Lancer d'un dé à 6 faces : $\Omega = \{1, 2, 3, 4, 5, 6\}$.</li>
        <li>Tirage d'une carte dans un jeu de 32 cartes : $\Omega = \{ \text{7 de Cœur}, \dots, \text{As de Pique} \}$. $|\Omega| = 32$.</li>
        <li>Résultat d'un match de foot pour une équipe : $\Omega = \{ \text{Gagné, Nul, Perdu} \}$.</li>
    </ul>
</div>

<div class="definition">
    <strong>Événement</strong><br>
    <ul>
        <li>Un <strong>événement</strong> est une partie (un sous-ensemble) de l'univers $\Omega$. Un événement est réalisé si l'issue de l'expérience appartient à cette partie.</li>
        <li>L'ensemble vide $\emptyset$ est appelé l'<strong>événement impossible</strong>.</li>
        <li>L'univers $\Omega$ est appelé l'<strong>événement certain</strong>.</li>
        <li>Un événement contenant une seule issue $\{\omega\}$ est appelé un <strong>événement élémentaire</strong>.</li>
    </ul>
</div>

<div class="definition">
    <strong>Opérations sur les événements</strong><br>
    Soient $A$ et $B$ deux événements (parties de $\Omega$).
    <ul>
        <li><strong>Réunion ($A \cup B$) :</strong> L'événement "$A$ ou $B$". Il est réalisé si $A$ est réalisé OU si $B$ est réalisé (ou les deux).</li>
        <li><strong>Intersection ($A \cap B$) :</strong> L'événement "$A$ et $B$". Il est réalisé si $A$ est réalisé ET si $B$ est réalisé.</li>
        <li><strong>Complémentaire ($\bar{A}$ ou $A^c$) :</strong> L'événement contraire de $A$. Il est réalisé si $A$ n'est pas réalisé. $\bar{A} = \Omega \setminus A$.</li>
        <li><strong>Incompatibilité :</strong> $A$ et $B$ sont dits <strong>incompatibles</strong> si leur réalisation simultanée est impossible, c'est-à-dire si $A \cap B = \emptyset$.</li>
    </ul>
    Le langage des événements se traduit directement en théorie des ensembles.
</div>

<div class="definition">
    <strong>Système complet d'événements</strong><br>
    Une famille finie d'événements $(A_1, \dots, A_n)$ forme un <strong>système complet d'événements</strong> (ou une partition de $\Omega$) si :
    <ol>
        <li>Ils sont deux à deux incompatibles : $\forall i \neq j, A_i \cap A_j = \emptyset$.</li>
        <li>Leur réunion est l'univers : $A_1 \cup A_2 \cup \dots \cup A_n = \bigcup_{i=1}^n A_i = \Omega$.</li>
    </ol>
    Autrement dit, lors de toute issue de l'expérience, un et un seul des événements $A_i$ est réalisé.
</div>

<div class="example">
    Lancer d'un dé : $\Omega = \{1, 2, 3, 4, 5, 6\}$.
    <ul>
        <li>$A = \{2, 4, 6\}$ : "obtenir un nombre pair".</li>
        <li>$B = \{1, 2, 3\}$ : "obtenir un nombre inférieur ou égal à 3".</li>
        <li>$A \cup B = \{1, 2, 3, 4, 6\}$.</li>
        <li>$A \cap B = \{2\}$.</li>
        <li>$\bar{A} = \{1, 3, 5\}$.</li>
        <li>$C = \{1, 3, 5\} = \bar{A}$. $A$ et $C$ sont incompatibles et $A \cup C = \Omega$. $(A, C)$ est un système complet d'événements.</li>
        <li>$D = \{1\}$, $E = \{2, 3\}$, $F = \{4, 5, 6\}$. $(D, E, F)$ est un système complet d'événements.</li>
    </ul>
</div>

<h2>II. Espace probabilisé fini</h2>

<div class="definition">
    <strong>Probabilité</strong><br>
    On appelle <strong>probabilité</strong> sur l'univers fini $\Omega$ toute application $P: \mathcal{P}(\Omega) \to [0, 1]$ (où $\mathcal{P}(\Omega)$ est l'ensemble des parties de $\Omega$) vérifiant les deux axiomes suivants :
    <ol>
        <li>$P(\Omega) = 1$.</li>
        <li><strong>Additivité (finie) :</strong> Pour toute famille $(A_1, \dots, A_p)$ d'événements deux à deux incompatibles,
           $$ P(A_1 \cup \dots \cup A_p) = \sum_{i=1}^p P(A_i) $$
           (En particulier, pour $A, B$ disjoints, $P(A \cup B) = P(A) + P(B)$).
        </li>
    </ol>
    Le couple $(\Omega, P)$ est appelé un <strong>espace probabilisé fini</strong>. $P(A)$ est la probabilité de l'événement $A$.
</div>

<div class="proposition">
    <strong>Propriétés des probabilités</strong><br>
    Soit $(\Omega, P)$ un espace probabilisé fini.
    <ul>
        <li>$P(\emptyset) = 0$.</li>
        <li>Pour tout événement $A$, $P(\bar{A}) = 1 - P(A)$.</li>
        <li>Pour tous événements $A, B$, si $A \subset B$, alors $P(A) \le P(B)$. (Croissance de la probabilité).</li>
        <li>Pour tous événements $A, B$, $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.
            <span class="video-link">(Démonstration de cette formule)</span>
        </li>
        <li>Pour tout système complet d'événements $(A_1, \dots, A_n)$, $\sum_{i=1}^n P(A_i) = 1$.</li>
    </ul>
</div>

<div class="proof">
    <strong>Démonstration de $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ :</strong>
    On peut décomposer $A \cup B$ en trois événements deux à deux incompatibles :
    $A \cup B = (A \setminus B) \cup (B \setminus A) \cup (A \cap B)$.
    Donc $P(A \cup B) = P(A \setminus B) + P(B \setminus A) + P(A \cap B)$. (1)
    D'autre part, $A = (A \setminus B) \cup (A \cap B)$, qui sont disjoints. Donc $P(A) = P(A \setminus B) + P(A \cap B)$, soit $P(A \setminus B) = P(A) - P(A \cap B)$. (2)
    De même, $B = (B \setminus A) \cup (A \cap B)$, qui sont disjoints. Donc $P(B) = P(B \setminus A) + P(A \cap B)$, soit $P(B \setminus A) = P(B) - P(A \cap B)$. (3)
    En remplaçant (2) et (3) dans (1) :
    $P(A \cup B) = (P(A) - P(A \cap B)) + (P(B) - P(A \cap B)) + P(A \cap B)$
    $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.
</div>

<div class="definition">
    <strong>Distribution de probabilité</strong><br>
    Une <strong>distribution de probabilité</strong> sur $\Omega = \{\omega_1, \dots, \omega_n\}$ est une famille de réels positifs ou nuls $(p_1, \dots, p_n)$ telle que $\sum_{i=1}^n p_i = 1$.
    On associe $p_i$ à l'issue $\omega_i$.
</div>

<div class="proposition">
    <strong>Lien entre probabilité et distribution</strong><br>
    Une application $P: \mathcal{P}(\Omega) \to [0, 1]$ est une probabilité sur $\Omega$ si et seulement si la famille $(P(\{\omega\}))_{\omega \in \Omega}$ est une distribution de probabilité sur $\Omega$.
    Dans ce cas, la probabilité d'un événement quelconque $A \subset \Omega$ est la somme des probabilités des événements élémentaires qui le composent :
    $$ P(A) = \sum_{\omega \in A} P(\{\omega\}) $$
    Ainsi, une probabilité sur un univers fini est entièrement déterminée par la donnée des probabilités de chaque issue.
</div>

<div class="definition">
    <strong>Probabilité uniforme (Équiprobabilité)</strong><br>
    Lorsque toutes les issues sont supposées avoir la même probabilité (situation d'équiprobabilité), on parle de <strong>probabilité uniforme</strong> sur $\Omega$.
    Si $\text{card}(\Omega) = n$, alors pour chaque issue $\omega \in \Omega$, $P(\{\omega\}) = \frac{1}{n}$.
    Pour tout événement $A \subset \Omega$, la probabilité uniforme est donnée par :
    $$ P(A) = \frac{\text{Nombre d'issues dans A}}{\text{Nombre total d'issues}} = \frac{\text{card}(A)}{\text{card}(\Omega)} $$
    Dans ce cas, calculer une probabilité revient à un problème de dénombrement.
</div>

<div class="example">
    On lance un dé équilibré. $\Omega=\{1, 2, 3, 4, 5, 6\}$. $|\Omega|=6$. On est en situation d'équiprobabilité. $P(\{i\}) = 1/6$ pour tout $i$.
    Soit $A = \{2, 4, 6\}$ l'événement "obtenir un nombre pair". $|A|=3$.
    $P(A) = \frac{|A|}{|\Omega|} = \frac{3}{6} = \frac{1}{2}$.
</div>

<h2>III. Indépendance</h2>

<div class="remark">
    $(\Omega, P)$ désigne un espace probabilisé fini. L'indépendance traduit l'absence d'influence d'un événement sur l'autre.
</div>

<div class="definition">
    <strong>Indépendance de deux événements</strong><br>
    Deux événements $A$ et $B$ sont dits <strong>indépendants</strong> (pour la probabilité $P$) si :
    $$ P(A \cap B) = P(A) P(B) $$
</div>

<div class="definition">
    <strong>Indépendance mutuelle</strong><br>
    Des événements $A_1, \dots, A_n$ sont dits <strong>mutuellement indépendants</strong> si, pour toute sous-famille d'indices $1 \le i_1 < i_2 < \dots < i_k \le n$ (avec $k \in \{1, \dots, n\}$), on a :
    $$ P(A_{i_1} \cap A_{i_2} \cap \dots \cap A_{i_k}) = P(A_{i_1}) P(A_{i_2}) \dots P(A_{i_k}) $$
    Attention : Il ne suffit pas que les événements soient indépendants deux à deux. L'indépendance mutuelle est une condition plus forte.
</div>

<div class="example">
    On lance deux dés équilibrés. $\Omega = \{(i, j) \mid 1 \le i, j \le 6\}$, $|\Omega|=36$. Probabilité uniforme $P(\{(i,j)\}) = 1/36$.
    Soit $A$: "le premier dé donne 1". $A = \{(1,j) \mid j=1..6\}$. $|A|=6$. $P(A)=6/36=1/6$.
    Soit $B$: "le second dé donne 1". $B = \{(i,1) \mid i=1..6\}$. $|B|=6$. $P(B)=6/36=1/6$.
    Soit $C$: "la somme des dés vaut 7". $C = \{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}$. $|C|=6$. $P(C)=6/36=1/6$.
    - $A \cap B = \{(1, 1)\}$. $P(A \cap B) = 1/36$. $P(A)P(B) = (1/6)(1/6) = 1/36$. Donc $A$ et $B$ sont indépendants (logique, les lancers sont indépendants).
    - $A \cap C = \{(1, 6)\}$. $P(A \cap C) = 1/36$. $P(A)P(C) = (1/6)(1/6) = 1/36$. Donc $A$ et $C$ sont indépendants.
    - $B \cap C = \{(6, 1)\}$. $P(B \cap C) = 1/36$. $P(B)P(C) = (1/6)(1/6) = 1/36$. Donc $B$ et $C$ sont indépendants.
    - Les événements $A, B, C$ sont-ils mutuellement indépendants ? Il faut vérifier $P(A \cap B \cap C) = P(A)P(B)P(C)$.
      $A \cap B \cap C = \emptyset$ (on ne peut pas avoir premier dé=1, second dé=1 et somme=7). Donc $P(A \cap B \cap C) = 0$.
      Mais $P(A)P(B)P(C) = (1/6)^3 = 1/216 \neq 0$.
      Donc $A, B, C$ ne sont pas mutuellement indépendants, bien qu'ils soient indépendants deux à deux.
</div>

<div class="proposition">
    <strong>Indépendance et passage au complémentaire</strong><br>
    Si les événements $A_1, \dots, A_n$ sont mutuellement indépendants, alors en remplaçant n'importe lequel d'entre eux par son complémentaire, la nouvelle famille reste mutuellement indépendante.
    Par exemple, si $A$ et $B$ sont indépendants, alors $A$ et $\bar{B}$ sont indépendants, $\bar{A}$ et $B$ sont indépendants, et $\bar{A}$ et $\bar{B}$ sont indépendants.
</div>

<div class="remark">
    <strong>Indépendance vs Incompatibilité</strong><br>
    Il ne faut pas confondre "indépendant" et "incompatible".
    - Incompatible : $A \cap B = \emptyset$. Si $P(A)>0$ et $P(B)>0$, alors $P(A \cap B) = 0$, tandis que $P(A)P(B) > 0$. Donc $A$ et $B$ ne peuvent pas être indépendants (sauf si l'un d'eux est de probabilité nulle). Des événements incompatibles non impossibles sont donc dépendants.
    - Indépendant : $P(A \cap B) = P(A)P(B)$. L'information "B est réalisé" ne change pas la probabilité de A (voir section suivante).
    <span class="video-link">(Événements indépendants ou événements incompatibles ?)</span>
</div>

<h2>IV. Probabilités conditionnelles</h2>

<div class="remark">
    $(\Omega, P)$ désigne un espace probabilisé fini.
</div>

<div class="definition">
    <strong>Probabilité conditionnelle</strong><br>
    Soient $A$ et $B$ deux événements, avec $P(B) > 0$. La <strong>probabilité conditionnelle de $A$ sachant $B$</strong> est le réel noté $P(A|B)$ ou $P_B(A)$ défini par :
    $$ P(A|B) = \frac{P(A \cap B)}{P(B)} $$
    Cela représente la probabilité que l'événement $A$ se réalise, sachant que l'événement $B$ est réalisé. Intuitivement, on restreint l'univers à $B$.
</div>

<div class="proposition">
    <strong>$P_B$ est une probabilité</strong><br>
    Si $B$ est un événement tel que $P(B) > 0$, alors l'application $A \mapsto P(A|B)$ est une nouvelle probabilité sur l'univers $\Omega$ (et aussi sur l'univers restreint $B$).
</div>

<div class="remark">
    <strong>Indépendance et probabilité conditionnelle :</strong>
    Si $P(B)>0$, $A$ et $B$ sont indépendants si et seulement si $P(A|B) = P(A)$.
    Savoir que $B$ est réalisé ne modifie pas la probabilité de $A$.
</div>

<div class="theorem">
    <strong>Formule des probabilités composées</strong><br>
    Soient $A_1, \dots, A_m$ des événements. Si $P(A_1 \cap \dots \cap A_{m-1}) > 0$, alors :
    $$ P(A_1 \cap \dots \cap A_m) = P(A_1) \times P(A_2|A_1) \times P(A_3|A_1 \cap A_2) \times \dots \times P(A_m|A_1 \cap \dots \cap A_{m-1}) $$
    Pour $m=2$, $P(A \cap B) = P(A) P(B|A)$ (si $P(A)>0$).
    <span class="video-link">(Présentation et démonstration de la formule)</span>
</div>

<div class="proof">
    <strong>Idée de la preuve :</strong> La formule se démontre par récurrence ou en écrivant le produit télescopique :
    $P(A_1) \frac{P(A_1 \cap A_2)}{P(A_1)} \frac{P(A_1 \cap A_2 \cap A_3)}{P(A_1 \cap A_2)} \dots \frac{P(A_1 \cap \dots \cap A_m)}{P(A_1 \cap \dots \cap A_{m-1})}$.
    Après simplification, il reste $P(A_1 \cap \dots \cap A_m)$.
</div>

<div class="theorem">
    <strong>Formule des probabilités totales</strong><br>
    Soit $(A_1, \dots, A_n)$ un système complet d'événements tel que $P(A_i) > 0$ for all $i$. Soit $B$ un événement quelconque. Alors :
    $$ P(B) = \sum_{i=1}^n P(B \cap A_i) = \sum_{i=1}^n P(B|A_i) P(A_i) $$
    Cette formule permet de calculer la probabilité d'un événement $B$ en le décomposant selon les différents cas $(A_i)$ qui peuvent se produire.
    <span class="video-link">(Démonstration et exemple d'application)</span>
</div>

<div class="proof">
    <strong>Preuve :</strong> Puisque $(A_1, \dots, A_n)$ est un système complet, les événements $(B \cap A_i)$ forment une partition de $B$ : ils sont deux à deux disjoints et leur réunion est $B$.
    $B = B \cap \Omega = B \cap (\cup_{i=1}^n A_i) = \cup_{i=1}^n (B \cap A_i)$.
    Les $(B \cap A_i)$ sont disjoints car les $A_i$ le sont.
    Par additivité de $P$ : $P(B) = P(\cup_{i=1}^n (B \cap A_i)) = \sum_{i=1}^n P(B \cap A_i)$.
    En utilisant la définition de la probabilité conditionnelle, $P(B \cap A_i) = P(B|A_i) P(A_i)$ (puisque $P(A_i)>0$).
    D'où $P(B) = \sum_{i=1}^n P(B|A_i) P(A_i)$.
</div>

<div class="theorem">
    <strong>Formule de Bayes (pour deux événements)</strong><br>
    Soient $A$ et $B$ deux événements de probabilités non nulles ($P(A)>0, P(B)>0$). Alors :
    $$ P(A|B) = \frac{P(B|A) P(A)}{P(B)} $$
    Cette formule permet d'"inverser" le conditionnement : calculer $P(A|B)$ à partir de $P(B|A)$.
    <span class="video-link">(A quoi sert la formule de Bayes ?)</span>
</div>

<div class="proof">
    <strong>Preuve :</strong> Par définition, $P(A|B) = P(A \cap B) / P(B)$.
    Et $P(B|A) = P(B \cap A) / P(A)$, donc $P(A \cap B) = P(B \cap A) = P(B|A)P(A)$.
    En substituant $P(A \cap B)$ dans la première équation, on obtient la formule.
</div>

<div class="theorem">
    <strong>Formule de Bayes (généralisée)</strong><br>
    Soit $(A_1, \dots, A_n)$ un système complet d'événements tel que $P(A_i) > 0$ pour tout $i$. Soit $B$ un événement tel que $P(B) > 0$. Alors, pour tout $j \in \{1, \dots, n\}$ :
    $$ P(A_j|B) = \frac{P(B|A_j) P(A_j)}{P(B)} = \frac{P(B|A_j) P(A_j)}{\sum_{i=1}^n P(B|A_i) P(A_i)} $$
    Cette formule permet de calculer la probabilité d'une "cause" $A_j$ sachant l'"effet" $B$. Le dénominateur est simplement $P(B)$ calculé par la formule des probabilités totales.
</div>

<div class="example">
    Une maladie touche 1% de la population. Un test de dépistage donne un résultat positif chez 95% des malades, et un résultat positif chez 2% des personnes saines (faux positif). Une personne est testée positive. Quelle est la probabilité qu'elle soit réellement malade ?
    Soit $M$ l'événement "être malade", $\bar{M}$ l'événement "être sain".
    Soit $T^+$ l'événement "le test est positif".
    Données : $P(M)=0.01$, $P(\bar{M})=1-0.01=0.99$.
    $P(T^+|M)=0.95$. $P(T^+|\bar{M})=0.02$.
    On cherche $P(M|T^+)$.
    On utilise la formule de Bayes. D'abord, calculons $P(T^+)$ avec la formule des probabilités totales (le système complet est $(M, \bar{M})$) :
    $P(T^+) = P(T^+|M)P(M) + P(T^+|\bar{M})P(\bar{M})$
    $P(T^+) = (0.95)(0.01) + (0.02)(0.99) = 0.0095 + 0.0198 = 0.0293$.
    Maintenant, appliquons Bayes :
    $P(M|T^+) = \frac{P(T^+|M) P(M)}{P(T^+)} = \frac{(0.95)(0.01)}{0.0293} = \frac{0.0095}{0.0293} \approx 0.324$.
    Même avec un test positif, la probabilité d'être malade n'est que d'environ 32.4%. Ceci est dû à la faible prévalence de la maladie et au taux de faux positifs.
</div>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            // customised options
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
        });
    });
</script>

</body>
</html>
