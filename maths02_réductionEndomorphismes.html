<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cours : Réduction des endomorphismes</title>
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            tags: 'ams', // Automatic equation numbering
            macros: {
              K: '\\mathbb{K}',
              C: '\\mathbb{C}',
              R: '\\mathbb{R}',
              N: '\\mathbb{N}',
              Z: '\\mathbb{Z}',
              GLnK: '{\\text{GL}_n(\\K)}',
              MnK: '{\\mathcal{M}_n(\\K)}',
              LE: '{\\mathcal{L}(E)}',
              Id: '\\text{Id}',
              dim: '\\text{dim}',
              Vect: '\\text{Vect}', // Changed from \vect to \Vect
              Ker: '\\text{Ker}',
              Im: '\\text{Im}',
              det: '\\text{det}',
              Tr: '\\text{Tr}',
              Sp: '\\text{Sp}',
              mult: '\\text{mult}'
            }
          },
          svg: {
            fontCache: 'global'
          }
        };
        </script>
        <script type="text/javascript" id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f8f9fa; /* Gris très clair */
        }
        h1, h2, h3 {
            color: #2E7D32; /* Vert foncé */
            border-bottom: 2px solid #2E7D32;
            padding-bottom: 5px;
            margin-top: 30px;
        }
        h1 { font-size: 2.2em; }
        h2 { font-size: 1.8em; }
        h3 { font-size: 1.4em; font-style: italic; border-bottom: none; color: #4CAF50;} /* Vert */

        .definition, .theorem, .proposition, .corollary, .example, .proof, .remark {
            margin-top: 15px;
            margin-bottom: 15px;
            padding: 15px;
            border-left: 5px solid #66BB6A; /* Vert clair */
            background-color: #E8F5E9; /* Vert très très clair */
            border-radius: 0 5px 5px 0;
        }
        .definition strong, .theorem strong, .proposition strong, .corollary strong, .remark strong {
            color: #1B5E20; /* Vert encore plus foncé */
            font-weight: bold;
        }
        .proof {
            border-left-color: #0288d1; /* Bleu */
            background-color: #e1f5fe; /* Bleu très clair */
        }
        .proof strong {
            color: #01579b; /* Bleu foncé */
            font-weight: bold;
        }
        .example {
            border-left-color: #ffc107; /* Jaune */
            background-color: #fff8e1; /* Jaune très clair */
        }
         .example strong {
            color: #856404; /* Jaune foncé */
            font-weight: bold;
        }
         .remark {
            border-left-color: #6c757d; /* Gris */
            background-color: #f8f9fa; /* Gris très clair */
        }
         .remark strong {
            color: #343a40; /* Gris foncé */
            font-weight: bold;
        }
        code {
            background-color: #e9ecef; /* Gris clair */
            padding: 2px 5px;
            border-radius: 4px;
            font-family: monospace;
        }
        ul {
            margin-left: 20px;
            list-style-type: square;
            padding-left: 20px;
        }
         ul ul {
             list-style-type: circle;
             margin-top: 5px;
         }
        li {
            margin-bottom: 8px;
        }
        .MJX-TeXAtom-ORD { }
        a { color: #1976d2; text-decoration: none; } /* Bleu pour les liens */
        a:hover { text-decoration: underline; }
    </style>
</head>
<body>

    <h1>Réduction des endomorphismes</h1>

    <p>Dans ce chapitre, $E$ désigne un $\K$-espace vectoriel où $\K = \R$ ou $\C$. L'objectif est de trouver des bases dans lesquelles la matrice d'un endomorphisme $u \in \LE$ (ou d'une matrice $A \in \MnK$) est la plus simple possible (diagonale ou triangulaire).</p>

    <h2>1. Sous-espaces stables</h2>

    <div class="definition">
        <strong>Définition (Sous-espace stable et endomorphisme induit)</strong><br>
        Soit $u \in \LE$. Un sous-espace vectoriel $F$ de $E$ est dit <strong>stable par $u$</strong> si $u(F) \subseteq F$, c'est-à-dire que l'image par $u$ de tout vecteur de $F$ reste dans $F$.
        <br>Si $F$ est stable par $u$, on peut définir un endomorphisme de $F$, appelé <strong>endomorphisme induit par $u$ sur $F$</strong>, noté $u_F$, en posant :
        $$ \forall x \in F, u_F(x) = u(x) $$
        (C'est la restriction de $u$ à $F$, vue comme une application de $F$ dans $F$).
    </div>

    <div class="example">
        <strong>Exemples triviaux :</strong> $\{0_E\}$ et $E$ sont toujours stables par tout $u \in \LE$.
    </div>

    <div class="proposition">
        <strong>Proposition (Stabilité et commutation)</strong><br>
        Soient $u, v \in \LE$. Si $u$ et $v$ commutent ($u \circ v = v \circ u$), alors le noyau $\Ker(u)$ et l'image $\Im(u)$ sont stables par $v$.
    </div>
    <div class="proof">
        <strong>Preuve :</strong><br>
        - <strong>Stabilité de $\Ker(u)$ par $v$ :</strong> Soit $x \in \Ker(u)$, c'est-à-dire $u(x) = 0_E$. On veut montrer que $v(x) \in \Ker(u)$. Calculons $u(v(x))$ :
        $u(v(x)) = (u \circ v)(x) = (v \circ u)(x)$ (car $u,v$ commutent) $= v(u(x)) = v(0_E) = 0_E$ (car $v$ est linéaire).
        Donc $v(x) \in \Ker(u)$.
        <br>- <strong>Stabilité de $\Im(u)$ par $v$ :</strong> Soit $y \in \Im(u)$. Il existe $x \in E$ tel que $y = u(x)$. On veut montrer que $v(y) \in \Im(u)$.
        $v(y) = v(u(x)) = (v \circ u)(x) = (u \circ v)(x)$ (car $u,v$ commutent) $= u(v(x))$.
        Posons $x' = v(x) \in E$. Alors $v(y) = u(x')$. Ceci montre que $v(y)$ est l'image par $u$ d'un vecteur de $E$, donc $v(y) \in \Im(u)$. $\Box$
    </div>

    <div class="remark">
        En particulier, $\Ker(u)$ et $\Im(u)$ sont toujours stables par $u$ lui-même. Plus généralement, $\Ker(P(u))$ et $\Im(P(u))$ sont stables par $u$ pour tout polynôme $P \in \K[X]$.
    </div>

    <p>Lorsque $E$ est de dimension finie, la stabilité se traduit bien matriciellement.</p>

    <div class="proposition">
        <strong>Proposition (Caractérisation matricielle de la stabilité)</strong><br>
        Soit $E$ de dimension finie $n$. Soit $F$ un sous-espace vectoriel de $E$ de dimension $p$. Soit $\mathcal{B} = (e_1, \dots, e_n)$ une base de $E$ telle que $\mathcal{B}_F = (e_1, \dots, e_p)$ soit une base de $F$. Soit $u \in \LE$.
        Alors $F$ est stable par $u$ si et seulement si la matrice de $u$ dans la base $\mathcal{B}$, $M = \text{Mat}(u, \mathcal{B})$, a une structure triangulaire supérieure par blocs :
        $$ M = \begin{pmatrix} A & B \\ 0 & C \end{pmatrix} $$
        où $A \in \mathcal{M}_p(\K)$, $B \in \mathcal{M}_{p, n-p}(\K)$, $0$ est la matrice nulle de $\mathcal{M}_{n-p, p}(\K)$, et $C \in \mathcal{M}_{n-p}(\K)$.
        <br>De plus, si $F$ est stable, alors $A$ est la matrice de l'endomorphisme induit $u_F$ dans la base $\mathcal{B}_F$.
    </div>
    <div class="proof">
        <strong>Preuve (Idée) :</strong><br>
        La $j$-ème colonne de $M$ contient les coordonnées de $u(e_j)$ dans la base $\mathcal{B}$.
        - Si $F$ est stable : Pour $j \in \{1, \dots, p\}$, $e_j \in F$, donc $u(e_j) \in F$. Ainsi $u(e_j)$ est une combinaison linéaire uniquement des vecteurs de $\mathcal{B}_F = (e_1, \dots, e_p)$. Les coordonnées de $u(e_j)$ sur les vecteurs $e_{p+1}, \dots, e_n$ sont donc nulles. Cela correspond au bloc nul en bas à gauche.
        - Si $M$ a cette forme : Pour $j \in \{1, \dots, p\}$, les coordonnées de $u(e_j)$ sur $e_{p+1}, \dots, e_n$ sont nulles. Donc $u(e_j)$ est combinaison linéaire de $(e_1, \dots, e_p)$, i.e., $u(e_j) \in F$. Comme $(e_1, \dots, e_p)$ engendre $F$, par linéarité, $u(F) \subseteq F$. $\Box$
    </div>

     <div class="proposition">
        <strong>Proposition (Stabilité et somme directe)</strong><br>
        Soit $E = \bigoplus_{i=1}^p E_i$ une décomposition en somme directe (dimensions finies). Soit $\mathcal{B} = (\mathcal{B}_1, \dots, \mathcal{B}_p)$ une base adaptée. Soit $u \in \LE$.
        Alors tous les sous-espaces $E_i$ sont stables par $u$ si et seulement si la matrice de $u$ dans la base $\mathcal{B}$ est <strong>diagonale par blocs</strong> :
        $$ \text{Mat}(u, \mathcal{B}) = \begin{pmatrix}
            A_1 & 0 & \dots & 0 \\
            0 & A_2 & \dots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \dots & A_p
            \end{pmatrix} $$
        où $A_i \in \mathcal{M}_{\dim(E_i)}(\K)$ est la matrice de l'endomorphisme induit $u_{E_i}$ dans la base $\mathcal{B}_i$.
    </div>

    <h2>2. Éléments propres d'un endomorphisme et d'une matrice carrée</h2>

    <div class="definition">
        <strong>Définition (Valeur propre, Vecteur propre)</strong><br>
        Soit $u \in \LE$.
        <ul>
            <li>On dit que $\lambda \in \K$ est une <strong>valeur propre</strong> de $u$ s'il existe un vecteur $x \in E$, <strong>non nul</strong> ($x \neq 0_E$), tel que $u(x) = \lambda x$.</li>
            <li>Un tel vecteur $x$ est appelé un <strong>vecteur propre</strong> de $u$ associé à la valeur propre $\lambda$.</li>
            <li>Si $x$ est un vecteur propre associé à $\lambda$, alors la droite vectorielle $\Vect(x)$ est stable par $u$. En effet, pour tout $y = \alpha x \in \Vect(x)$, $u(y) = u(\alpha x) = \alpha u(x) = \alpha (\lambda x) = \lambda (\alpha x) = \lambda y \in \Vect(x)$.</li>
        </ul>
    </div>

    <div class="definition">
        <strong>Définition (Sous-espace propre)</strong><br>
        Si $\lambda \in \K$ est une valeur propre de $u$, l'ensemble des vecteurs $x \in E$ tels que $u(x) = \lambda x$ est un sous-espace vectoriel de $E$, appelé <strong>sous-espace propre</strong> associé à $\lambda$. On le note $E_\lambda(u)$ ou simplement $E_\lambda$.
        $$ E_\lambda = \{ x \in E \mid u(x) = \lambda x \} = \{ x \in E \mid (u - \lambda \Id_E)(x) = 0_E \} = \Ker(u - \lambda \Id_E) $$
        Par définition d'une valeur propre, $E_\lambda$ contient toujours des vecteurs non nuls, donc $\dim(E_\lambda) \ge 1$. Le sous-espace propre $E_\lambda$ est stable par $u$.
    </div>
     <div class="remark">
        <strong>Attention :</strong> Le vecteur nul $0_E$ vérifie $u(0_E) = \lambda 0_E$ pour tout $\lambda$, mais il n'est <strong>jamais</strong> considéré comme un vecteur propre. Un vecteur propre est toujours non nul. Par contre $0_E$ appartient à tous les sous-espaces propres $E_\lambda$.
    </div>

    <div class="definition">
        <strong>Définition (Spectre)</strong><br>
        L'ensemble des valeurs propres de $u$ s'appelle le <strong>spectre</strong> de $u$ et est noté $\Sp(u)$ ou $\text{Sp}_\K(u)$.
        $$ \lambda \in \Sp(u) \iff \Ker(u - \lambda \Id_E) \neq \{0_E\} \iff u - \lambda \Id_E \text{ n'est pas injectif} $$
    </div>

    <div class="theorem">
        <strong>Théorème (Somme directe des sous-espaces propres)</strong><br>
        Soient $\lambda_1, \dots, \lambda_p$ des valeurs propres distinctes de $u \in \LE$. Alors les sous-espaces propres associés $E_{\lambda_1}, \dots, E_{\lambda_p}$ sont en <strong>somme directe</strong>.
        $$ E_{\lambda_1} + \dots + E_{\lambda_p} = E_{\lambda_1} \oplus \dots \oplus E_{\lambda_p} $$
    </div>
     <div class="proof">
        <a href="#">(Démonstration en vidéo!)</a>
        <strong>Preuve (Idée par récurrence sur $p$) :</strong><br>
        - Initialisation (p=1) : Trivial.
        - Hérédité : Supposons le résultat vrai pour $p-1$ valeurs propres distinctes. Soit $x_1 + \dots + x_p = 0_E$ avec $x_i \in E_{\lambda_i}$. On veut montrer $x_i=0$ pour tout $i$.
        Appliquons $u$ : $\sum_{i=1}^p u(x_i) = \sum_{i=1}^p \lambda_i x_i = 0_E$.
        Multiplions la somme initiale par $\lambda_p$ : $\sum_{i=1}^p \lambda_p x_i = 0_E$.
        Soustrayons les deux équations : $\sum_{i=1}^{p-1} (\lambda_i - \lambda_p) x_i = 0_E$.
        Chaque terme $(\lambda_i - \lambda_p) x_i$ appartient à $E_{\lambda_i}$. Par hypothèse de récurrence (appliquée aux $p-1$ premières valeurs propres, qui sont distinctes), comme la somme est directe, chaque terme est nul : $(\lambda_i - \lambda_p) x_i = 0_E$ pour $i=1, \dots, p-1$.
        Puisque les $\lambda_i$ sont distincts, $\lambda_i - \lambda_p \neq 0$ pour $i < p$. Donc $x_i = 0_E$ pour $i=1, \dots, p-1$.
        En revenant à la somme initiale $x_1 + \dots + x_p = 0_E$, on obtient $x_p = 0_E$.
        Donc tous les $x_i$ sont nuls, ce qui prouve que la somme est directe. $\Box$
    </div>

    <div class="corollary">
        <strong>Corollaire (Liberté des vecteurs propres)</strong><br>
        Une famille de vecteurs propres $(x_1, \dots, x_p)$ associés à des valeurs propres distinctes $(\lambda_1, \dots, \lambda_p)$ est toujours une famille libre.
    </div>
    <div class="proof">
        <strong>Preuve :</strong><br>
        Soit $\sum_{i=1}^p \alpha_i x_i = 0_E$. Chaque $\alpha_i x_i$ appartient à $E_{\lambda_i}$. Comme les $E_{\lambda_i}$ sont en somme directe, cela implique $\alpha_i x_i = 0_E$ pour tout $i$. Or, un vecteur propre $x_i$ est non nul par définition. Donc $\alpha_i = 0$ pour tout $i$. La famille est libre. $\Box$
    </div>

    <div class="corollary">
        <strong>Corollaire (Nombre de valeurs propres)</strong><br>
        Si $E$ est de dimension finie $n$, un endomorphisme $u \in \LE$ admet au plus $n$ valeurs propres distinctes.
        <br>(Car si $u$ avait $p > n$ valeurs propres distinctes, on pourrait trouver $p$ vecteurs propres associés formant une famille libre de $p$ vecteurs dans un espace de dimension $n$, ce qui est impossible).
    </div>

    <div class="definition">
        <strong>Définition (Éléments propres d'une matrice)</strong><br>
        Soit $A \in \MnK$. L'endomorphisme canoniquement associé à $A$ est $u_A : \K^n \to \K^n, X \mapsto AX$.
        <ul>
            <li>Les <strong>valeurs propres</strong> de $A$ sont les valeurs propres de $u_A$. $\lambda \in \Sp(A) \iff \exists X \in \K^n, X \neq 0, AX = \lambda X$.</li>
            <li>Les <strong>vecteurs propres</strong> de $A$ sont les vecteurs propres de $u_A$. $X \in \K^n, X \neq 0$ est vecteur propre pour $\lambda$ si $AX = \lambda X$.</li>
            <li>Le <strong>sous-espace propre</strong> de $A$ associé à $\lambda$ est $E_\lambda(A) = \Ker(A - \lambda I_n) = \{ X \in \K^n \mid AX = \lambda X \}$.</li>
            <li>Le <strong>spectre</strong> de $A$ est $\Sp(A) = \Sp(u_A)$.</li>
        </ul>
         $\lambda \in \Sp(A) \iff A - \lambda I_n$ n'est pas inversible $\iff \det(A - \lambda I_n) = 0$.
    </div>

    <div class="definition">
        <strong>Définition (Matrices semblables)</strong><br>
        Deux matrices $A, B \in \MnK$ sont dites <strong>semblables</strong> s'il existe une matrice inversible $P \in \GLnK$ telle que $B = P^{-1} A P$.
        <br>(Être semblables signifie qu'elles représentent le même endomorphisme dans des bases différentes).
    </div>

    <div class="proposition">
        <strong>Proposition (Spectre et similitude)</strong><br>
        Deux matrices semblables ont le même spectre.
    </div>
     <div class="proof">
        <strong>Preuve :</strong><br>
        Soit $B = P^{-1} A P$. $X$ est vecteur propre de $B$ pour $\lambda$
        $\iff BX = \lambda X$ (avec $X \neq 0$)
        $\iff P^{-1} A P X = \lambda X$
        $\iff A (PX) = \lambda (PX)$.
        Posons $Y = PX$. Comme $P$ est inversible et $X \neq 0$, on a $Y \neq 0$. L'équation devient $AY = \lambda Y$.
        Donc $\lambda$ est valeur propre de $A$ avec $Y=PX$ comme vecteur propre associé.
        Réciproquement, si $Y$ est vecteur propre de $A$ pour $\lambda$, $AY = \lambda Y$. Alors $A P (P^{-1} Y) = \lambda P (P^{-1} Y)$. En multipliant par $P^{-1}$ à gauche : $P^{-1} A P (P^{-1} Y) = \lambda (P^{-1} Y)$. Soit $X = P^{-1} Y \neq 0$. On a $BX = \lambda X$. Donc $\lambda$ est valeur propre de $B$.
        Ainsi $\Sp(A) = \Sp(B)$. $\Box$
    </div>

    <h2>3. Polynôme caractéristique</h2>

    <p>Dans cette section, $E$ est de dimension finie $n$. Soit $u \in \LE$ et $A \in \MnK$.</p>

    <div class="definition">
        <strong>Définition (Polynôme caractéristique d'une matrice)</strong><br>
        On appelle <strong>polynôme caractéristique</strong> de la matrice $A \in \MnK$ le polynôme $\chi_A \in \K[X]$ défini par :
        $$ \chi_A(X) = \det(X I_n - A) $$
        C'est un polynôme de degré $n$.
    </div>
     <div class="remark">
        Certains auteurs définissent $\chi_A(X) = \det(A - X I_n) = (-1)^n \det(X I_n - A)$. Avec notre définition, $\chi_A(X)$ est toujours unitaire.
    </div>

    <div class="example">
        <strong>Exemple (Matrices triangulaires) :</strong><br>
        Si $A$ est une matrice triangulaire (supérieure ou inférieure) avec les éléments diagonaux $\lambda_1, \dots, \lambda_n$, alors $X I_n - A$ est aussi triangulaire avec les éléments diagonaux $X - \lambda_1, \dots, X - \lambda_n$. Son déterminant est le produit des éléments diagonaux :
        $$ \chi_A(X) = (X - \lambda_1) (X - \lambda_2) \dots (X - \lambda_n) $$
        Les racines du polynôme caractéristique sont bien les coefficients diagonaux.
    </div>

     <div class="proposition">
        <strong>Proposition (Polynôme caractéristique et similitude)</strong><br>
        Deux matrices semblables ont le même polynôme caractéristique.
    </div>
      <div class="proof">
        <strong>Preuve :</strong><br>
        Soit $B = P^{-1} A P$.
        $\chi_B(X) = \det(X I_n - B) = \det(X I_n - P^{-1} A P)$.
        $X I_n = X P^{-1} I_n P = P^{-1} (X I_n) P$.
        Donc $X I_n - B = P^{-1} (X I_n) P - P^{-1} A P = P^{-1} (X I_n - A) P$.
        Par multiplicativité du déterminant :
        $\det(X I_n - B) = \det(P^{-1}) \det(X I_n - A) \det(P) = \frac{1}{\det(P)} \det(X I_n - A) \det(P) = \det(X I_n - A)$.
        Donc $\chi_B(X) = \chi_A(X)$. $\Box$
    </div>

     <div class="definition">
        <strong>Définition (Polynôme caractéristique d'un endomorphisme)</strong><br>
        Soit $u \in \LE$. Le polynôme caractéristique de $u$, noté $\chi_u(X)$, est défini comme le polynôme caractéristique de n'importe quelle matrice $A$ représentant $u$ dans une base de $E$.
        <br>(Cette définition est cohérente car toutes les matrices représentant $u$ sont semblables entre elles et ont donc le même polynôme caractéristique).
    </div>

    <div class="proposition">
        <strong>Proposition (Coefficients du polynôme caractéristique)</strong><br>
        Soit $A \in \MnK$. Son polynôme caractéristique $\chi_A(X) = \det(XI_n - A)$ est un polynôme unitaire de degré $n$. De plus :
        $$ \chi_A(X) = X^n - \Tr(A) X^{n-1} + \dots + (-1)^n \det(A) $$
        Le coefficient de $X^{n-1}$ est $-\Tr(A)$ et le coefficient constant est $\chi_A(0) = \det(-A) = (-1)^n \det(A)$.
    </div>
     <div class="example">
        <strong>Cas $n=2$ :</strong> Pour $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$,
        $XI_2 - A = \begin{pmatrix} X-a & -b \\ -c & X-d \end{pmatrix}$.
        $\chi_A(X) = (X-a)(X-d) - (-b)(-c) = X^2 - (a+d)X + (ad-bc)$.
        On retrouve $\chi_A(X) = X^2 - \Tr(A) X + \det(A)$.
    </div>

    <div class="theorem">
        <strong>Théorème (Valeurs propres et racines de $\chi_u$)</strong><br>
        Soit $u \in \LE$ (ou $A \in \MnK$). Les valeurs propres de $u$ (resp. $A$) dans $\K$ sont exactement les racines de son polynôme caractéristique $\chi_u(X)$ (resp. $\chi_A(X)$) qui appartiennent à $\K$.
        $$ \lambda \in \Sp(u) \iff \chi_u(\lambda) = 0 $$
    </div>
     <div class="proof">
        <strong>Preuve :</strong><br>
        $\lambda \in \Sp(u) \iff \Ker(u - \lambda \Id_E) \neq \{0_E\}$
        $\iff u - \lambda \Id_E$ n'est pas un isomorphisme (car en dimension finie, injectif $\iff$ surjectif $\iff$ bijectif)
        $\iff \det(u - \lambda \Id_E) = 0$.
        Soit $A$ la matrice de $u$ dans une base. $\det(u - \lambda \Id_E) = \det(A - \lambda I_n)$.
        $\chi_u(\lambda) = \det(\lambda I_n - A) = (-1)^n \det(A - \lambda I_n)$.
        Donc $\lambda \in \Sp(u) \iff \det(A - \lambda I_n) = 0 \iff \chi_u(\lambda) = 0$. $\Box$
    </div>

    <div class="definition">
        <strong>Définition (Multiplicité algébrique)</strong><br>
        Soit $\lambda \in \Sp(u)$. La <strong>multiplicité algébrique</strong> de $\lambda$, notée $\mult(\lambda)$, est sa multiplicité en tant que racine du polynôme caractéristique $\chi_u(X)$.
    </div>

    <div class="proposition">
        <strong>Proposition (Multiplicité géométrique vs algébrique)</strong><br>
        Pour toute valeur propre $\lambda \in \Sp(u)$, la dimension du sous-espace propre associé $E_\lambda$ (appelée multiplicité géométrique) est inférieure ou égale à la multiplicité algébrique $\mult(\lambda)$ :
        $$ 1 \le \dim(E_\lambda) \le \mult(\lambda) $$
    </div>

    <div class="proposition">
        <strong>Proposition (Polynôme caractéristique et sous-espaces stables)</strong><br>
        Si $F$ est un sous-espace vectoriel de $E$ stable par $u$, et si $u_F$ est l'endomorphisme induit par $u$ sur $F$, alors le polynôme caractéristique $\chi_{u_F}(X)$ de $u_F$ divise le polynôme caractéristique $\chi_u(X)$ de $u$.
    </div>
     <div class="proof">
        <strong>Preuve (Idée) :</strong><br>
        On choisit une base $\mathcal{B} = (\mathcal{B}_F, \mathcal{B}')$ de $E$ où $\mathcal{B}_F$ est une base de $F$. La matrice de $u$ dans cette base est de la forme $M = \begin{pmatrix} A & B \\ 0 & C \end{pmatrix}$, où $A = \text{Mat}(u_F, \mathcal{B}_F)$.
        Alors $\chi_u(X) = \det(XI_n - M) = \det \begin{pmatrix} XI_p - A & -B \\ 0 & XI_{n-p} - C \end{pmatrix}$.
        Le déterminant d'une matrice triangulaire par blocs est le produit des déterminants des blocs diagonaux :
        $\chi_u(X) = \det(XI_p - A) \times \det(XI_{n-p} - C) = \chi_A(X) \times \chi_C(X) = \chi_{u_F}(X) \times \chi_C(X)$.
        Donc $\chi_{u_F}$ divise $\chi_u$. $\Box$
    </div>

    <h2>4. Endomorphismes et matrices diagonalisables</h2>

    <p>L'espace vectoriel $E$ est de dimension finie $n$.</p>

    <div class="definition">
        <strong>Définition (Endomorphisme diagonalisable)</strong><br>
        Un endomorphisme $u \in \LE$ est dit <strong>diagonalisable</strong> s'il existe une base $\mathcal{B}$ de $E$ dans laquelle la matrice de $u$, $\text{Mat}(u, \mathcal{B})$, est une matrice diagonale.
        <br>Une telle base $\mathcal{B}$ est alors nécessairement constituée de vecteurs propres de $u$.
    </div>

    <div class="proposition">
        <strong>Proposition (Caractérisations de la diagonalisabilité)</strong><br>
        Soit $u \in \LE$. Les assertions suivantes sont équivalentes :
        <ol>
            <li>$u$ est diagonalisable.</li>
            <li>Il existe une base de $E$ formée de vecteurs propres de $u$.</li>
            <li>La somme des sous-espaces propres de $u$ est égale à $E$ : $\sum_{\lambda \in \Sp(u)} E_\lambda = E$. (Comme cette somme est toujours directe, cela revient à $E = \bigoplus_{\lambda \in \Sp(u)} E_\lambda$).</li>
            <li>La somme des dimensions des sous-espaces propres est égale à la dimension de $E$ : $\sum_{\lambda \in \Sp(u)} \dim(E_\lambda) = \dim(E)$.</li>
        </ol>
    </div>
    <div class="proof">
        <strong>Preuve (Idée des équivalences) :</strong><br>
        (1 $\iff$ 2) : Par définition, si $u$ est diagonalisable, sa matrice dans une base $\mathcal{B}=(e_1, \dots, e_n)$ est $D = \text{diag}(d_1, \dots, d_n)$. Alors $u(e_i) = d_i e_i$, donc chaque $e_i$ est un vecteur propre pour la valeur propre $d_i$. Réciproquement, si $\mathcal{B}$ est une base de vecteurs propres $u(e_i)=\lambda_i e_i$, la matrice est diagonale.
        <br>(2 $\implies$ 3) : Si $\mathcal{B}$ est une base de vecteurs propres, elle engendre $E$. Chaque $e_i \in E_{\lambda_i} \subseteq \sum E_\lambda$. Donc $E = \Vect(\mathcal{B}) \subseteq \sum E_\lambda$. Comme $\sum E_\lambda \subseteq E$, on a l'égalité.
        <br>(3 $\implies$ 4) : Comme la somme des $E_\lambda$ est directe, $\dim(\sum E_\lambda) = \sum \dim(E_\lambda)$. Si $\sum E_\lambda = E$, alors $\sum \dim(E_\lambda) = \dim(E)$.
        <br>(4 $\implies$ 2) : Soit $S = \bigoplus E_\lambda$. On a $\dim(S) = \sum \dim(E_\lambda) = \dim(E)$. Comme $S$ est un sous-espace de $E$ de même dimension que $E$, on a $S=E$. En concaténant des bases $\mathcal{B}_\lambda$ de chaque $E_\lambda$, on obtient une base de $S=E$ (base adaptée à la somme directe). Cette base est formée de vecteurs propres. $\Box$
    </div>

    <div class="example">
        <strong>Exemples :</strong>
        <ul>
            <li>Un <strong>projecteur</strong> $p$ ($p \circ p = p$) non trivial est diagonalisable. Ses seules valeurs propres possibles sont 0 et 1. $E_1 = \Im(p)$ et $E_0 = \Ker(p)$. On a $E = \Ker(p) \oplus \Im(p) = E_0 \oplus E_1$. La somme des dimensions est $\dim E$, donc $p$ est diagonalisable.</li>
            <li>Une <strong>symétrie</strong> $s$ ($s \circ s = \Id$) non triviale est diagonalisable. Ses seules valeurs propres possibles sont 1 et -1. $E_1 = \Ker(s-\Id)$ et $E_{-1} = \Ker(s+\Id)$. On a $E = E_1 \oplus E_{-1}$. La somme des dimensions est $\dim E$, donc $s$ est diagonalisable.</li>
        </ul>
    </div>

    <div class="theorem">
        <strong>Théorème (Critère de diagonalisabilité)</strong><br>
        Un endomorphisme $u \in \LE$ est diagonalisable si et seulement si les deux conditions suivantes sont remplies :
        <ol>
            <li>Le polynôme caractéristique $\chi_u(X)$ est <strong>scindé</strong> sur $\K$ (c'est-à-dire qu'il peut s'écrire comme produit de facteurs de degré 1 dans $\K[X]$).
            $$ \chi_u(X) = \prod_{i=1}^p (X - \lambda_i)^{\mult(\lambda_i)} \quad \text{avec } \sum_{i=1}^p \mult(\lambda_i) = n $$
            où $\lambda_1, \dots, \lambda_p$ sont les valeurs propres distinctes.</li>
            <li>Pour chaque valeur propre $\lambda_i$, la dimension du sous-espace propre (multiplicité géométrique) est égale à sa multiplicité algébrique :
            $$ \dim(E_{\lambda_i}) = \mult(\lambda_i) $$</li>
        </ol>
    </div>
     <div class="proof">
        <a href="#">(Démonstration en vidéo!)</a>
         <strong>Preuve (Idée) :</strong><br>
         On sait que $1 \le \dim(E_\lambda) \le \mult(\lambda)$. Soient $\lambda_1, \dots, \lambda_p$ les valeurs propres distinctes.
         La somme des dimensions des sous-espaces propres est $S_d = \sum_{i=1}^p \dim(E_{\lambda_i})$.
         La somme des multiplicités algébriques est $S_a = \sum_{i=1}^p \mult(\lambda_i)$.
         On a $S_d \le S_a$.
         De plus, $S_a \le n = \deg(\chi_u)$, avec égalité si et seulement si $\chi_u$ est scindé sur $\K$.
         L'endomorphisme $u$ est diagonalisable $\iff S_d = n$ (par la Proposition précédente).
         En combinant, $u$ est diagonalisable $\iff S_d = n$.
         Or, $S_d \le S_a \le n$. Donc $S_d = n$ impose $S_d = S_a = n$.
         $S_a = n$ signifie que $\chi_u$ est scindé.
         $S_d = S_a$ signifie $\sum \dim(E_{\lambda_i}) = \sum \mult(\lambda_i)$. Puisque $\dim(E_{\lambda_i}) \le \mult(\lambda_i)$ pour chaque $i$, l'égalité des sommes n'est possible que si $\dim(E_{\lambda_i}) = \mult(\lambda_i)$ pour tout $i$.
         Réciproquement, si $\chi_u$ est scindé ($S_a=n$) et $\dim(E_{\lambda_i}) = \mult(\lambda_i)$ pour tout $i$ ($S_d = S_a$), alors $S_d = n$, donc $u$ est diagonalisable. $\Box$
    </div>

    <div class="corollary">
        <strong>Corollaire (Condition suffisante : $n$ valeurs propres distinctes)</strong><br>
        Si $u \in \LE$ admet $n = \dim(E)$ valeurs propres distinctes dans $\K$, alors $u$ est diagonalisable sur $\K$.
    </div>
    <div class="proof">
        <strong>Preuve :</strong><br>
        S'il y a $n$ valeurs propres distinctes $\lambda_1, \dots, \lambda_n$, alors $\chi_u(X)$ a au moins $n$ racines distinctes. Comme $\deg(\chi_u) = n$, on a $\chi_u(X) = C \prod_{i=1}^n (X - \lambda_i)$ (avec $C=1$ car $\chi_u$ est unitaire). Le polynôme est scindé à racines simples.
        La multiplicité algébrique $\mult(\lambda_i) = 1$ pour chaque $i$.
        On sait que $1 \le \dim(E_{\lambda_i}) \le \mult(\lambda_i)$. Donc $\dim(E_{\lambda_i}) = 1 = \mult(\lambda_i)$ pour chaque $i$.
        Les deux conditions du théorème sont vérifiées, $u$ est diagonalisable. $\Box$
    </div>

    <div class="definition">
        <strong>Définition (Matrice diagonalisable)</strong><br>
        Une matrice $A \in \MnK$ est dite <strong>diagonalisable</strong> sur $\K$ si l'endomorphisme $u_A$ canoniquement associé est diagonalisable.
    </div>

    <div class="proposition">
        <strong>Proposition (Caractérisation matricielle de la diagonalisabilité)</strong><br>
        Une matrice $A \in \MnK$ est diagonalisable sur $\K$ si et seulement si $A$ est <strong>semblable</strong> à une matrice diagonale $D \in \MnK$.
        $$ A \text{ diagonalisable} \iff \exists P \in \GLnK, \exists D \text{ diagonale}, A = P D P^{-1} $$
        La matrice $D$ a sur sa diagonale les valeurs propres de $A$ (répétées avec leur multiplicité). Les colonnes de $P$ forment une base de $\K^n$ constituée de vecteurs propres de $A$, dans l'ordre correspondant aux valeurs propres sur la diagonale de $D$.
    </div>

    <div class="remark">
        <strong>Conséquences : Trace et Déterminant</strong><br>
        Si $A \in \MnK$ est diagonalisable (ou même juste trigonalisable, voir section suivante), alors :
        <ul>
            <li>La <strong>trace</strong> de $A$ est égale à la somme de ses valeurs propres (comptées avec leur multiplicité algébrique).</li>
            <li>Le <strong>déterminant</strong> de $A$ est égal au produit de ses valeurs propres (comptées avec leur multiplicité algébrique).</li>
        </ul>
        ($\Tr(A) = \sum \lambda_i \mult(\lambda_i)$ et $\det(A) = \prod \lambda_i^{\mult(\lambda_i)}$ si $\chi_A$ est scindé).
        Ceci vient du fait que la trace et le déterminant sont invariants par similitude, et sont faciles à calculer pour une matrice diagonale (ou triangulaire).
    </div>


    <h2>5. Endomorphismes et matrices trigonalisables</h2>

    <p>L'espace vectoriel $E$ est de dimension finie $n$.</p>

    <div class="definition">
        <strong>Définition (Endomorphisme trigonalisable)</strong><br>
        Un endomorphisme $u \in \LE$ est dit <strong>trigonalisable</strong> (ou triangularisable) sur $\K$ s'il existe une base $\mathcal{B}$ de $E$ dans laquelle la matrice de $u$, $\text{Mat}(u, \mathcal{B})$, est une matrice <strong>triangulaire supérieure</strong>.
    </div>

     <div class="remark">
        Il est équivalent d'exiger l'existence d'une base où la matrice est triangulaire inférieure.
     </div>

    <div class="theorem">
        <strong>Théorème (Critère de trigonalisabilité)</strong><br>
        Un endomorphisme $u \in \LE$ est trigonalisable sur $\K$ si et seulement si son polynôme caractéristique $\chi_u(X)$ est <strong>scindé</strong> sur $\K$.
    </div>
     <div class="proof">
        <a href="#">(Démonstration en vidéo!)</a>
        <strong>Preuve (Idée du sens $\impliedby$) :</strong><br>
        Par récurrence sur la dimension $n = \dim(E)$.
        - Si $n=1$, tout endomorphisme est représenté par une matrice (1,1) qui est triangulaire.
        - Supposons le résultat vrai en dimension $n-1$. Soit $u \in \LE$ avec $\chi_u$ scindé sur $\K$.
        Comme $\chi_u$ est scindé, $u$ admet au moins une valeur propre $\lambda_1 \in \K$. Soit $e_1$ un vecteur propre associé ($e_1 \neq 0$).
        Complétons $e_1$ en une base $\mathcal{B}'=(e_1, e'_2, \dots, e'_n)$ de $E$. La matrice de $u$ dans cette base est de la forme :
        $ M' = \begin{pmatrix} \lambda_1 & L \\ 0 & C \end{pmatrix} $ où $L$ est une ligne et $C \in \mathcal{M}_{n-1}(\K)$.
        On a $\chi_u(X) = \det(XI_n - M') = (X-\lambda_1) \det(XI_{n-1} - C) = (X-\lambda_1) \chi_C(X)$.
        Comme $\chi_u$ est scindé sur $\K$, $\chi_C$ l'est aussi.
        $C$ représente un endomorphisme $u'$ sur l'espace quotient $E/\Vect(e_1)$ ou (plus simplement ici) on peut considérer l'endomorphisme induit par la projection sur $F = \Vect(e'_2, \dots, e'_n)$ parallèlement à $e_1$. Son polynôme caractéristique $\chi_{u'}$ est $\chi_C(X)$.
        Par hypothèse de récurrence, comme $\chi_{u'}$ est scindé, il existe une base $\mathcal{B}_F = (e_2, \dots, e_n)$ de $F$ dans laquelle la matrice de $u'$ est triangulaire supérieure $T'$.
        Alors, dans la base $\mathcal{B} = (e_1, e_2, \dots, e_n)$, la matrice de $u$ sera triangulaire supérieure. (Des détails techniques sur la construction de la base sont nécessaires pour être rigoureux). $\Box$
    </div>

    <div class="corollary">
        <strong>Corollaire (Trigonalisabilité sur $\C$)</strong><br>
        Sur $\K = \C$, le théorème de d'Alembert-Gauss affirme que tout polynôme non constant est scindé.
        Par conséquent, <strong>tout endomorphisme d'un $\C$-espace vectoriel de dimension finie est trigonalisable</strong>. Toute matrice $A \in \mathcal{M}_n(\C)$ est trigonalisable sur $\C$.
    </div>

    <div class="definition">
        <strong>Définition (Matrice trigonalisable)</strong><br>
        Une matrice $A \in \MnK$ est dite <strong>trigonalisable</strong> sur $\K$ si l'endomorphisme $u_A$ canoniquement associé est trigonalisable.
    </div>

    <div class="proposition">
        <strong>Proposition (Caractérisation matricielle de la trigonalisabilité)</strong><br>
        Une matrice $A \in \MnK$ est trigonalisable sur $\K$ si et seulement si $A$ est <strong>semblable</strong> à une matrice triangulaire supérieure $T \in \MnK$.
        $$ A \text{ trigonalisable} \iff \exists P \in \GLnK, \exists T \text{ triangulaire supérieure}, A = P T P^{-1} $$
        La diagonale de $T$ contient alors les valeurs propres de $A$ (répétées avec leur multiplicité algébrique).
    </div>

    <div class="remark">
        <strong>Conséquences : Trace et Déterminant (confirmé)</strong><br>
        Si $A \in \MnK$ est trigonalisable sur $\K$ (ce qui est toujours le cas si $\K=\C$), alors :
        <ul>
            <li>$\Tr(A) = \sum_{\lambda \in \Sp(A)} \mult(\lambda) \lambda$ (somme des valeurs propres avec multiplicités).</li>
            <li>$\det(A) = \prod_{\lambda \in \Sp(A)} \lambda^{\mult(\lambda)}$ (produit des valeurs propres avec multiplicités).</li>
        </ul>
        Cela découle du fait que $A$ est semblable à une matrice triangulaire $T$, $\Tr(A)=\Tr(T)$, $\det(A)=\det(T)$, et la trace/déterminant de $T$ sont la somme/produit de ses éléments diagonaux, qui sont les valeurs propres $\lambda_i$ répétées $\mult(\lambda_i)$ fois.
    </div>


</body>
</html>
