<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>Cours de Calcul Différentiel (Math Spé)</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
    <style>
        body { font-family: sans-serif; line-height: 1.6; margin: 20px; }
        h1, h2, h3 { color: #0056b3; }
        .definition, .theorem, .proposition, .corollary, .remark, .example {
            margin: 15px 0; padding: 10px; border-left: 5px solid;
        }
        .definition { border-color: #17a2b8; background-color: #e1f5fe; }
        .theorem { border-color: #dc3545; background-color: #ffebee; }
        .proposition { border-color: #28a745; background-color: #e8f5e9; }
        .corollary { border-color: #ffc107; background-color: #fffde7; }
        .remark { border-color: #6c757d; background-color: #f8f9fa; }
        .example { border-color: #fd7e14; background-color: #fff3e0; }
        .proof { margin: 10px 0 10px 20px; padding: 10px; border: 1px dashed #ccc; background-color: #f9f9f9; }
        .proof-title { font-weight: bold; }
        code { background-color: #eee; padding: 2px 4px; border-radius: 3px; }
        .katex-display { overflow-x: auto; overflow-y: hidden; } /* Pour les formules longues */
    </style>
</head>
<body>

<h1>Calcul Différentiel</h1>

<h2>Préambule et Notations</h2>

<p>
    Dans tout ce cours, $E$ désigne un $\mathbb{R}$-espace vectoriel normé de dimension finie $p$, et $F$ désigne un $\mathbb{R}$-espace vectoriel normé de dimension finie $n$.
    Les normes sur $E$ et $F$ sont notées $\|\cdot\|$. Puisque les espaces sont de dimension finie, toutes les normes sont équivalentes et la topologie ne dépend pas du choix de la norme.
    On fixe une base $(e_1, \dots, e_p)$ de $E$ et une base $(u_1, \dots, u_n)$ de $F$.
    Un vecteur $h \in E$ s'écrira $h = \sum_{i=1}^p h_i e_i$.
</p>
<p>
    Soit $U$ un ouvert de $E$ et $f: U \to F$ une application. On peut écrire $f$ en termes de ses fonctions coordonnées dans la base $(u_1, \dots, u_n)$ :
    $$ f(x) = \sum_{i=1}^n f_i(x) u_i $$
    où $f_i: U \to \mathbb{R}$ pour $i=1, \dots, n$. On notera aussi $f = (f_1, \dots, f_n)$.
</p>

<h2>1. Dérivées suivant un vecteur, dérivées partielles</h2>

<div class="definition">
    <strong>Définition (Dérivée suivant un vecteur).</strong>
    Soit $f: U \to F$, $a \in U$ et $v \in E$. On dit que $f$ admet une dérivée suivant le vecteur $v$ en $a$ si l'application $\phi: t \mapsto f(a + tv)$ (définie pour $t$ dans un voisinage de $0$ tel que $a+tv \in U$) est dérivable en $0$.
    Dans ce cas, on note $D_v f(a)$ cette dérivée :
    $$ D_v f(a) = \phi'(0) = \lim_{t \to 0} \frac{f(a + tv) - f(a)}{t} $$
    $D_v f(a)$ est un vecteur de $F$.
</div>

<div class="definition">
    <strong>Définition (Dérivées partielles).</strong>
    Soit $f: U \to F$ et $a \in U$. Pour $1 \le i \le p$, on appelle $i$-ème dérivée partielle de $f$ en $a$ la dérivée de $f$ en $a$ suivant le vecteur de base $e_i$. On la note $\frac{\partial f}{\partial x_i}(a)$ ou $\partial_i f(a)$ :
    $$ \frac{\partial f}{\partial x_i}(a) = \partial_i f(a) = D_{e_i} f(a) = \lim_{t \to 0} \frac{f(a + t e_i) - f(a)}{t} $$
    Chaque $\frac{\partial f}{\partial x_i}(a)$ est un vecteur de $F$.
</div>

<div class="proposition">
    <strong>Proposition.</strong>
    Soit $f = (f_1, \dots, f_n): U \to F$ et $a \in U$, $v \in E$.
    $f$ admet une dérivée suivant $v$ en $a$ si et seulement si toutes ses fonctions coordonnées $f_k: U \to \mathbb{R}$ ($1 \le k \le n$) admettent une dérivée suivant $v$ en $a$. Dans ce cas,
    $$ D_v f(a) = \sum_{k=1}^n (D_v f_k(a)) u_k = (D_v f_1(a), \dots, D_v f_n(a)) $$
    En particulier,
    $$ \frac{\partial f}{\partial x_i}(a) = \left( \frac{\partial f_1}{\partial x_i}(a), \dots, \frac{\partial f_n}{\partial x_i}(a) \right) $$
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    Soit $\phi(t) = f(a+tv)$. On a $\phi(t) = \sum_{k=1}^n f_k(a+tv) u_k$.
    L'application $\phi$ est dérivable en $0$ si et seulement si chacune de ses composantes $t \mapsto f_k(a+tv)$ est dérivable en $0$.
    Par définition, la dérivée de $t \mapsto f_k(a+tv)$ en $0$ est $D_v f_k(a)$.
    Si ces dérivées existent, alors $\phi'(0) = \sum_{k=1}^n (D_v f_k(a)) u_k$.
    Le cas des dérivées partielles s'obtient en prenant $v = e_i$. $\square$
</div>

<div class="example">
    <strong>Exemple :</strong> Soit $E = \mathbb{R}^2$, $F = \mathbb{R}^2$. $f(x, y) = (x^2 y, x+y^3)$. Soit $a=(1, 2)$ et $v=(v_1, v_2)$.
    Calculons $D_v f(a)$. On a $f_1(x,y) = x^2 y$ et $f_2(x,y) = x+y^3$.
    $a+tv = (1+tv_1, 2+tv_2)$.
    $f_1(a+tv) = (1+tv_1)^2 (2+tv_2) = (1+2tv_1 + t^2 v_1^2)(2+tv_2) = 2 + tv_2 + 4tv_1 + O(t^2)$.
    $\frac{f_1(a+tv) - f_1(a)}{t} = \frac{2 + t(v_2+4v_1) + O(t^2) - 2}{t} = v_2+4v_1 + O(t)$.
    Donc $D_v f_1(a) = \lim_{t \to 0} \frac{f_1(a+tv) - f_1(a)}{t} = 4v_1 + v_2$.
    $f_2(a+tv) = (1+tv_1) + (2+tv_2)^3 = 1+tv_1 + (8 + 3 \cdot 4 \cdot tv_2 + O(t^2)) = 9 + t v_1 + 12 t v_2 + O(t^2)$.
    $\frac{f_2(a+tv) - f_2(a)}{t} = \frac{9 + t(v_1+12v_2) + O(t^2) - 9}{t} = v_1+12v_2 + O(t)$.
    Donc $D_v f_2(a) = \lim_{t \to 0} \frac{f_2(a+tv) - f_2(a)}{t} = v_1 + 12v_2$.
    Finalement, $D_v f(1, 2) = (4v_1 + v_2, v_1 + 12v_2)$.

    Calculons les dérivées partielles en $a=(1, 2)$ :
    $\frac{\partial f}{\partial x}(1, 2) = D_{e_1} f(1, 2)$. Ici $e_1 = (1, 0)$, donc $v_1=1, v_2=0$.
    $D_{e_1} f(1, 2) = (4(1) + 0, 1 + 12(0)) = (4, 1)$.
    $\frac{\partial f}{\partial y}(1, 2) = D_{e_2} f(1, 2)$. Ici $e_2 = (0, 1)$, donc $v_1=0, v_2=1$.
    $D_{e_2} f(1, 2) = (4(0) + 1, 0 + 12(1)) = (1, 12)$.
    On peut vérifier : $\frac{\partial f_1}{\partial x} = 2xy$, $\frac{\partial f_1}{\partial y} = x^2$. En $(1, 2)$, cela donne $4$ et $1$.
    $\frac{\partial f_2}{\partial x} = 1$, $\frac{\partial f_2}{\partial y} = 3y^2$. En $(1, 2)$, cela donne $1$ et $12$.
    On retrouve bien $\frac{\partial f}{\partial x}(1, 2) = (4, 1)$ et $\frac{\partial f}{\partial y}(1, 2) = (1, 12)$.
</div>

<h2>2. Différentielle</h2>

<div class="definition">
    <strong>Définition (Différentiabilité).</strong>
    Soit $f: U \to F$ et $a \in U$. On dit que $f$ est différentiable en $a$ s'il existe une application linéaire $L: E \to F$ et une application $\varepsilon: V \to F$ définies sur un voisinage $V$ de $0$ dans $E$ telles que :
    <ol>
        <li>Pour tout $h \in V$ tel que $a+h \in U$, on a :
            $$ f(a+h) = f(a) + L(h) + \|h\| \varepsilon(h) $$
        </li>
        <li>$\lim_{h \to 0} \varepsilon(h) = 0$.</li>
    </ol>
    Si une telle application linéaire $L$ existe, elle est unique. On l'appelle la différentielle de $f$ en $a$ et on la note $df_a$ ou $df(a)$.
    L'application $df_a$ appartient à $\mathcal{L}(E, F)$, l'espace des applications linéaires continues de $E$ dans $F$. (En dimension finie, toute application linéaire est continue).
    On dit que $f$ est différentiable sur $U$ si elle est différentiable en tout point de $U$.
</div>

<div class="proof">
    <span class="proof-title">Preuve (Unicité de L) :</span>
    Supposons qu'il existe deux applications linéaires $L_1, L_2 \in \mathcal{L}(E, F)$ et deux fonctions $\varepsilon_1, \varepsilon_2$ vérifiant la définition.
    Alors $f(a+h) = f(a) + L_1(h) + \|h\| \varepsilon_1(h)$ et $f(a+h) = f(a) + L_2(h) + \|h\| \varepsilon_2(h)$.
    En soustrayant, on obtient $L_1(h) - L_2(h) = \|h\| (\varepsilon_2(h) - \varepsilon_1(h))$.
    Soit $L = L_1 - L_2$. $L$ est linéaire. On a $L(h) = \|h\| \delta(h)$ avec $\delta(h) = \varepsilon_2(h) - \varepsilon_1(h)$, et $\lim_{h \to 0} \delta(h) = 0$.
    Fixons $v \in E$, $v \neq 0$. Pour $t \in \mathbb{R}^*$ assez petit, $h = tv \in V$.
    $L(tv) = \|tv\| \delta(tv) \implies t L(v) = |t| \|v\| \delta(tv)$.
    Pour $t \neq 0$, $L(v) = \frac{|t|}{t} \|v\| \delta(tv)$.
    Quand $t \to 0$, $\delta(tv) \to 0$. Donc $L(v) = 0$.
    Ceci étant vrai pour tout $v \in E$, on a $L = 0$, c'est-à-dire $L_1 = L_2$. $\square$
</div>

<div class="remark">
    <strong>Remarque (Notation petit o).</strong>
    La condition $\|h\| \varepsilon(h)$ avec $\lim_{h \to 0} \varepsilon(h) = 0$ signifie que $\|h\| \varepsilon(h)$ est négligeable devant $\|h\|$ quand $h \to 0$. On écrit souvent cette condition sous la forme :
    $$ f(a+h) = f(a) + L(h) + o(\|h\|) $$
    Attention, le terme $o(\|h\|)$ est une fonction de $h$ à valeurs dans $F$.
</div>

<div class="proposition">
    <strong>Proposition (Propriétés de la différentielle).</strong>
    Si $f: U \to F$ est différentiable en $a \in U$, alors :
    <ol>
        <li>$f$ est continue en $a$.</li>
        <li>$f$ admet en $a$ une dérivée suivant tout vecteur $v \in E$.</li>
        <li>Pour tout $v \in E$, on a $D_v f(a) = df_a(v)$.</li>
    </ol>
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    Soit $f(a+h) = f(a) + df_a(h) + \|h\| \varepsilon(h)$ avec $\varepsilon(h) \to 0$ quand $h \to 0$.
    <ol>
        <li>Continuité : $f(a+h) - f(a) = df_a(h) + \|h\| \varepsilon(h)$.
            Comme $df_a$ est linéaire en dimension finie, elle est continue. Donc $\lim_{h \to 0} df_a(h) = df_a(0) = 0$.
            De plus, $\lim_{h \to 0} \|h\| \varepsilon(h) = 0$.
            Donc $\lim_{h \to 0} (f(a+h) - f(a)) = 0$, ce qui signifie que $f$ est continue en $a$.
        </li>
        <li>Existence de $D_v f(a)$ : Soit $v \in E$. Pour $t \in \mathbb{R}^*$ assez petit, posons $h = tv$.
            $f(a+tv) = f(a) + df_a(tv) + \|tv\| \varepsilon(tv)$.
            Comme $df_a$ est linéaire, $df_a(tv) = t df_a(v)$.
            $\frac{f(a+tv) - f(a)}{t} = \frac{t df_a(v) + |t| \|v\| \varepsilon(tv)}{t} = df_a(v) + \frac{|t|}{t} \|v\| \varepsilon(tv)$.
            Quand $t \to 0$, $\varepsilon(tv) \to 0$. Le terme $\frac{|t|}{t}$ est borné (vaut $\pm 1$). Donc $\frac{|t|}{t} \|v\| \varepsilon(tv) \to 0$.
            Ainsi, $\lim_{t \to 0} \frac{f(a+tv) - f(a)}{t}$ existe et vaut $df_a(v)$.
        </li>
        <li>$D_v f(a) = df_a(v)$ : Cela découle directement du calcul précédent. $\square$
        </li>
    </ol>
</div>

<div class="proposition">
    <strong>Proposition (Expression de la différentielle avec les dérivées partielles).</strong>
    Si $f: U \to F$ est différentiable en $a \in U$, alors les $p$ dérivées partielles $\frac{\partial f}{\partial x_i}(a)$ existent. De plus, pour tout $h = \sum_{i=1}^p h_i e_i \in E$, on a :
    $$ df_a(h) = \sum_{i=1}^p h_i \frac{\partial f}{\partial x_i}(a) $$
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    L'existence des dérivées partielles découle de la proposition précédente, car $\frac{\partial f}{\partial x_i}(a) = D_{e_i} f(a)$.
    De plus, $D_{e_i} f(a) = df_a(e_i)$.
    Comme $df_a$ est linéaire, pour $h = \sum_{i=1}^p h_i e_i$, on a :
    $$ df_a(h) = df_a\left(\sum_{i=1}^p h_i e_i\right) = \sum_{i=1}^p h_i df_a(e_i) = \sum_{i=1}^p h_i \frac{\partial f}{\partial x_i}(a) $$
    $\square$
</div>

<div class="definition">
    <strong>Définition (Matrice Jacobienne).</strong>
    Soit $f: U \to F$ différentiable en $a \in U$. On écrit $f = (f_1, \dots, f_n)$ dans la base $(u_1, \dots, u_n)$ de $F$. La matrice jacobienne de $f$ en $a$ est la matrice de l'application linéaire $df_a: E \to F$ dans les bases $(e_1, \dots, e_p)$ de $E$ et $(u_1, \dots, u_n)$ de $F$.
    Ses colonnes sont les coordonnées des vecteurs $\frac{\partial f}{\partial x_j}(a)$ dans la base $(u_1, \dots, u_n)$.
    La $j$-ème colonne est $\begin{pmatrix} \frac{\partial f_1}{\partial x_j}(a) \\ \vdots \\ \frac{\partial f_n}{\partial x_j}(a) \end{pmatrix}$.
    La matrice jacobienne est donc :
    $$ J_a f = \operatorname{Mat}_{(e_j), (u_i)}(df_a) = \begin{pmatrix}
    \frac{\partial f_1}{\partial x_1}(a) & \frac{\partial f_1}{\partial x_2}(a) & \cdots & \frac{\partial f_1}{\partial x_p}(a) \\
    \frac{\partial f_2}{\partial x_1}(a) & \frac{\partial f_2}{\partial x_2}(a) & \cdots & \frac{\partial f_2}{\partial x_p}(a) \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial f_n}{\partial x_1}(a) & \frac{\partial f_n}{\partial x_2}(a) & \cdots & \frac{\partial f_n}{\partial x_p}(a)
    \end{pmatrix} \in \mathcal{M}_{n, p}(\mathbb{R}) $$
    Autrement dit, $(J_a f)_{i, j} = \frac{\partial f_i}{\partial x_j}(a)$ pour $1 \le i \le n, 1 \le j \le p$.

    Si $h = \sum_{j=1}^p h_j e_j$ a pour vecteur colonne de coordonnées $H = \begin{pmatrix} h_1 \\ \vdots \\ h_p \end{pmatrix}$, alors le vecteur $df_a(h)$ a pour vecteur colonne de coordonnées $Y = J_a f \times H$.

    Lorsque $n=p$, la matrice jacobienne est carrée. Son déterminant $\det(J_a f)$ s'appelle le déterminant jacobien de $f$ en $a$.
</div>

<div class="example">
    <strong>Exemples :</strong>
    <ol>
        <li>Si $f: E \to F$ est une application linéaire, alors $f$ est différentiable en tout point $a \in E$.
            On a $f(a+h) = f(a) + f(h)$. Ici, $L(h) = f(h)$ et $\varepsilon(h) = 0$.
            Donc $df_a = f$ pour tout $a \in E$. La différentielle est constante égale à $f$.
            Sa matrice jacobienne est la matrice de $f$ dans les bases choisies, indépendante de $a$.
        </li>
        <li>Si $f: U \to F$ est une application constante, $f(x) = c$ pour tout $x \in U$.
            Alors $f(a+h) = c = f(a) + 0 + 0$. Ici $L(h) = 0$ (l'application linéaire nulle) et $\varepsilon(h) = 0$.
            Donc $df_a = 0 \in \mathcal{L}(E, F)$ pour tout $a \in U$.
            Sa matrice jacobienne est la matrice nulle.
        </li>
    </ol>
</div>

<div class="proposition">
    <strong>Proposition (Cas E = R).</strong>
    Si $E = \mathbb{R}$ (donc $p=1$), $U$ est un ouvert de $\mathbb{R}$ (une réunion d'intervalles ouverts). Soit $f: U \to F$.
    $f$ est différentiable en $a \in U$ si et seulement si $f$ est dérivable en $a$ au sens usuel des fonctions vectorielles d'une variable réelle.
    Dans ce cas, $f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h} \in F$.
    La différentielle $df_a: \mathbb{R} \to F$ est l'application linéaire définie par $df_a(h) = h f'(a)$.
    En particulier, en prenant la base canonique $e_1 = 1$ de $\mathbb{R}$, on a $\frac{\partial f}{\partial x_1}(a) = D_1 f(a) = df_a(1) = 1 \cdot f'(a) = f'(a)$.
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    Si $f$ est différentiable en $a$, $f(a+h) = f(a) + df_a(h) + |h|\varepsilon(h)$ avec $\varepsilon(h) \to 0$.
    $df_a$ est une application linéaire de $\mathbb{R}$ dans $F$, donc il existe un vecteur $v \in F$ tel que $df_a(h) = h v$ pour tout $h \in \mathbb{R}$.
    $f(a+h) = f(a) + h v + |h|\varepsilon(h)$.
    $\frac{f(a+h) - f(a)}{h} = v + \frac{|h|}{h} \varepsilon(h)$.
    Quand $h \to 0$, $\varepsilon(h) \to 0$ et $|h|/h$ est borné, donc le deuxième terme tend vers 0.
    Ainsi $\lim_{h \to 0} \frac{f(a+h) - f(a)}{h} = v$. Donc $f$ est dérivable en $a$ et $f'(a) = v$. On a $df_a(h) = h f'(a)$.

    Réciproquement, si $f$ est dérivable en $a$, posons $v = f'(a) \in F$. Par définition,
    $\lim_{h \to 0} \frac{f(a+h) - f(a)}{h} = v$.
    Ceci signifie $\frac{f(a+h) - f(a)}{h} = v + \delta(h)$ avec $\lim_{h \to 0} \delta(h) = 0$.
    $f(a+h) - f(a) = h v + h \delta(h)$.
    Posons $L(h) = h v$. C'est une application linéaire $\mathbb{R} \to F$.
    Posons $\varepsilon(h) = \frac{h}{|h|} \delta(h)$ pour $h \neq 0$, et $\varepsilon(0) = 0$.
    Alors $h \delta(h) = |h| \varepsilon(h)$.
    On a $f(a+h) = f(a) + L(h) + |h| \varepsilon(h)$.
    De plus, $|\varepsilon(h)| = |\delta(h)|$, donc $\lim_{h \to 0} \varepsilon(h) = 0$.
    La norme sur $E=\mathbb{R}$ est $\|h\| = |h|$. Donc $f(a+h) = f(a) + L(h) + \|h\| \varepsilon(h)$.
    $f$ est différentiable en $a$ et $df_a = L$. $\square$
</div>

<h2>3. Opérations sur les applications différentiables</h2>

<p>Dans cette partie, $G$ et $H$ désignent des $\mathbb{R}$-espaces vectoriels normés de dimension finie.</p>

<div class="proposition">
    <strong>Proposition (Combinaison linéaire).</strong>
    Si $f: U \to F$ et $g: U \to F$ sont différentiables en $a \in U$, et $\lambda, \mu \in \mathbb{R}$, alors l'application $\lambda f + \mu g$ est différentiable en $a$ et
    $$ d(\lambda f + \mu g)_a = \lambda df_a + \mu dg_a $$
    L'ensemble des fonctions différentiables en $a$ à valeurs dans $F$ est un sous-espace vectoriel de l'espace des fonctions de $U$ dans $F$. L'application $f \mapsto df_a$ est linéaire.
</div>

<div class="proof">
    <span class="proof-title">Preuve (Sketch) :</span>
    $f(a+h) = f(a) + df_a(h) + \|h\|\varepsilon_f(h)$
    $g(a+h) = g(a) + dg_a(h) + \|h\|\varepsilon_g(h)$
    $(\lambda f + \mu g)(a+h) = \lambda f(a+h) + \mu g(a+h)$
    $= \lambda f(a) + \lambda df_a(h) + \lambda \|h\|\varepsilon_f(h) + \mu g(a) + \mu dg_a(h) + \mu \|h\|\varepsilon_g(h)$
    $= (\lambda f + \mu g)(a) + (\lambda df_a + \mu dg_a)(h) + \|h\| (\lambda \varepsilon_f(h) + \mu \varepsilon_g(h))$.
    Posons $L = \lambda df_a + \mu dg_a$. C'est une application linéaire $E \to F$.
    Posons $\varepsilon(h) = \lambda \varepsilon_f(h) + \mu \varepsilon_g(h)$. On a $\lim_{h \to 0} \varepsilon(h) = \lambda \cdot 0 + \mu \cdot 0 = 0$.
    Donc $\lambda f + \mu g$ est différentiable en $a$ de différentielle $L$. $\square$
</div>

<div class="proposition">
    <strong>Proposition (Application bilinéaire).</strong>
    Soit $f: U \to F$, $g: U \to G$ différentiables en $a \in U$. Soit $B: F \times G \to H$ une application bilinéaire.
    Alors l'application $B(f, g): U \to H$ définie par $x \mapsto B(f(x), g(x))$ est différentiable en $a$ et sa différentielle est donnée par :
    $$ d(B(f, g))_a(h) = B(df_a(h), g(a)) + B(f(a), dg_a(h)) $$
    Note: Toute application bilinéaire entre espaces de dimension finie est continue.
</div>

<div class="proof">
    <span class="proof-title">Preuve (Sketch) :</span>
    Notons $\phi(x) = B(f(x), g(x))$. On veut calculer $\phi(a+h) - \phi(a)$.
    $\phi(a+h) - \phi(a) = B(f(a+h), g(a+h)) - B(f(a), g(a))$
    $= B(f(a) + df_a(h) + o(\|h\|), g(a) + dg_a(h) + o(\|h\|)) - B(f(a), g(a))$
    Par bilinéarité de $B$:
    $= B(f(a), g(a)) + B(f(a), dg_a(h)) + B(f(a), o(\|h\|))$
    $+ B(df_a(h), g(a)) + B(df_a(h), dg_a(h)) + B(df_a(h), o(\|h\|))$
    $+ B(o(\|h\|), g(a)) + B(o(\|h\|), dg_a(h)) + B(o(\|h\|), o(\|h\|))$
    $- B(f(a), g(a))$
    $= \underbrace{B(df_a(h), g(a)) + B(f(a), dg_a(h))}_{L(h)} + \text{reste}$
    Le terme $L(h)$ est linéaire en $h$. Il faut montrer que le reste est un $o(\|h\|)$.
    Le reste est une somme de termes. Par exemple, $B(df_a(h), dg_a(h))$. Comme $df_a$ et $dg_a$ sont linéaires continues, $\|df_a(h)\| \le C_1 \|h\|$ et $\|dg_a(h)\| \le C_2 \|h\|$. Comme $B$ est bilinéaire continue, $\|B(u,v)\| \le C_B \|u\| \|v\|$.
    Donc $\|B(df_a(h), dg_a(h))\| \le C_B (C_1 \|h\|) (C_2 \|h\|) = C' \|h\|^2 = o(\|h\|)$.
    Les termes avec $o(\|h\|)$ (qui signifie $\|h\|\varepsilon(h)$ avec $\varepsilon \to 0$) sont aussi des $o(\|h\|)$ car $B$ est continue (donc bornée sur la boule unité) et $f(a), g(a), df_a(h)/\|h\|, dg_a(h)/\|h\|$ sont bornés pour $h$ proche de 0.
    Par exemple, $\|B(f(a), o(\|h\|))\| \le C_B \|f(a)\| \|o(\|h\|)\| = o(\|h\|)$. $\square$
</div>

<div class="remark">
    <strong>Généralisation (Application multilinéaire).</strong>
    Si $M: F_1 \times \dots \times F_k \to H$ est une application $k$-linéaire continue, et si $f_j: U \to F_j$ sont différentiables en $a$ pour $j=1, \dots, k$, alors l'application $x \mapsto M(f_1(x), \dots, f_k(x))$ est différentiable en $a$ et sa différentielle est :
    $$ d(M(f_1, \dots, f_k))_a(h) = \sum_{j=1}^k M(f_1(a), \dots, f_{j-1}(a), df_j(a)(h), f_{j+1}(a), \dots, f_k(a)) $$
</div>

<div class="proposition">
    <strong>Proposition (Composée d'applications différentiables - Règle de la chaîne).</strong>
    Soit $U \subset E$ un ouvert, $V \subset F$ un ouvert. Soit $f: U \to F$ telle que $f(U) \subset V$. Soit $g: V \to G$.
    Si $f$ est différentiable en $a \in U$ et si $g$ est différentiable en $b = f(a) \in V$, alors la composée $g \circ f: U \to G$ est différentiable en $a$ et
    $$ d(g \circ f)_a = dg_b \circ df_a $$
    où $b=f(a)$. C'est la composition de deux applications linéaires.
</div>

<div class="proof">
    <span class="proof-title">Preuve (Sketch) :</span>
    On a :
    $f(a+h) = f(a) + df_a(h) + \|h\|\varepsilon_f(h)$, avec $\varepsilon_f(h) \to 0$ quand $h \to 0$.
    $g(b+k) = g(b) + dg_b(k) + \|k\|\varepsilon_g(k)$, avec $\varepsilon_g(k) \to 0$ quand $k \to 0$.
    Posons $b = f(a)$ et $k = f(a+h) - f(a) = df_a(h) + \|h\|\varepsilon_f(h)$.
    Notons que $k \to 0$ quand $h \to 0$ (car $f$ est continue en $a$).
    Alors $(g \circ f)(a+h) = g(f(a+h)) = g(b+k)$.
    $g(b+k) = g(b) + dg_b(k) + \|k\| \varepsilon_g(k)$
    $= g(f(a)) + dg_b(df_a(h) + \|h\|\varepsilon_f(h)) + \|k\| \varepsilon_g(k)$
    $= (g \circ f)(a) + dg_b(df_a(h)) + dg_b(\|h\|\varepsilon_f(h)) + \|k\| \varepsilon_g(k)$
    Posons $L = dg_b \circ df_a$. C'est une application linéaire $E \to G$.
    On veut montrer que le reste $R(h) = dg_b(\|h\|\varepsilon_f(h)) + \|k\| \varepsilon_g(k)$ est un $o(\|h\|)$.
    $dg_b(\|h\|\varepsilon_f(h)) = \|h\| dg_b(\varepsilon_f(h))$. Comme $dg_b$ est continue et $\varepsilon_f(h) \to 0$, $dg_b(\varepsilon_f(h)) \to 0$. Donc ce terme est $o(\|h\|)$.
    Pour le second terme : $\|k\| = \|df_a(h) + \|h\|\varepsilon_f(h)\| \le \|df_a(h)\| + \|h\| \|\varepsilon_f(h)\|$.
    Comme $df_a$ est linéaire continue, $\|df_a(h)\| \le C \|h\|$. Donc $\|k\| \le (C + \|\varepsilon_f(h)\|) \|h\|$.
    Ceci montre que $\|k\|$ est un $O(\|h\|)$.
    Puisque $k \to 0$ quand $h \to 0$, on a $\varepsilon_g(k) \to 0$ quand $h \to 0$.
    Donc $\|k\| \varepsilon_g(k) = O(\|h\|) \times \varepsilon_g(k)$ où $\varepsilon_g(k) \to 0$. Ceci est bien un $o(\|h\|)$.
    Finalement, le reste $R(h)$ est $o(\|h\|)$, ce qui prouve le résultat. $\square$
</div>

<div class="remark">
    <strong>Traduction matricielle.</strong>
    En termes de matrices jacobiennes, la composition des différentielles se traduit par le produit des matrices :
    $$ J_a(g \circ f) = J_{f(a)} g \times J_a f $$
    C'est le produit de la matrice jacobienne de $g$ au point $f(a)$ (taille $m \times n$, si $G$ est de dimension $m$) par la matrice jacobienne de $f$ au point $a$ (taille $n \times p$). Le résultat est une matrice $m \times p$.
</div>

<div class="corollary">
    <strong>Corollaire (Dérivées partielles d'une composée).</strong>
    Avec les mêmes notations, si $f = (f_1, \dots, f_n)$ et $g = (g_1, \dots, g_m)$, et si on note $y = f(x)$ (donc $y_j = f_j(x_1, \dots, x_p)$), alors les dérivées partielles de $H = g \circ f$ par rapport aux variables $x_i$ sont données par :
    $$ \frac{\partial H_k}{\partial x_i}(a) = \sum_{j=1}^n \frac{\partial g_k}{\partial y_j}(f(a)) \frac{\partial f_j}{\partial x_i}(a) $$
    pour $1 \le k \le m$ et $1 \le i \le p$.
    Ceci correspond exactement à la formule du produit matriciel $J_a(g \circ f) = J_{f(a)} g \times J_a f$. L'élément $(k, i)$ de $J_a(g \circ f)$ est $\frac{\partial H_k}{\partial x_i}(a)$. L'élément $(k, j)$ de $J_{f(a)} g$ est $\frac{\partial g_k}{\partial y_j}(f(a))$. L'élément $(j, i)$ de $J_a f$ est $\frac{\partial f_j}{\partial x_i}(a)$.
</div>

<div class="corollary">
    <strong>Corollaire (Cas particuliers importants).</strong>
    <ol>
        <li>Si $\gamma: I \to U \subset E$ est une application définie sur un intervalle $I$ de $\mathbb{R}$, dérivable en $t_0 \in I$, et si $f: U \to F$ est différentiable en $a = \gamma(t_0)$, alors $f \circ \gamma: I \to F$ est dérivable en $t_0$ et :
            $$ (f \circ \gamma)'(t_0) = df_{\gamma(t_0)}(\gamma'(t_0)) $$
            Si $\gamma(t) = (\gamma_1(t), \dots, \gamma_p(t))$ dans la base $(e_1, \dots, e_p)$, alors
            $$ (f \circ \gamma)'(t_0) = \sum_{k=1}^p \gamma'_k(t_0) \frac{\partial f}{\partial x_k}(\gamma(t_0)) $$
            Ceci est parfois appelé "dérivée le long d'un arc".
        </li>
        <li>
            Soit $f: U \subset E \to F$ différentiable sur $U$. Soit $V \subset \mathbb{R}^m$ un ouvert et $\phi: V \to U$ une application différentiable sur $V$. On note $\phi(t_1, \dots, t_m) = (x_1(t_1, \dots, t_m), \dots, x_p(t_1, \dots, t_m))$.
            Alors la fonction $H = f \circ \phi: V \to F$ est différentiable sur $V$ et ses dérivées partielles sont données par :
            $$ \frac{\partial H}{\partial t_i}(t) = df_{\phi(t)} \left( \frac{\partial \phi}{\partial t_i}(t) \right) = \sum_{j=1}^p \frac{\partial f}{\partial x_j}(\phi(t)) \frac{\partial x_j}{\partial t_i}(t) $$
            pour $t = (t_1, \dots, t_m) \in V$ et $i=1, \dots, m$.
        </li>
    </ol>
</div>


<h2>4. Applications de classe $C^k$</h2>

<div class="definition">
    <strong>Définition (Application de classe $C^1$).</strong>
    On dit qu'une application $f: U \to F$ est de classe $C^1$ sur l'ouvert $U$ si elle est différentiable en tout point $a \in U$ et si l'application
    $$ df: U \to \mathcal{L}(E, F), \quad a \mapsto df_a $$
    est continue sur $U$.
    (Rappel : $\mathcal{L}(E, F)$ est l'espace vectoriel des applications linéaires de $E$ dans $F$. En dimension finie, il peut être muni d'une norme d'opérateur, et la topologie ne dépend pas du choix de cette norme).
</div>

<div class="theorem">
    <strong>Théorème (Caractérisation $C^1$).</strong>
    Soit $f: U \to F$. $f$ est de classe $C^1$ sur $U$ si et seulement si toutes les dérivées partielles $\frac{\partial f}{\partial x_i}: U \to F$ (pour $i=1, \dots, p$) existent et sont continues sur $U$.
    <!-- Démonstration en vidéo! -->
</div>

<div class="proof">
    <span class="proof-title">Idée de la preuve :</span>
    ($\Rightarrow$) Si $f$ est $C^1$, $a \mapsto df_a$ est continue. On a $\frac{\partial f}{\partial x_i}(a) = df_a(e_i)$. L'application $L \mapsto L(e_i)$ de $\mathcal{L}(E, F)$ dans $F$ est linéaire, donc continue (car dimension finie). Par composition, l'application $a \mapsto \frac{\partial f}{\partial x_i}(a)$ est continue.
    ($\Leftarrow$) C'est la partie difficile. On suppose que toutes les $\frac{\partial f}{\partial x_i}$ existent et sont continues sur $U$. Il faut montrer que $f$ est différentiable en tout $a \in U$ et que $a \mapsto df_a$ est continue.
    Pour $f = (f_1, \dots, f_n)$, il suffit de montrer le résultat pour chaque composante $f_k: U \to \mathbb{R}$. On peut donc supposer $F = \mathbb{R}$.
    Fixons $a \in U$. On utilise le théorème des accroissements finis pour les fonctions d'une variable réelle.
    On écrit $f(a+h) - f(a)$ comme une somme télescopique faisant intervenir les variations selon chaque coordonnée. Par exemple pour $p=2$, $h=(h_1, h_2)$:
    $f(a_1+h_1, a_2+h_2) - f(a_1, a_2) = [f(a_1+h_1, a_2+h_2) - f(a_1, a_2+h_2)] + [f(a_1, a_2+h_2) - f(a_1, a_2)]$.
    On applique le TAF à $x \mapsto f(x, a_2+h_2)$ entre $a_1$ et $a_1+h_1$, et à $y \mapsto f(a_1, y)$ entre $a_2$ et $a_2+h_2$.
    $f(a+h) - f(a) = h_1 \frac{\partial f}{\partial x_1}(c_1, a_2+h_2) + h_2 \frac{\partial f}{\partial x_2}(a_1, c_2)$, où $c_1$ est entre $a_1$ et $a_1+h_1$, $c_2$ entre $a_2$ et $a_2+h_2$.
    En utilisant la continuité des dérivées partielles en $a=(a_1, a_2)$, on a $\frac{\partial f}{\partial x_1}(c_1, a_2+h_2) = \frac{\partial f}{\partial x_1}(a) + \varepsilon_1(h)$ et $\frac{\partial f}{\partial x_2}(a_1, c_2) = \frac{\partial f}{\partial x_2}(a) + \varepsilon_2(h)$, avec $\varepsilon_1, \varepsilon_2 \to 0$ quand $h \to 0$.
    $f(a+h) - f(a) = h_1 \frac{\partial f}{\partial x_1}(a) + h_2 \frac{\partial f}{\partial x_2}(a) + h_1 \varepsilon_1(h) + h_2 \varepsilon_2(h)$.
    Le terme $L(h) = \sum h_i \frac{\partial f}{\partial x_i}(a)$ est linéaire. Le reste $R(h) = h_1 \varepsilon_1(h) + h_2 \varepsilon_2(h)$ est un $o(\|h\|)$ car $|h_i| \le \|h\|$ (si on utilise la norme sup, sinon $|h_i| \le C \|h\|$).
    Donc $f$ est différentiable en $a$ et $df_a(h) = \sum h_i \frac{\partial f}{\partial x_i}(a)$.
    La continuité de $a \mapsto df_a$ découle alors de la continuité des $a \mapsto \frac{\partial f}{\partial x_i}(a)$ et du fait que l'application $(\psi_1, \dots, \psi_p) \mapsto (h \mapsto \sum h_i \psi_i)$ de $F^p$ dans $\mathcal{L}(E, F)$ est linéaire continue. $\square$
</div>

<div class="remark">
    <strong>Opérations sur les fonctions de classe $C^1$.</strong>
    Les opérations algébriques et la composition préservent la classe $C^1$.
    <ul>
        <li>Si $f, g: U \to F$ sont $C^1$ et $\lambda, \mu \in \mathbb{R}$, alors $\lambda f + \mu g$ est $C^1$.</li>
        <li>Si $f: U \to F$ et $g: U \to G$ sont $C^1$, et $B: F \times G \to H$ est bilinéaire, alors $B(f, g)$ est $C^1$. (La formule de $d(B(f,g))_a$ montre que sa continuité découle de celle de $df_a, dg_a, f, g$).</li>
        <li>Si $f: U \to V$ est $C^1$ et $g: V \to G$ est $C^1$, alors $g \circ f: U \to G$ est $C^1$. (La formule $d(g \circ f)_a = dg_{f(a)} \circ df_a$ montre que sa continuité découle de la continuité de $a \mapsto f(a)$, $b \mapsto dg_b$, $a \mapsto df_a$ et de la continuité de la composition $(L_1, L_2) \mapsto L_1 \circ L_2$).</li>
    </ul>
</div>

<div class="theorem">
    <strong>Théorème (Intégration le long d'un chemin $C^1$).</strong>
    Soit $f: U \to F$ une application de classe $C^1$. Soit $\gamma: [0, 1] \to U$ une application de classe $C^1$ (un chemin $C^1$ dans $U$). Posons $a = \gamma(0)$ et $b = \gamma(1)$. Alors :
    $$ f(b) - f(a) = \int_0^1 df_{\gamma(t)}(\gamma'(t)) dt $$
    L'intégrale est une intégrale de Riemann de fonction vectorielle continue sur $[0, 1]$.
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    Considérons la fonction $\phi: [0, 1] \to F$ définie par $\phi(t) = f(\gamma(t))$.
    Comme $f$ est $C^1$ et $\gamma$ est $C^1$, la composée $\phi = f \circ \gamma$ est de classe $C^1$ sur $[0, 1]$.
    D'après la règle de la chaîne (Corollaire, cas 1), la dérivée de $\phi$ est :
    $$ \phi'(t) = (f \circ \gamma)'(t) = df_{\gamma(t)}(\gamma'(t)) $$
    Le Théorème Fondamental de l'Analyse pour les fonctions vectorielles d'une variable réelle nous dit que si $\phi$ est $C^1$ sur $[0, 1]$, alors :
    $$ \phi(1) - \phi(0) = \int_0^1 \phi'(t) dt $$
    En remplaçant $\phi(1)$ par $f(\gamma(1)) = f(b)$ et $\phi(0)$ par $f(\gamma(0)) = f(a)$, on obtient le résultat. $\square$
</div>

<div class="corollary">
    <strong>Corollaire (Caractérisation des fonctions constantes).</strong>
    Soit $U$ un ouvert connexe par arcs de $E$. Soit $f: U \to F$ une application de classe $C^1$.
    $f$ est constante sur $U$ si et seulement si $df_a = 0$ pour tout $a \in U$.
    (Ceci est équivalent à $\frac{\partial f}{\partial x_i}(a) = 0$ pour tout $a \in U$ et tout $i=1, \dots, p$).
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    ($\Rightarrow$) Si $f$ est constante, $f(x)=c$. Alors $df_a = 0$ pour tout $a$ (Exemple 2).
    ($\Leftarrow$) Supposons $df_a = 0$ pour tout $a \in U$. Soient $a, b \in U$. Comme $U$ est connexe par arcs, il existe un chemin $\gamma: [0, 1] \to U$, continu, tel que $\gamma(0)=a$ et $\gamma(1)=b$. On peut même montrer qu'il existe un chemin de classe $C^1$ (ou au moins $C^1$ par morceaux). Admettons qu'il existe un chemin $\gamma$ de classe $C^1$.
    D'après le théorème précédent :
    $$ f(b) - f(a) = \int_0^1 df_{\gamma(t)}(\gamma'(t)) dt $$
    Puisque $df_x = 0$ pour tout $x \in U$, en particulier $df_{\gamma(t)} = 0$ pour tout $t \in [0, 1]$.
    Donc l'intégrande est nulle : $df_{\gamma(t)}(\gamma'(t)) = 0(\gamma'(t)) = 0$.
    L'intégrale est donc nulle, $f(b) - f(a) = 0$, soit $f(b) = f(a)$.
    Ceci étant vrai pour tous $a, b \in U$, $f$ est constante sur $U$. $\square$
</div>

<div class="definition">
    <strong>Définition (Dérivées partielles d'ordre supérieur).</strong>
    Soit $f: U \to F$. Si les dérivées partielles $\frac{\partial f}{\partial x_j}$ existent sur $U$, on peut chercher à calculer leurs propres dérivées partielles.
    Si $\frac{\partial}{\partial x_i} \left( \frac{\partial f}{\partial x_j} \right)$ existe en un point $a \in U$, on la note $\frac{\partial^2 f}{\partial x_i \partial x_j}(a)$ ou $\partial_i \partial_j f(a)$. C'est une dérivée partielle seconde.
    Plus généralement, pour $k \ge 2$, on définit par récurrence les dérivées partielles d'ordre $k$ comme les dérivées partielles des dérivées partielles d'ordre $k-1$.
    Pour un multi-indice $(i_1, \dots, i_k) \in \{1, \dots, p\}^k$, on note :
    $$ \frac{\partial^k f}{\partial x_{i_1} \dots \partial x_{i_k}} = \frac{\partial}{\partial x_{i_1}} \left( \frac{\partial}{\partial x_{i_2}} \left( \dots \left( \frac{\partial f}{\partial x_{i_k}} \right) \dots \right) \right) $$
</div>

<div class="definition">
    <strong>Définition (Application de classe $C^k$).</strong>
    Soit $k \ge 1$ un entier. On dit qu'une application $f: U \to F$ est de classe $C^k$ sur l'ouvert $U$ si elle admet sur $U$ toutes les dérivées partielles possibles jusqu'à l'ordre $k$, et si ces dérivées partielles sont toutes continues sur $U$.
    On note $C^k(U, F)$ l'ensemble de ces fonctions.
    Une fonction est dite de classe $C^\infty$ sur $U$ si elle est de classe $C^k$ pour tout $k \ge 1$.
</div>

<div class="theorem">
    <strong>Théorème de Schwarz (Symétrie des dérivées secondes).</strong>
    Si $f: U \to F$ est de classe $C^2$ sur l'ouvert $U$, alors pour tout $a \in U$ et pour tous $i, j \in \{1, \dots, p\}$, on a :
    $$ \frac{\partial^2 f}{\partial x_i \partial x_j}(a) = \frac{\partial^2 f}{\partial x_j \partial x_i}(a) $$
    <!-- Démonstration en vidéo! -->
</div>

<div class="proof">
    <span class="proof-title">Idée de la preuve (cas $F=\mathbb{R}, p=2$) :</span>
    On suppose $f: U \subset \mathbb{R}^2 \to \mathbb{R}$ de classe $C^2$. On veut montrer $\frac{\partial^2 f}{\partial x \partial y}(a) = \frac{\partial^2 f}{\partial y \partial x}(a)$.
    Soit $a=(x_0, y_0)$. Considérons la quantité $\Delta(h, k) = f(x_0+h, y_0+k) - f(x_0+h, y_0) - f(x_0, y_0+k) + f(x_0, y_0)$ pour $h, k$ petits non nuls.
    Posons $\phi_k(x) = f(x, y_0+k) - f(x, y_0)$. Alors $\Delta(h, k) = \phi_k(x_0+h) - \phi_k(x_0)$.
    Par le TAF appliqué à $\phi_k$ sur $[x_0, x_0+h]$, il existe $c_h \in ]x_0, x_0+h[$ tel que $\Delta(h, k) = h \phi'_k(c_h)$.
    $\phi'_k(x) = \frac{\partial f}{\partial x}(x, y_0+k) - \frac{\partial f}{\partial x}(x, y_0)$.
    Donc $\Delta(h, k) = h \left( \frac{\partial f}{\partial x}(c_h, y_0+k) - \frac{\partial f}{\partial x}(c_h, y_0) \right)$.
    Appliquons le TAF à la fonction $y \mapsto \frac{\partial f}{\partial x}(c_h, y)$ sur $[y_0, y_0+k]$. Il existe $d_k \in ]y_0, y_0+k[$ tel que :
    $\frac{\partial f}{\partial x}(c_h, y_0+k) - \frac{\partial f}{\partial x}(c_h, y_0) = k \frac{\partial}{\partial y} \left( \frac{\partial f}{\partial x} \right)(c_h, d_k) = k \frac{\partial^2 f}{\partial y \partial x}(c_h, d_k)$.
    Ainsi, $\frac{\Delta(h, k)}{hk} = \frac{\partial^2 f}{\partial y \partial x}(c_h, d_k)$.
    Comme $f$ est $C^2$, $\frac{\partial^2 f}{\partial y \partial x}$ est continue. Quand $(h, k) \to (0, 0)$, on a $(c_h, d_k) \to (x_0, y_0)$. Donc $\lim_{(h,k) \to (0,0)} \frac{\Delta(h, k)}{hk} = \frac{\partial^2 f}{\partial y \partial x}(x_0, y_0)$.

    Maintenant, posons $\psi_h(y) = f(x_0+h, y) - f(x_0, y)$. Alors $\Delta(h, k) = \psi_h(y_0+k) - \psi_h(y_0)$.
    Par le TAF pour $\psi_h$ sur $[y_0, y_0+k]$, il existe $d'_k \in ]y_0, y_0+k[$ tel que $\Delta(h, k) = k \psi'_h(d'_k)$.
    $\psi'_h(y) = \frac{\partial f}{\partial y}(x_0+h, y) - \frac{\partial f}{\partial y}(x_0, y)$.
    $\Delta(h, k) = k \left( \frac{\partial f}{\partial y}(x_0+h, d'_k) - \frac{\partial f}{\partial y}(x_0, d'_k) \right)$.
    Appliquons le TAF à $x \mapsto \frac{\partial f}{\partial y}(x, d'_k)$ sur $[x_0, x_0+h]$. Il existe $c'_h \in ]x_0, x_0+h[$ tel que :
    $\frac{\partial f}{\partial y}(x_0+h, d'_k) - \frac{\partial f}{\partial y}(x_0, d'_k) = h \frac{\partial}{\partial x} \left( \frac{\partial f}{\partial y} \right)(c'_h, d'_k) = h \frac{\partial^2 f}{\partial x \partial y}(c'_h, d'_k)$.
    Ainsi, $\frac{\Delta(h, k)}{hk} = \frac{\partial^2 f}{\partial x \partial y}(c'_h, d'_k)$.
    Par continuité de $\frac{\partial^2 f}{\partial x \partial y}$, on a $\lim_{(h,k) \to (0,0)} \frac{\Delta(h, k)}{hk} = \frac{\partial^2 f}{\partial x \partial y}(x_0, y_0)$.
    Par unicité de la limite, on conclut $\frac{\partial^2 f}{\partial y \partial x}(a) = \frac{\partial^2 f}{\partial x \partial y}(a)$. $\square$
</div>

<div class="corollary">
    <strong>Corollaire.</strong>
    Si $f$ est de classe $C^k$ sur un ouvert $U$, l'ordre de dérivation dans les dérivées partielles d'ordre $\le k$ n'importe pas. Par exemple, pour $f \in C^3$, $\frac{\partial^3 f}{\partial x \partial y \partial z} = \frac{\partial^3 f}{\partial y \partial z \partial x}$, etc.
</div>

<div class="remark">
    <strong>Opérations sur les fonctions de classe $C^k$.</strong>
    Les opérations algébriques et la composition préservent la classe $C^k$.
    <ul>
        <li>Si $f, g \in C^k(U, F)$ et $\lambda, \mu \in \mathbb{R}$, alors $\lambda f + \mu g \in C^k(U, F)$.</li>
        <li>Si $f \in C^k(U, F)$, $g \in C^k(U, G)$, et $B: F \times G \to H$ est bilinéaire, alors $B(f, g) \in C^k(U, H)$.</li>
        <li>Si $f: U \to V$ est $C^k$ et $g: V \to G$ est $C^k$, alors $g \circ f: U \to G$ est $C^k$.</li>
    </ul>
    La preuve se fait par récurrence sur $k$, en utilisant les règles de dérivation et le fait que les opérations préservent la continuité.
</div>


<h2>5. Vecteur gradient</h2>

<p>
    Dans cette partie, on suppose que $E$ est un espace euclidien, c'est-à-dire un $\mathbb{R}$-espace vectoriel de dimension finie $p$ muni d'un produit scalaire $\langle \cdot, \cdot \rangle$. La norme associée est $\|x\| = \sqrt{\langle x, x \rangle}$.
    On considère une fonction $f: U \to \mathbb{R}$ (à valeurs réelles, $F=\mathbb{R}$).
</p>
<p>
    On rappelle le théorème de représentation de Riesz : pour tout forme linéaire $\phi \in E^*$ (c'est-à-dire $\phi: E \to \mathbb{R}$ linéaire), il existe un unique vecteur $u \in E$ tel que, pour tout $x \in E$, $\phi(x) = \langle u, x \rangle$.
</p>

<div class="theorem">
    <strong>Théorème (Vecteur gradient).</strong>
    Si $f: U \subset E \to \mathbb{R}$ est différentiable en $a \in U$, alors sa différentielle $df_a$ est une forme linéaire sur $E$ ($df_a \in E^*$). Il existe donc un unique vecteur de $E$, noté $\nabla f(a)$ (ou $\operatorname{grad} f(a)$), tel que pour tout $h \in E$:
    $$ df_a(h) = \langle \nabla f(a), h \rangle $$
    Le vecteur $\nabla f(a) \in E$ s'appelle le vecteur gradient de $f$ en $a$.
</div>

<div class="proposition">
    <strong>Proposition (Expression du gradient en base orthonormée).</strong>
    Si $(e_1, \dots, e_p)$ est une base orthonormée de $E$, alors le gradient de $f$ en $a$ s'exprime par :
    $$ \nabla f(a) = \sum_{k=1}^p \frac{\partial f}{\partial x_k}(a) e_k $$
    Autrement dit, les coordonnées du vecteur gradient dans une base orthonormée sont les dérivées partielles de $f$ par rapport aux variables correspondantes.
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    Soit $h = \sum_{k=1}^p h_k e_k$. D'après la proposition sur l'expression de la différentielle, on a
    $df_a(h) = \sum_{k=1}^p h_k \frac{\partial f}{\partial x_k}(a)$.
    D'autre part, si on pose $u = \sum_{j=1}^p u_j e_j$, alors $\langle u, h \rangle = \left\langle \sum_j u_j e_j, \sum_k h_k e_k \right\rangle$.
    Comme la base est orthonormée, $\langle e_j, e_k \rangle = \delta_{jk}$ (symbole de Kronecker).
    Donc $\langle u, h \rangle = \sum_{j, k} u_j h_k \langle e_j, e_k \rangle = \sum_{k=1}^p u_k h_k$.
    On cherche l'unique vecteur $u = \nabla f(a)$ tel que $df_a(h) = \langle u, h \rangle$ pour tout $h$.
    En identifiant les deux expressions $\sum_{k=1}^p h_k \frac{\partial f}{\partial x_k}(a)$ et $\sum_{k=1}^p u_k h_k$, on voit que cela doit être vrai pour tout choix des $h_k$. En prenant $h=e_j$ ($h_j=1$, $h_k=0$ pour $k \neq j$), on obtient $u_j = \frac{\partial f}{\partial x_j}(a)$.
    Donc $u = \sum_{j=1}^p \frac{\partial f}{\partial x_j}(a) e_j$. C'est le vecteur $\nabla f(a)$. $\square$
</div>

<div class="theorem">
    <strong>Théorème (Direction de la plus grande pente).</strong>
    Soit $f: U \to \mathbb{R}$ différentiable en $a \in U$. Si $\nabla f(a) \neq 0$, alors l'application
    $$ \varphi: S \to \mathbb{R}, \quad u \mapsto D_u f(a) = df_a(u) = \langle \nabla f(a), u \rangle $$
    où $S = \{u \in E \mid \|u\| = 1\}$ est la sphère unité, admet un maximum unique atteint pour le vecteur $u_0 = \frac{\nabla f(a)}{\|\nabla f(a)\|}$.
    Ce maximum vaut $\|\nabla f(a)\|$.
    Le minimum est atteint en $-u_0$ et vaut $-\|\nabla f(a)\|$.
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    On a $D_u f(a) = \langle \nabla f(a), u \rangle$.
    Par l'inégalité de Cauchy-Schwarz, $|\langle \nabla f(a), u \rangle| \le \|\nabla f(a)\| \|u\|$.
    Si $u \in S$, alors $\|u\|=1$, donc $|D_u f(a)| \le \|\nabla f(a)\|$.
    L'égalité est atteinte si et seulement si $u$ et $\nabla f(a)$ sont colinéaires.
    Comme $\nabla f(a) \neq 0$, il existe $\lambda \in \mathbb{R}$ tel que $u = \lambda \nabla f(a)$.
    Puisque $\|u\|=1$, on a $\|\lambda \nabla f(a)\| = 1 \implies |\lambda| \|\nabla f(a)\| = 1 \implies \lambda = \pm \frac{1}{\|\nabla f(a)\|}$.
    Si $\lambda = + \frac{1}{\|\nabla f(a)\|}$, alors $u = u_0 = \frac{\nabla f(a)}{\|\nabla f(a)\|}$. Dans ce cas,
    $D_{u_0} f(a) = \langle \nabla f(a), u_0 \rangle = \left\langle \nabla f(a), \frac{\nabla f(a)}{\|\nabla f(a)\|} \right\rangle = \frac{1}{\|\nabla f(a)\|} \langle \nabla f(a), \nabla f(a) \rangle = \frac{\|\nabla f(a)\|^2}{\|\nabla f(a)\|} = \|\nabla f(a)\|$.
    C'est la valeur maximale possible.
    Si $\lambda = - \frac{1}{\|\nabla f(a)\|}$, alors $u = -u_0$. Dans ce cas,
    $D_{-u_0} f(a) = \langle \nabla f(a), -u_0 \rangle = - \langle \nabla f(a), u_0 \rangle = - \|\nabla f(a)\|$.
    C'est la valeur minimale possible.
    Le maximum est donc $\|\nabla f(a)\|$, atteint uniquement en $u_0$. $\square$
</div>
<div class="remark">
Le gradient $\nabla f(a)$ indique la direction dans laquelle $f$ augmente le plus rapidement au point $a$. Sa norme $\|\nabla f(a)\|$ est le taux maximal d'augmentation.
</div>

<h2>6. Espace tangent</h2>

<div class="definition">
    <strong>Définition (Vecteur tangent).</strong>
    Soit $X$ une partie de $E$, $x$ un point de $X$ et $v$ un vecteur de $E$. On dit que $v$ est tangent à $X$ en $x$ s'il existe $\varepsilon > 0$ et un arc $\gamma: ]-\varepsilon, \varepsilon[ \to X$ dérivable en $0$ tel que $\gamma(0) = x$ et $\gamma'(0) = v$.
    On note $T_x X$ l'ensemble des vecteurs tangents à $X$ en $x$. $T_x X$ est un sous-ensemble de $E$.
</div>
<div class="remark">
Si $X$ est un ouvert de $E$, alors $T_x X = E$ pour tout $x \in X$.
Si $X = \{x_0\}$ est un singleton, $T_{x_0} X = \{0\}$.
On peut montrer que $T_x X$ est un cône (si $v \in T_x X$ et $\lambda \ge 0$, alors $\lambda v \in T_x X$) mais pas nécessairement un sous-espace vectoriel en général. Cependant, dans les cas usuels (variétés, sous-variétés), $T_x X$ sera un sous-espace vectoriel.
</div>

<div class="theorem">
    <strong>Théorème (Espace tangent à une hypersurface).</strong>
    Soit $U$ un ouvert de $E$, $g: U \to \mathbb{R}$ une application de classe $C^1$. Soit $X = \{x \in U \mid g(x) = c\}$ un ensemble de niveau de $g$ (souvent $c=0$).
    Soit $x \in X$. Si la différentielle $dg_x \in \mathcal{L}(E, \mathbb{R})$ n'est pas l'application nulle (i.e., $dg_x \neq 0$), alors $T_x X$ est un sous-espace vectoriel de $E$ donné par :
    $$ T_x X = \ker dg_x = \{v \in E \mid dg_x(v) = 0 \} $$
    C'est un hyperplan vectoriel de $E$ (car $dg_x$ est une forme linéaire non nulle).

    En particulier, si $E$ est euclidien, comme $dg_x(v) = \langle \nabla g(x), v \rangle$, la condition $dg_x \neq 0$ équivaut à $\nabla g(x) \neq 0$. Dans ce cas :
    $$ T_x X = \{v \in E \mid \langle \nabla g(x), v \rangle = 0 \} = (\nabla g(x))^\perp $$
    $T_x X$ est l'hyperplan orthogonal au vecteur gradient de $g$ en $x$.
</div>

<div class="proof">
    <span class="proof-title">Preuve (Inclusion $T_x X \subset \ker dg_x$) :</span>
    Soit $v \in T_x X$. Par définition, il existe $\gamma: ]-\varepsilon, \varepsilon[ \to X$ dérivable en 0 avec $\gamma(0)=x$ et $\gamma'(0)=v$.
    Comme $\gamma(t) \in X$ pour tout $t$, on a $g(\gamma(t)) = c$ (constante) pour tout $t \in ]-\varepsilon, \varepsilon[$.
    Considérons la fonction $\phi(t) = g(\gamma(t))$. Elle est constante, donc sa dérivée est nulle : $\phi'(t) = 0$ pour tout $t$.
    En particulier, $\phi'(0) = 0$.
    Par la règle de la chaîne (dérivée le long d'un arc),
    $\phi'(0) = (g \circ \gamma)'(0) = dg_{\gamma(0)}(\gamma'(0)) = dg_x(v)$.
    Donc $dg_x(v) = 0$, ce qui signifie $v \in \ker dg_x$.
    On a montré $T_x X \subset \ker dg_x$.

    L'autre inclusion, $\ker dg_x \subset T_x X$, est plus difficile et repose typiquement sur le Théorème des Fonctions Implicites, qui garantit que localement, $X$ est difféomorphe à un ouvert de $\ker dg_x$. Cela dépasse le cadre d'une simple preuve ici, mais le résultat est fondamental. Il assure que $\dim(T_x X) = \dim(\ker dg_x) = p-1$. $\square$
</div>

<div class="definition">
    <strong>Définition (Espace tangent affine).</strong>
    Soit $X$ une partie de $E$ et $x \in X$. On appelle espace tangent (affine) à $X$ en $x$ l'ensemble
    $$ \mathcal{T}_x X = x + T_x X = \{ x + v \mid v \in T_x X \} $$
    Si $T_x X$ est un sous-espace vectoriel (comme dans le théorème précédent), $\mathcal{T}_x X$ est un sous-espace affine de $E$ passant par $x$ et de direction $T_x X$.
    Si $X$ est une hypersurface définie par $g(y)=c$ avec $\nabla g(x) \neq 0$ dans un espace euclidien, l'espace tangent affine est l'hyperplan affine passant par $x$ et orthogonal à $\nabla g(x)$. Son équation est :
    $$ \langle y - x, \nabla g(x) \rangle = 0 $$
</div>

<div class="corollary">
    <strong>Corollaire (Plan tangent à un graphe).</strong>
    Soit $U \subset \mathbb{R}^2$ un ouvert, $f: U \to \mathbb{R}$ une fonction différentiable sur $U$.
    Soit $X = \{ (x, y, z) \in \mathbb{R}^3 \mid (x, y) \in U, z = f(x, y) \}$ le graphe de $f$. C'est une surface dans $\mathbb{R}^3$.
    Soit $P_0 = (x_0, y_0, z_0)$ un point de $X$, avec $z_0 = f(x_0, y_0)$.
    Alors l'espace tangent (affine) à $X$ au point $P_0$ est le plan d'équation :
    $$ z - z_0 = \frac{\partial f}{\partial x}(x_0, y_0) (x - x_0) + \frac{\partial f}{\partial y}(x_0, y_0) (y - y_0) $$
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    Le graphe $X$ peut être vu comme l'ensemble de niveau 0 de la fonction $g: U \times \mathbb{R} \to \mathbb{R}$ définie par $g(x, y, z) = z - f(x, y)$.
    L'ouvert $U \times \mathbb{R}$ est un ouvert de $E = \mathbb{R}^3$.
    La fonction $g$ est de classe $C^1$ car $f$ l'est (on suppose $f$ différentiable, si on veut $g$ soit $C^1$ il faut $f$ soit $C^1$. Le résultat reste vrai si $f$ est juste différentiable).
    Calculons la différentielle ou le gradient de $g$.
    $\frac{\partial g}{\partial x} = - \frac{\partial f}{\partial x}$, $\frac{\partial g}{\partial y} = - \frac{\partial f}{\partial y}$, $\frac{\partial g}{\partial z} = 1$.
    Le gradient de $g$ en $P_0 = (x_0, y_0, z_0)$ est :
    $\nabla g(P_0) = \left( - \frac{\partial f}{\partial x}(x_0, y_0), - \frac{\partial f}{\partial y}(x_0, y_0), 1 \right)$.
    Ce vecteur n'est jamais nul (car sa troisième composante est 1). Donc $dg_{P_0} \neq 0$.
    Le théorème précédent s'applique. L'espace tangent affine $\mathcal{T}_{P_0} X$ est l'hyperplan (ici, un plan) passant par $P_0$ et orthogonal à $\nabla g(P_0)$.
    Son équation est $\langle P - P_0, \nabla g(P_0) \rangle = 0$, où $P = (x, y, z)$.
    $P - P_0 = (x - x_0, y - y_0, z - z_0)$.
    $\left\langle (x - x_0, y - y_0, z - z_0), \left( - \frac{\partial f}{\partial x}(P_0'), - \frac{\partial f}{\partial y}(P_0'), 1 \right) \right\rangle = 0$, où $P_0'=(x_0, y_0)$.
    $(x - x_0) \left( - \frac{\partial f}{\partial x}(x_0, y_0) \right) + (y - y_0) \left( - \frac{\partial f}{\partial y}(x_0, y_0) \right) + (z - z_0) (1) = 0$.
    $z - z_0 = \frac{\partial f}{\partial x}(x_0, y_0) (x - x_0) + \frac{\partial f}{\partial y}(x_0, y_0) (y - y_0)$. $\square$
</div>

<h2>7. Optimisation - Conditions du premier ordre</h2>

<p>On s'intéresse ici à la recherche des extrema (minima ou maxima) d'une fonction $f: U \to \mathbb{R}$.</p>

<div class="definition">
    <strong>Définition (Extrema locaux et globaux).</strong>
    Soit $f: U \to \mathbb{R}$ et $a \in U$.
    <ul>
        <li>$f$ admet un <strong>minimum global</strong> en $a$ si, pour tout $x \in U$, $f(x) \ge f(a)$.</li>
        <li>$f$ admet un <strong>minimum local</strong> en $a$ s'il existe $r > 0$ tel que la boule $B(a, r)$ est incluse dans $U$ (ou $B(a,r) \cap U$ si $a$ est au bord, mais ici $U$ est ouvert) et pour tout $x \in B(a, r)$, on a $f(x) \ge f(a)$.</li>
        <li>Le minimum local est <strong>strict</strong> si pour $x \in B(a, r)$, $x \neq a$, on a $f(x) > f(a)$.</li>
    </ul>
    On définit de manière analogue maximum global, maximum local, maximum local strict en inversant les inégalités.
    Un <strong>extremum</strong> est un minimum ou un maximum.
</div>

<div class="definition">
    <strong>Définition (Point critique).</strong>
    Soit $f: U \to \mathbb{R}$ différentiable sur $U$. Un point $a \in U$ est dit <strong>point critique</strong> (ou point stationnaire) de $f$ si $df_a = 0$.
    Si $E$ est euclidien, cela équivaut à $\nabla f(a) = 0$.
    En coordonnées (avec une base $e_1, \dots, e_p$), cela équivaut à $\frac{\partial f}{\partial x_i}(a) = 0$ pour tout $i = 1, \dots, p$.
</div>

<div class="theorem">
    <strong>Théorème (Condition nécessaire du premier ordre).</strong>
    Soit $f: U \to \mathbb{R}$ différentiable en $a \in U$. Si $f$ admet un extremum local en $a$, alors $a$ est un point critique de $f$, c'est-à-dire $df_a = 0$.
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    Supposons que $f$ admet un minimum local en $a$. Il existe $r>0$ tel que $f(x) \ge f(a)$ pour $x \in B(a, r)$.
    Soit $v \in E$ un vecteur quelconque. Considérons la fonction $\phi(t) = f(a+tv)$ définie pour $t$ dans un voisinage de 0 (par exemple $|t| < r/\|v\|$ si $v \neq 0$).
    Comme $f$ est différentiable en $a$, $\phi$ est dérivable en 0 et $\phi'(0) = D_v f(a) = df_a(v)$.
    Puisque $f(a+tv) \ge f(a)$ pour $t$ assez petit, $\phi(t) \ge \phi(0)$ pour $t$ dans un voisinage de 0.
    La fonction réelle $\phi$ admet donc un minimum local en $t=0$. Sa dérivée en 0 doit donc être nulle (condition nécessaire pour un extremum d'une fonction d'une variable).
    Ainsi, $\phi'(0) = df_a(v) = 0$.
    Ceci étant vrai pour tout vecteur $v \in E$, l'application linéaire $df_a$ est nulle.
    La preuve est identique si $f$ admet un maximum local en $a$. $\square$
</div>

<div class="remark">
La réciproque est fausse. Un point critique n'est pas nécessairement un extremum local. Par exemple, $f(x) = x^3$ a un point critique en $a=0$ ($f'(0)=0$) mais pas d'extremum local. En dimension $p \ge 2$, on peut avoir des points selles, comme $f(x, y) = x^2 - y^2$ en $(0, 0)$. $\nabla f = (2x, -2y)$, donc $\nabla f(0,0) = (0, 0)$. Mais $f(x, 0) = x^2 \ge 0$ et $f(0, y) = -y^2 \le 0$, donc $(0, 0)$ n'est ni un min local ni un max local.
</div>

<div class="proposition">
    <strong>Proposition (Extremum sur une partie).</strong>
    Soit $X$ une partie non vide de $U$ et soit $x \in X$. Soit $f: U \to \mathbb{R}$ de classe $C^1$.
    Si la restriction $f|_X$ de $f$ à $X$ admet en $x$ un extremum local (relativement à $X$), alors $df_x(v) = 0$ pour tout vecteur $v \in T_x X$ (tout vecteur tangent à $X$ en $x$).
    Si $E$ est euclidien, cela signifie $\langle \nabla f(x), v \rangle = 0$ pour tout $v \in T_x X$, c'est-à-dire $\nabla f(x) \in (T_x X)^\perp$.
</div>

<div class="proof">
    <span class="proof-title">Preuve :</span>
    Supposons que $f|_X$ admette un minimum local en $x$. Soit $v \in T_x X$.
    Par définition de $T_x X$, il existe $\gamma: ]-\varepsilon, \varepsilon[ \to X$ dérivable en 0 avec $\gamma(0)=x$ et $\gamma'(0)=v$.
    Considérons $\phi(t) = f(\gamma(t))$. Pour $t$ assez proche de 0, $\gamma(t)$ est proche de $x$ et reste dans $X$. Donc $\phi(t) = f(\gamma(t)) \ge f(x) = \phi(0)$.
    La fonction $\phi$ admet donc un minimum local en $t=0$.
    Comme $f$ est $C^1$ et $\gamma$ est dérivable, $\phi$ est dérivable en 0 et sa dérivée doit être nulle : $\phi'(0) = 0$.
    Or, $\phi'(0) = (f \circ \gamma)'(0) = df_{\gamma(0)}(\gamma'(0)) = df_x(v)$.
    Donc $df_x(v) = 0$. $\square$
</div>

<div class="theorem">
    <strong>Théorème (Optimisation sous contrainte - Multiplicateurs de Lagrange).</strong>
    Soit $U \subset E$ un ouvert, et $f, g: U \to \mathbb{R}$ deux fonctions de classe $C^1$. On note $X = \{x \in U \mid g(x) = c\}$ l'ensemble de niveau $c$ de $g$.
    Soit $x_0 \in X$ un point tel que $dg_{x_0} \neq 0$ (ou $\nabla g(x_0) \neq 0$ si $E$ est euclidien).
    Si la restriction $f|_X$ admet un extremum local en $x_0$, alors il existe un unique réel $\lambda \in \mathbb{R}$ tel que :
    $$ df_{x_0} = \lambda dg_{x_0} $$
    Si $E$ est euclidien, cette condition s'écrit :
    $$ \nabla f(x_0) = \lambda \nabla g(x_0) $$
    Le réel $\lambda$ s'appelle un multiplicateur de Lagrange associé à la contrainte $g(x)=c$.
</div>

<div class="proof">
    <span class="proof-title">Idée de la preuve :</span>
    Si $f|_X$ a un extremum local en $x_0$, alors d'après la proposition précédente, $df_{x_0}(v) = 0$ pour tout $v \in T_{x_0} X$.
    Comme $dg_{x_0} \neq 0$, le théorème sur l'espace tangent nous dit que $T_{x_0} X = \ker dg_{x_0}$.
    Donc, $df_{x_0}(v) = 0$ pour tout $v \in \ker dg_{x_0}$.
    Cela signifie que $\ker dg_{x_0} \subset \ker df_{x_0}$.
    $dg_{x_0}$ et $df_{x_0}$ sont des formes linéaires sur $E$. $\ker dg_{x_0}$ est un hyperplan car $dg_{x_0} \neq 0$.
    Le fait que le noyau de $dg_{x_0}$ soit inclus dans le noyau de $df_{x_0}$ implique que les deux formes linéaires sont proportionnelles (c'est un résultat d'algèbre linéaire : si $\phi, \psi \in E^*$ et $\ker \phi \subset \ker \psi$, alors $\psi = \lambda \phi$ pour un $\lambda \in \mathbb{R}$. Si $\phi \neq 0$, $\lambda$ est unique).
    Donc il existe $\lambda \in \mathbb{R}$ tel que $df_{x_0} = \lambda dg_{x_0}$.
    L'unicité de $\lambda$ vient du fait que $dg_{x_0} \neq 0$. $\square$
</div>

<div class="remark">
Ce théorème donne une condition nécessaire pour un extremum sous contrainte. Pour trouver les extrema de $f$ sur $X$, on cherche les points $x \in U$ qui satisfont le système d'équations :
$$ \begin{cases} g(x) = c \\ \nabla f(x) = \lambda \nabla g(x) \quad (\text{ou } df_x = \lambda dg_x) \end{cases} $$
pour un certain $\lambda \in \mathbb{R}$. C'est un système de $p+1$ équations (p équations de $\nabla f = \lambda \nabla g$, 1 équation $g(x)=c$) pour $p+1$ inconnues (les $p$ coordonnées de $x$ et le multiplicateur $\lambda$).
Il faut aussi considérer les points de $X$ où $dg_x = 0$, car le théorème ne s'applique pas en ces points (ils peuvent être des extrema).
</div>


<h2>8. Optimisation - Conditions du second ordre</h2>

<p>On suppose ici $E = \mathbb{R}^p$ muni de sa structure euclidienne canonique, et $f: U \subset \mathbb{R}^p \to \mathbb{R}$ de classe $C^2$. On identifie $h \in \mathbb{R}^p$ avec le vecteur colonne de ses coordonnées.</p>

<div class="theorem">
    <strong>Théorème (Formule de Taylor-Young à l'ordre 2).</strong>
    Soit $f: U \subset \mathbb{R}^p \to \mathbb{R}$ de classe $C^2$ et $a \in U$. Alors, pour tout $h \in \mathbb{R}^p$ tel que le segment $[a, a+h] \subset U$, on a :
    $$ f(a+h) = f(a) + \sum_{i=1}^p h_i \frac{\partial f}{\partial x_i}(a) + \frac{1}{2} \sum_{i=1}^p \sum_{j=1}^p h_i h_j \frac{\partial^2 f}{\partial x_i \partial x_j}(a) + o(\|h\|^2) $$
    où $\lim_{h \to 0} \frac{o(\|h\|^2)}{\|h\|^2} = 0$.
</div>

<div class="definition">
    <strong>Définition (Matrice Hessienne).</strong>
    Si $f: U \subset \mathbb{R}^p \to \mathbb{R}$ est de classe $C^2$ et $a \in U$, on appelle matrice hessienne de $f$ en $a$ la matrice symétrique (d'après le théorème de Schwarz) $H_f(a) \in \mathcal{M}_p(\mathbb{R})$ définie par :
    $$ (H_f(a))_{i, j} = \frac{\partial^2 f}{\partial x_i \partial x_j}(a) $$
    $$ H_f(a) = \begin{pmatrix}
    \frac{\partial^2 f}{\partial x_1^2}(a) & \frac{\partial^2 f}{\partial x_1 \partial x_2}(a) & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_p}(a) \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1}(a) & \frac{\partial^2 f}{\partial x_2^2}(a) & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_p}(a) \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_p \partial x_1}(a) & \frac{\partial^2 f}{\partial x_p \partial x_2}(a) & \cdots & \frac{\partial^2 f}{\partial x_p^2}(a)
    \end{pmatrix} $$
</div>

<div class="remark">
    <strong>Réécriture de la formule de Taylor-Young.</strong>
    Avec le gradient $\nabla f(a) = \left(\frac{\partial f}{\partial x_1}(a), \dots, \frac{\partial f}{\partial x_p}(a)\right)^T$ et la matrice hessienne $H_f(a)$, la formule de Taylor-Young s'écrit :
    $$ f(a+h) = f(a) + \langle \nabla f(a), h \rangle + \frac{1}{2} \langle H_f(a) h, h \rangle + o(\|h\|^2) $$
    ou en notation matricielle (en identifiant $h$ et $\nabla f(a)$ à des vecteurs colonnes) :
    $$ f(a+h) = f(a) + (\nabla f(a))^T h + \frac{1}{2} h^T H_f(a) h + o(\|h\|^2) $$
    Le terme $\frac{1}{2} h^T H_f(a) h$ est la forme quadratique associée à la matrice hessienne.
</div>

<div class="theorem">
    <strong>Théorème (Condition nécessaire du second ordre).</strong>
    Soit $f: U \subset \mathbb{R}^p \to \mathbb{R}$ de classe $C^2$. Soit $a \in U$ un point critique ($df_a = 0$ ou $\nabla f(a) = 0$).
    <ul>
        <li>Si $f$ admet un minimum local en $a$, alors la matrice hessienne $H_f(a)$ est positive (ou semi-définie positive). C'est-à-dire, $\langle H_f(a) h, h \rangle \ge 0$ for all $h \in \mathbb{R}^p$. (Toutes ses valeurs propres sont $\ge 0$).</li>
        <li>Si $f$ admet un maximum local en $a$, alors la matrice hessienne $H_f(a)$ est négative (ou semi-définie négative). C'est-à-dire, $\langle H_f(a) h, h \rangle \le 0$ for all $h \in \mathbb{R}^p$. (Toutes ses valeurs propres sont $\le 0$).</li>
    </ul>
</div>

<div class="proof">
    <span class="proof-title">Idée de la preuve :</span>
    Supposons que $f$ ait un minimum local en $a$. Comme $a$ est un point critique, $\nabla f(a) = 0$.
    La formule de Taylor-Young donne : $f(a+h) - f(a) = \frac{1}{2} \langle H_f(a) h, h \rangle + o(\|h\|^2)$.
    Soit $Q(h) = \langle H_f(a) h, h \rangle$ la forme quadratique hessienne.
    On a $\frac{f(a+h) - f(a)}{\|h\|^2} = \frac{1}{2} Q\left(\frac{h}{\|h\|}\right) + \frac{o(\|h\|^2)}{\|h\|^2}$.
    Pour $h$ assez petit, $f(a+h) - f(a) \ge 0$. Donc $\frac{f(a+h) - f(a)}{\|h\|^2} \ge 0$.
    Soit $u = h/\|h\|$ un vecteur unitaire. Quand $h \to 0$, $h/\|h\|$ peut s'approcher de n'importe quel vecteur unitaire $u$.
    Si on prend $h=tu$ avec $t \to 0^+$, alors $h/\|h\| = u$.
    $f(a+tu) - f(a) = \frac{1}{2} \langle H_f(a) (tu), tu \rangle + o(\|tu\|^2) = \frac{t^2}{2} Q(u) + o(t^2)$.
    Comme $f(a+tu) - f(a) \ge 0$ pour $t$ petit, on a $\frac{t^2}{2} Q(u) + o(t^2) \ge 0$.
    En divisant par $t^2$ et en faisant $t \to 0^+$, on obtient $\frac{1}{2} Q(u) \ge 0$, soit $Q(u) \ge 0$.
    Ceci est vrai pour tout vecteur unitaire $u$. Pour un $h$ quelconque, si $h \neq 0$, $Q(h) = \|h\|^2 Q(h/\|h\|) \ge 0$. Si $h=0$, $Q(0)=0$. Donc $Q(h) \ge 0$ pour tout $h$, $H_f(a)$ est positive. $\square$
</div>

<div class="theorem">
    <strong>Théorème (Condition suffisante du second ordre).</strong>
    Soit $f: U \subset \mathbb{R}^p \to \mathbb{R}$ de classe $C^2$ et soit $a \in U$ un point critique ($\nabla f(a) = 0$).
    <ul>
        <li>Si la matrice hessienne $H_f(a)$ est définie positive (c'est-à-dire $\langle H_f(a) h, h \rangle > 0$ pour tout $h \neq 0$, ou encore toutes ses valeurs propres sont $> 0$), alors $f$ admet un minimum local strict en $a$.</li>
        <li>Si la matrice hessienne $H_f(a)$ est définie négative (c'est-à-dire $\langle H_f(a) h, h \rangle < 0$ pour tout $h \neq 0$, ou encore toutes ses valeurs propres sont $< 0$), alors $f$ admet un maximum local strict en $a$.</li>
        <li>Si $H_f(a)$ a des valeurs propres strictement positives et des valeurs propres strictement négatives (elle est indéfinie), alors $f$ n'admet pas d'extremum local en $a$. On dit que $a$ est un point selle (ou point col).</li>
        <li>Si $H_f(a)$ est seulement semi-définie (positive ou négative, mais pas définie) ou si $H_f(a)=0$, on ne peut pas conclure avec ce seul critère (il faut examiner les termes d'ordre supérieur ou le comportement local).</li>
    </ul>
</div>

<div class="proof">
    <span class="proof-title">Idée de la preuve (cas défini positif) :</span>
    On a $f(a+h) - f(a) = \frac{1}{2} Q(h) + o(\|h\|^2)$, où $Q(h) = \langle H_f(a) h, h \rangle$.
    Si $H_f(a)$ est définie positive, la forme quadratique $Q$ est définie positive. Sur la sphère unité $S=\{u \mid \|u\|=1\}$, la fonction continue $u \mapsto Q(u)$ atteint son minimum $\lambda_{min}$. Comme $Q$ est définie positive, $\lambda_{min} > 0$.
    Pour tout $h \neq 0$, $Q(h) = \|h\|^2 Q(h/\|h\|) \ge \|h\|^2 \lambda_{min}$.
    Donc $f(a+h) - f(a) \ge \frac{\lambda_{min}}{2} \|h\|^2 + o(\|h\|^2)$.
    On peut écrire $o(\|h\|^2) = \|h\|^2 \varepsilon(h)$ avec $\varepsilon(h) \to 0$ quand $h \to 0$.
    $f(a+h) - f(a) \ge \|h\|^2 \left( \frac{\lambda_{min}}{2} + \varepsilon(h) \right)$.
    Comme $\lambda_{min} > 0$, pour $\|h\|$ assez petit, $|\varepsilon(h)| < \frac{\lambda_{min}}{2}$.
    Alors $\frac{\lambda_{min}}{2} + \varepsilon(h) > 0$.
    Donc, pour $h$ non nul et assez petit, $f(a+h) - f(a) > 0$.
    Ceci montre que $f$ admet un minimum local strict en $a$. $\square$
</div>

<div class="theorem">
    <strong>Théorème (Extrema des fonctions de deux variables).</strong>
    Soit $f: U \subset \mathbb{R}^2 \to \mathbb{R}$ de classe $C^2$ et $a \in U$ un point critique ($\frac{\partial f}{\partial x}(a) = \frac{\partial f}{\partial y}(a) = 0$). Posons les notations de Monge :
    $$ r = \frac{\partial^2 f}{\partial x^2}(a), \quad s = \frac{\partial^2 f}{\partial x \partial y}(a), \quad t = \frac{\partial^2 f}{\partial y^2}(a) $$
    La matrice hessienne est $H_f(a) = \begin{pmatrix} r & s \\ s & t \end{pmatrix}$. Son déterminant est $\det(H_f(a)) = rt - s^2$. Sa trace est $\operatorname{Tr}(H_f(a)) = r+t$.
    Alors :
    <ol>
        <li>Si $rt - s^2 > 0$ et $r > 0$ (ou $t > 0$), alors $H_f(a)$ est définie positive et $f$ admet un minimum local strict en $a$.</li>
        <li>Si $rt - s^2 > 0$ et $r < 0$ (ou $t < 0$), alors $H_f(a)$ est définie négative et $f$ admet un maximum local strict en $a$.</li>
        <li>Si $rt - s^2 < 0$, alors $H_f(a)$ est indéfinie et $f$ n'admet pas d'extremum local en $a$ (c'est un point selle).</li>
        <li>Si $rt - s^2 = 0$, on ne peut pas conclure à l'aide de la hessienne seule.</li>
    </ol>
</div>

<div class="proof">
    <span class="proof-title">Rappel d'algèbre linéaire :</span>
    Une matrice symétrique $H = \begin{pmatrix} r & s \\ s & t \end{pmatrix}$ est :
    <ul>
        <li>Définie positive ssi ses valeurs propres sont $>0$, ce qui équivaut à $\det(H) > 0$ et $\operatorname{Tr}(H) > 0$. Comme $\operatorname{Tr}(H)=r+t$, si $\det=rt-s^2>0$, alors $r$ et $t$ ont même signe ($rt > s^2 \ge 0$). Si $r>0$, alors $t>0$, et $\operatorname{Tr}>0$. Donc la condition est $\det > 0$ et $r>0$.</li>
        <li>Définie négative ssi ses valeurs propres sont $<0$, ce qui équivaut à $\det(H) > 0$ et $\operatorname{Tr}(H) < 0$. La condition est $\det > 0$ et $r<0$.</li>
        <li>Indéfinie ssi ses valeurs propres sont de signes opposés, ce qui équivaut à $\det(H) < 0$.</li>
        <li>Semi-définie (mais pas définie) ou nulle ssi $\det(H)=0$.</li>
    </ul>
    Le théorème est donc une application directe du théorème précédent et de ces critères pour la nature d'une matrice symétrique $2 \times 2$. $\square$
</div>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

</body>
</html>
