<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cours - Espaces Préhilbertiens et Euclidiens</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"  crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"  crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <style>
        body { font-family: sans-serif; line-height: 1.6; padding: 20px; }
        h1, h2, h3 { color: #0056b3; }
        .definition, .theorem, .proposition, .proof, .example, .corollary, .remark {
            margin: 15px 0;
            padding: 15px;
            border-left: 4px solid;
        }
        .definition { border-color: #17a2b8; background-color: #e1f5fe; }
        .theorem { border-color: #28a745; background-color: #e8f5e9; }
        .proposition { border-color: #ffc107; background-color: #fff8e1; }
        .proof { border-color: #6c757d; background-color: #f8f9fa; font-style: italic; }
        .example { border-color: #fd7e14; background-color: #fff3e0; }
        .corollary { border-color: #007bff; background-color: #e7f3ff; }
        .remark { border-color: #adb5bd; background-color: #e9ecef; }
        strong { color: #0056b3; }
        code { background-color: #eee; padding: 2px 4px; border-radius: 3px; }
        .katex-display { overflow-x: auto; overflow-y: hidden; } /* Allow scrolling for wide formulas */
        .video-link { font-style: italic; color: #6c757d; margin-left: 10px; }
    </style>
</head>
<body>

<h1>Espaces Préhilbertiens et Euclidiens</h1>

<div class="remark">
    <strong>Conventions :</strong> Dans ce cours, $E$ désigne un $\mathbb{R}$-espace vectoriel. Le corps des scalaires est $\mathbb{R}$.
</div>

<h2>I. Produit scalaire</h2>

<div class="definition">
    <strong>Produit scalaire</strong><br>
    Soit $E$ un $\mathbb{R}$-espace vectoriel. On appelle <strong>produit scalaire</strong> sur $E$ toute application $\langle \cdot, \cdot \rangle : E \times E \to \mathbb{R}$ vérifiant les propriétés suivantes :
    <ol>
        <li><strong>Bilinéarité :</strong> L'application est linéaire par rapport à chaque variable. Pour tous $x, y, z \in E$ et tout $\lambda \in \mathbb{R}$ :
            <ul>
                <li>Linéarité à gauche : $\langle \lambda x + y, z \rangle = \lambda \langle x, z \rangle + \langle y, z \rangle$</li>
                <li>Linéarité à droite : $\langle x, \lambda y + z \rangle = \lambda \langle x, y \rangle + \langle x, z \rangle$</li>
            </ul>
        </li>
        <li><strong>Symétrie :</strong> Pour tous $x, y \in E$ :
            $$ \langle x, y \rangle = \langle y, x \rangle $$
            (Note : En présence de la symétrie, la linéarité à droite découle de la linéarité à gauche, et vice-versa).
        </li>
        <li><strong>Caractère défini positif :</strong> Pour tout $x \in E$ :
            <ul>
                <li>Positivité : $\langle x, x \rangle \ge 0$</li>
                <li>Caractère défini : $\langle x, x \rangle = 0 \iff x = 0_E$ (vecteur nul)</li>
            </ul>
        </li>
    </ol>
</div>

<div class="definition">
    <strong>Espace préhilbertien, Espace euclidien</strong><br>
    <ul>
        <li>Un $\mathbb{R}$-espace vectoriel $E$ muni d'un produit scalaire $\langle \cdot, \cdot \rangle$ est appelé un <strong>espace préhilbertien réel</strong>.</li>
        <li>Si de plus $E$ est de dimension finie, $E$ est appelé un <strong>espace euclidien</strong>.</li>
    </ul>
</div>

<div class="example">
    <strong>Exemples fondamentaux de produits scalaires :</strong>
    <ol>
        <li>Sur $E = \mathbb{R}^n$, l'application $(X, Y) \mapsto X^T Y = \sum_{k=1}^n x_k y_k$ est un produit scalaire, appelé <strong>produit scalaire canonique</strong> sur $\mathbb{R}^n$. $(\mathbb{R}^n$ muni de ce produit scalaire est un espace euclidien).</li>
        <li>Sur $E = \mathcal{M}_{n,p}(\mathbb{R})$, l'application $(A, B) \mapsto \text{Tr}(A^T B) = \sum_{i=1}^n \sum_{j=1}^p a_{i,j} b_{i,j}$ est un produit scalaire, appelé <strong>produit scalaire canonique</strong> sur $\mathcal{M}_{n,p}(\mathbb{R})$. (C'est un espace euclidien de dimension $np$).</li>
        <li>Sur $E = \mathcal{C}([a, b], \mathbb{R})$, l'espace des fonctions continues de $[a, b]$ dans $\mathbb{R}$, l'application $(f, g) \mapsto \int_a^b f(t)g(t) dt$ est un produit scalaire. (C'est un espace préhilbertien de dimension infinie).</li>
    </ol>
</div>

<div class="remark">
    Dans toute la suite, $(E, \langle \cdot, \cdot \rangle)$ désigne un espace préhilbertien réel.
</div>

<h2>II. Norme associée</h2>

<div class="definition">
    <strong>Norme associée</strong><br>
    Pour tout $x \in E$, on définit la <strong>norme associée</strong> au produit scalaire (ou norme euclidienne) par :
    $$ \|x\| = \sqrt{\langle x, x \rangle} $$
    (La racine carrée est bien définie car $\langle x, x \rangle \ge 0$).
</div>

<div class="proposition">
    <strong>Identités remarquables et de polarisation</strong><br>
    Pour tous $x, y \in E$ :
    <ul>
        <li>$\|x+y\|^2 = \langle x+y, x+y \rangle = \langle x, x \rangle + 2 \langle x, y \rangle + \langle y, y \rangle = \|x\|^2 + \|y\|^2 + 2 \langle x, y \rangle$</li>
        <li>$\|x-y\|^2 = \|x\|^2 + \|y\|^2 - 2 \langle x, y \rangle$</li>
        <li>$\|x+y\|^2 + \|x-y\|^2 = 2(\|x\|^2 + \|y\|^2)$ (Identité du parallélogramme)</li>
        <li><strong>Identité de polarisation :</strong> $\langle x, y \rangle = \frac{1}{2} (\|x+y\|^2 - \|x\|^2 - \|y\|^2)$</li>
        <li>Autre identité de polarisation : $\langle x, y \rangle = \frac{1}{4} (\|x+y\|^2 - \|x-y\|^2)$</li>
    </ul>
    Ces identités montrent que le produit scalaire est entièrement déterminé par la norme associée.
</div>

<div class="theorem">
    <strong>Inégalité de Cauchy-Schwarz et Inégalité Triangulaire</strong><br>
    Soient $x, y \in E$.
    <ul>
        <li><strong>Inégalité de Cauchy-Schwarz :</strong>
            $$ |\langle x, y \rangle| \le \|x\| \|y\| $$
            L'égalité a lieu si et seulement si la famille $(x, y)$ est liée (c'est-à-dire si $x$ et $y$ sont colinéaires).
        </li>
        <li><strong>Inégalité Triangulaire (de Minkowski) :</strong>
            $$ \|x+y\| \le \|x\| + \|y\| $$
            L'égalité a lieu si et seulement si $x = 0_E$ ou $y = 0_E$ ou s'il existe $\lambda \in \mathbb{R}^+$ tel que $y = \lambda x$ (on dit que $x$ et $y$ sont <strong>positivement liés</strong>).
        </li>
    </ul>
    <span class="video-link">(Démonstration en vidéo !)</span>
</div>

<div class="proof">
    <strong>Idée de la preuve de Cauchy-Schwarz :</strong>
    Pour tout $\lambda \in \mathbb{R}$, $P(\lambda) = \|x + \lambda y\|^2 = \langle x + \lambda y, x + \lambda y \rangle = \|y\|^2 \lambda^2 + 2 \langle x, y \rangle \lambda + \|x\|^2 \ge 0$.
    Si $y=0$, l'inégalité est triviale. Si $y \neq 0$, $\|y\|^2 > 0$. Le polynôme $P(\lambda)$ en $\lambda$ est du second degré et toujours positif ou nul. Son discriminant $\Delta$ doit donc être négatif ou nul.
    $\Delta = (2 \langle x, y \rangle)^2 - 4 (\|y\|^2)(\|x\|^2) = 4 ( \langle x, y \rangle^2 - \|x\|^2 \|y\|^2 ) \le 0$.
    Donc $\langle x, y \rangle^2 \le \|x\|^2 \|y\|^2$. En prenant la racine carrée, $|\langle x, y \rangle| \le \|x\| \|y\|$.
    Le cas d'égalité $\Delta = 0$ correspond à l'existence d'une unique racine réelle $\lambda_0$, pour laquelle $P(\lambda_0) = \|x + \lambda_0 y\|^2 = 0$, ce qui signifie $x + \lambda_0 y = 0$, donc $(x, y)$ est liée.
</div>

<div class="example">
    <strong>Applications de Cauchy-Schwarz :</strong>
    <ol>
        <li>Pour $E = \mathcal{C}([a, b], \mathbb{R})$ :
           $$ \left| \int_a^b f(t)g(t) dt \right| \le \left( \int_a^b f(t)^2 dt \right)^{1/2} \left( \int_a^b g(t)^2 dt \right)^{1/2} $$
        </li>
        <li>Pour $E = \mathbb{R}^n$ (produit scalaire canonique) :
           $$ \left| \sum_{k=1}^n x_k y_k \right| \le \left( \sum_{k=1}^n x_k^2 \right)^{1/2} \left( \sum_{k=1}^n y_k^2 \right)^{1/2} $$
        </li>
    </ol>
</div>

<div class="proposition">
    <strong>La norme associée est une norme</strong><br>
    L'application $\| \cdot \| : E \to \mathbb{R}^+$ définie par $\|x\| = \sqrt{\langle x, x \rangle}$ est une norme sur $E$. Elle vérifie :
    <ol>
        <li><strong>Séparation :</strong> $\|x\| = 0 \iff x = 0_E$. (Provient du caractère défini du produit scalaire).</li>
        <li><strong>Homogénéité :</strong> Pour tout $\lambda \in \mathbb{R}$ et tout $x \in E$, $\|\lambda x\| = |\lambda| \|x\|$.
           ($\|\lambda x\|^2 = \langle \lambda x, \lambda x \rangle = \lambda^2 \langle x, x \rangle = \lambda^2 \|x\|^2$).</li>
        <li><strong>Inégalité triangulaire :</strong> Pour tous $x, y \in E$, $\|x+y\| \le \|x\| + \|y\|$. (Démontrée ci-dessus).</li>
    </ol>
</div>

<h2>III. Familles orthogonales, orthonormales</h2>

<div class="definition">
    <strong>Orthogonalité</strong><br>
    <ul>
        <li>Deux vecteurs $x, y \in E$ sont dits <strong>orthogonaux</strong> si $\langle x, y \rangle = 0$. On note $x \perp y$.</li>
        <li>Deux parties $A, B \subseteq E$ sont dites <strong>orthogonales</strong> si $\forall x \in A, \forall y \in B$, $x \perp y$.</li>
        <li>Une famille $(x_i)_{i \in I}$ de vecteurs de $E$ est dite <strong>orthogonale</strong> si ses vecteurs sont deux à deux orthogonaux, i.e., $\forall i \neq j, \langle x_i, x_j \rangle = 0$.</li>
    </ul>
</div>

<div class="proposition">
    <strong>Liberté des familles orthogonales</strong><br>
    Toute famille orthogonale $(x_i)_{i \in I}$ constituée de vecteurs <strong>non nuls</strong> est une famille libre.
</div>

<div class="proof">
    <strong>Preuve :</strong> Soit $(x_1, \dots, x_p)$ une sous-famille finie. Supposons $\sum_{k=1}^p \lambda_k x_k = 0$. Pour un $j \in \{1, \dots, p\}$ fixé, prenons le produit scalaire avec $x_j$ :
    $\langle \sum_{k=1}^p \lambda_k x_k, x_j \rangle = \langle 0, x_j \rangle = 0$.
    Par linéarité : $\sum_{k=1}^p \lambda_k \langle x_k, x_j \rangle = 0$.
    Comme la famille est orthogonale, $\langle x_k, x_j \rangle = 0$ si $k \neq j$. Il reste :
    $\lambda_j \langle x_j, x_j \rangle = \lambda_j \|x_j\|^2 = 0$.
    Puisque $x_j \neq 0$, $\|x_j\|^2 > 0$. Donc $\lambda_j = 0$. Ceci étant vrai pour tout $j$, la famille est libre.
</div>

<div class="theorem">
    <strong>Théorème de Pythagore</strong><br>
    Soit $(x_1, \dots, x_p)$ une famille orthogonale de $E$. Alors :
    $$ \left\| \sum_{k=1}^p x_k \right\|^2 = \sum_{k=1}^p \|x_k\|^2 $$
</div>

<div class="proof">
    <strong>Preuve :</strong>
    $\|\sum x_k\|^2 = \langle \sum_{i=1}^p x_i, \sum_{j=1}^p x_j \rangle = \sum_{i=1}^p \sum_{j=1}^p \langle x_i, x_j \rangle$.
    Comme la famille est orthogonale, $\langle x_i, x_j \rangle = 0$ si $i \neq j$. Il reste les termes où $i=j$ :
    $\sum_{i=1}^p \langle x_i, x_i \rangle = \sum_{i=1}^p \|x_i\|^2$.
</div>

<div class="definition">
    <strong>Orthogonal d'une partie</strong><br>
    Soit $X$ une partie (non vide) de $E$. L'<strong>orthogonal</strong> de $X$, noté $X^\perp$, est l'ensemble des vecteurs de $E$ orthogonaux à tous les vecteurs de $X$ :
    $$ X^\perp = \{ y \in E \mid \forall x \in X, \langle x, y \rangle = 0 \} $$
    $X^\perp$ est toujours un sous-espace vectoriel de $E$ (même si $X$ n'en est pas un). On a aussi $X^\perp = (\text{Vect}(X))^\perp$.
</div>

<div class="definition">
    <strong>Vecteur unitaire, Famille orthonormale</strong><br>
    <ul>
        <li>Un vecteur $x \in E$ est dit <strong>unitaire</strong> ou <strong>normé</strong> si $\|x\| = 1$.</li>
        <li>Une famille $(e_i)_{i \in I}$ est dite <strong>orthonormale</strong> (ou orthonormée) si elle est orthogonale et si tous ses vecteurs sont unitaires.
            $$ \forall i, j \in I, \quad \langle e_i, e_j \rangle = \delta_{i,j} $$
            où $\delta_{i,j}$ est le symbole de Kronecker ($\delta_{i,j}=1$ si $i=j$, $0$ si $i \neq j$).
        </li>
    </ul>
    Une famille orthonormale est toujours libre (car ses vecteurs sont non nuls).
</div>

<div class="theorem">
    <strong>Procédé d'orthonormalisation de Gram-Schmidt</strong><br>
    Soit $(x_1, \dots, x_p)$ une famille libre de $E$. Il existe une unique famille orthonormale $(e_1, \dots, e_p)$ telle que :
    <ol>
        <li>Pour tout $k \in \{1, \dots, p\}$, $\text{Vect}(e_1, \dots, e_k) = \text{Vect}(x_1, \dots, x_k)$.</li>
        <li>Pour tout $k \in \{1, \dots, p\}$, $\langle e_k, x_k \rangle > 0$.</li>
    </ol>
    Cette famille est construite par récurrence :
    <ul>
        <li>$e'_1 = x_1$, puis $e_1 = \frac{e'_1}{\|e'_1\|}$.</li>
        <li>Pour $k$ de 2 à $p$ :
            $e'_k = x_k - \sum_{i=1}^{k-1} \langle x_k, e_i \rangle e_i$ (projection de $x_k$ sur l'orthogonal de $\text{Vect}(e_1, \dots, e_{k-1})$).
            Puis $e_k = \frac{e'_k}{\|e'_k\|}$.
        </li>
    </ul>
    La condition $\langle e_k, x_k \rangle > 0$ est assurée par $\langle e'_k, x_k \rangle = \|e'_k\|^2 > 0$ (car la famille est libre).
    <span class="video-link">(Démonstration en vidéo !)</span>
</div>

<h2>IV. Bases orthonormées</h2>

<div class="remark">
    Dans cette partie, $E$ est supposé être un <strong>espace euclidien</strong> de dimension $n$.
</div>

<div class="definition">
    <strong>Base orthonormée (BON)</strong><br>
    Une base $(e_1, \dots, e_n)$ de $E$ est dite <strong>orthonormée</strong> si c'est une famille orthonormale.
</div>

<div class="theorem">
    <strong>Existence et Complétion</strong><br>
    <ul>
        <li>Tout espace euclidien $E$ admet au moins une base orthonormée. (Appliquer Gram-Schmidt à une base quelconque de $E$).</li>
        <li>Toute famille orthonormale $(e_1, \dots, e_p)$ d'un espace euclidien $E$ (avec $p \le n$) peut être complétée en une base orthonormée $(e_1, \dots, e_p, e_{p+1}, \dots, e_n)$ de $E$.</li>
    </ul>
</div>

<div class="proof">
    <strong>Idée de la preuve de complétion :</strong> La famille $(e_1, \dots, e_p)$ est libre. On peut la compléter en une base $(e_1, \dots, e_p, x_{p+1}, \dots, x_n)$ de $E$. On applique ensuite le procédé de Gram-Schmidt à cette base. Les $p$ premiers vecteurs obtenus seront $e_1, \dots, e_p$ (par unicité et car ils satisfont déjà les conditions), et les suivants $e_{p+1}, \dots, e_n$ compléteront la famille en une BON.
</div>

<div class="proposition">
    <strong>Calculs dans une base orthonormée</strong><br>
    Soit $\mathcal{B} = (e_1, \dots, e_n)$ une base orthonormée de $E$.
    <ol>
        <li><strong>Coordonnées :</strong> Pour tout $x \in E$, ses coordonnées dans la base $\mathcal{B}$ sont données par les produits scalaires :
           $$ x = \sum_{i=1}^n \langle x, e_i \rangle e_i $$
           La $i$-ème coordonnée de $x$ est $x_i = \langle x, e_i \rangle$.
        </li>
        <li><strong>Produit scalaire et Norme :</strong> Si $x = \sum_{i=1}^n x_i e_i$ et $y = \sum_{i=1}^n y_i e_i$ (où $x_i, y_i$ sont les coordonnées dans $\mathcal{B}$), alors :
           $$ \langle x, y \rangle = \sum_{i=1}^n x_i y_i $$
           $$ \|x\|^2 = \sum_{i=1}^n x_i^2 $$
           (Le produit scalaire dans une BON a la même expression que le produit scalaire canonique de $\mathbb{R}^n$).
        </li>
    </ol>
</div>

<div class="proof">
    <strong>Preuve (1) :</strong> Soit $x = \sum_{j=1}^n \alpha_j e_j$. Prenons le produit scalaire avec $e_i$ :
    $\langle x, e_i \rangle = \langle \sum_{j=1}^n \alpha_j e_j, e_i \rangle = \sum_{j=1}^n \alpha_j \langle e_j, e_i \rangle = \sum_{j=1}^n \alpha_j \delta_{j,i} = \alpha_i$. Donc $\alpha_i = \langle x, e_i \rangle$.
    <strong>Preuve (2) :</strong> $\langle x, y \rangle = \langle \sum x_i e_i, \sum y_j e_j \rangle = \sum_i \sum_j x_i y_j \langle e_i, e_j \rangle = \sum_i \sum_j x_i y_j \delta_{i,j} = \sum_i x_i y_i$. La formule pour la norme s'en déduit en prenant $y=x$.
</div>

<h2>V. Projection orthogonale</h2>

<div class="remark">
    Dans cette partie, $E$ est un espace préhilbertien et $F$ est un sous-espace vectoriel de $E$ de <strong>dimension finie</strong> $p$.
</div>

<div class="theorem">
    <strong>Supplémentaire orthogonal</strong><br>
    Soit $F$ un sous-espace vectoriel de dimension finie d'un espace préhilbertien $E$. Alors $F$ et son orthogonal $F^\perp$ sont supplémentaires dans $E$ :
    $$ E = F \oplus F^\perp $$
    $F^\perp$ est appelé le <strong>supplémentaire orthogonal</strong> de $F$. C'est l'unique supplémentaire de $F$ qui lui est orthogonal.
</div>

<div class="corollary">
    <strong>Propriétés de l'orthogonal</strong><br>
    Soit $F$ un sous-espace de dimension finie.
    <ul>
        <li>$(F^\perp)^\perp = F$.</li>
        <li>Si $E$ est de dimension finie $n$, alors $F^\perp$ est aussi de dimension finie et :
            $$ \dim(F^\perp) = \dim(E) - \dim(F) $$
        </li>
    </ul>
</div>

<div class="definition">
    <strong>Projection orthogonale</strong><br>
    Puisque $E = F \oplus F^\perp$, tout vecteur $x \in E$ s'écrit de manière unique $x = y + z$ avec $y \in F$ et $z \in F^\perp$.
    Le vecteur $y$ est appelé le <strong>projeté orthogonal</strong> de $x$ sur $F$. On le note $p_F(x)$.
    L'application $p_F: E \to F$ est linéaire, $p_F \circ p_F = p_F$ (c'est un projecteur), $\text{Im}(p_F) = F$ et $\ker(p_F) = F^\perp$.
    De même, $z = x - p_F(x)$ est le projeté orthogonal de $x$ sur $F^\perp$, noté $p_{F^\perp}(x)$.
</div>

<div class="theorem">
    <strong>Calcul du projeté orthogonal</strong><br>
    Si $(e_1, \dots, e_p)$ est une base orthonormée de $F$, alors pour tout $x \in E$ :
    $$ p_F(x) = \sum_{i=1}^p \langle x, e_i \rangle e_i $$
</div>

<div class="proof">
    <strong>Preuve :</strong> Posons $y = \sum_{i=1}^p \langle x, e_i \rangle e_i$. Clairement $y \in F$ (combinaison linéaire des $e_i$).
    Montrons que $z = x - y$ est dans $F^\perp$. Il suffit de montrer que $z$ est orthogonal à chaque vecteur $e_j$ de la base de $F$.
    $\langle z, e_j \rangle = \langle x - y, e_j \rangle = \langle x, e_j \rangle - \langle y, e_j \rangle$.
    $\langle y, e_j \rangle = \langle \sum_{i=1}^p \langle x, e_i \rangle e_i, e_j \rangle = \sum_{i=1}^p \langle x, e_i \rangle \langle e_i, e_j \rangle = \sum_{i=1}^p \langle x, e_i \rangle \delta_{i,j} = \langle x, e_j \rangle$.
    Donc $\langle z, e_j \rangle = \langle x, e_j \rangle - \langle x, e_j \rangle = 0$.
    Ainsi $z \in F^\perp$. L'unicité de la décomposition $x = y + z$ montre que $y = p_F(x)$.
</div>

<div class="definition">
    <strong>Distance à une partie</strong><br>
    Soit $A$ une partie non vide de $E$ et $x \in E$. La <strong>distance</strong> de $x$ à $A$ est le réel positif :
    $$ d(x, A) = \inf_{a \in A} \|x - a\| $$
</div>

<div class="theorem">
    <strong>Distance à un sous-espace de dimension finie (Théorème de meilleure approximation)</strong><br>
    Soit $F$ un sous-espace de dimension finie de $E$. Pour tout $x \in E$, le projeté orthogonal $p_F(x)$ est l'unique élément de $F$ qui minimise la distance à $x$. Pour tout $f \in F$:
    $$ \|x - f\| \ge \|x - p_F(x)\| $$
    avec égalité si et seulement si $f = p_F(x)$.
    En particulier, la distance de $x$ à $F$ est atteinte en $p_F(x)$ :
    $$ d(x, F) = \|x - p_F(x)\| = \|p_{F^\perp}(x)\| $$
    De plus, par le théorème de Pythagore appliqué à $x = p_F(x) + p_{F^\perp}(x)$ :
    $$ \|x\|^2 = \|p_F(x)\|^2 + \|p_{F^\perp}(x)\|^2 = \|p_F(x)\|^2 + d(x, F)^2 $$
    Donc,
    $$ d(x, F) = \sqrt{\|x\|^2 - \|p_F(x)\|^2} $$
</div>

<div class="proof">
    <strong>Idée de la preuve :</strong> Soit $f \in F$. On écrit $x - f = (x - p_F(x)) + (p_F(x) - f)$.
    Le premier terme $x - p_F(x) = p_{F^\perp}(x)$ est dans $F^\perp$.
    Le second terme $p_F(x) - f$ est dans $F$ (car $p_F(x) \in F$ et $f \in F$).
    Ces deux termes sont donc orthogonaux. Par Pythagore :
    $\|x - f\|^2 = \|x - p_F(x)\|^2 + \|p_F(x) - f\|^2$.
    Comme $\|p_F(x) - f\|^2 \ge 0$, on a $\|x - f\|^2 \ge \|x - p_F(x)\|^2$.
    L'égalité a lieu si et seulement si $\|p_F(x) - f\|^2 = 0$, c'est-à-dire $p_F(x) - f = 0$, soit $f = p_F(x)$.
</div>

<div class="remark">
    <strong>Cas des hyperplans (en dimension finie)</strong><br>
    On suppose maintenant que $E$ est un espace euclidien de dimension $n$. Soit $H$ un hyperplan de $E$.
    Alors $\dim(H^\perp) = \dim(E) - \dim(H) = n - (n-1) = 1$. $H^\perp$ est une droite vectorielle.
</div>

<div class="definition">
    <strong>Vecteur normal à un hyperplan</strong><br>
    Tout vecteur non nul $u \in H^\perp$ est appelé un <strong>vecteur normal</strong> à l'hyperplan $H$. Un tel vecteur $u$ vérifie $\langle u, h \rangle = 0$ pour tout $h \in H$, et l'hyperplan $H$ est l'ensemble des vecteurs $x \in E$ tels que $\langle x, u \rangle = 0$.
    $$ H = \{ x \in E \mid \langle x, u \rangle = 0 \} = (\text{Vect}(u))^\perp $$
</div>

<div class="example">
    Dans $\mathbb{R}^n$ muni du produit scalaire canonique, soit $(a_1, \dots, a_n) \in \mathbb{R}^n \setminus \{0\}$. L'ensemble $H = \{ (x_1, \dots, x_n) \in \mathbb{R}^n \mid a_1 x_1 + \dots + a_n x_n = 0 \}$ est un hyperplan.
    L'équation s'écrit $\langle a, x \rangle = 0$ avec $a = (a_1, \dots, a_n)$ et $x = (x_1, \dots, x_n)$.
    Le vecteur $a$ est donc un vecteur normal à $H$.
</div>

<div class="theorem">
    <strong>Projection et distance à un hyperplan</strong><br>
    Soit $H$ un hyperplan de $E$ et $u$ un vecteur normal à $H$. Alors, pour tout $x \in E$ :
    <ul>
        <li>Le projeté orthogonal de $x$ sur la droite $H^\perp = \text{Vect}(u)$ est :
            $$ p_{H^\perp}(x) = \frac{\langle x, u \rangle}{\|u\|^2} u $$
            (car $u/\|u\|$ est une BON de $H^\perp$).
        </li>
        <li>Le projeté orthogonal de $x$ sur l'hyperplan $H$ est :
            $$ p_H(x) = x - p_{H^\perp}(x) = x - \frac{\langle x, u \rangle}{\|u\|^2} u $$
        </li>
        <li>La distance de $x$ à $H$ est :
            $$ d(x, H) = \|p_{H^\perp}(x)\| = \left\| \frac{\langle x, u \rangle}{\|u\|^2} u \right\| = \frac{|\langle x, u \rangle|}{\|u\|^2} \|u\| = \frac{|\langle x, u \rangle|}{\|u\|} $$
        </li>
    </ul>
</div>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            // customised options
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
        });
    });
</script>

</body>
</html>
