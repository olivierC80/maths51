<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MATHS MP/MP* - Savoir & Faire en Prépas</title>
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css">
    <!-- KaTeX JS -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"></script>
    <!-- KaTeX auto-render extension -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 2em;
            max-width: 800px; /* Limit width for readability */
            margin-left: auto;
            margin-right: auto;
        }
        h1, h2, h3, h4, h5, h6 {
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }
        h1 { border-bottom: 2px solid #ccc; padding-bottom: 0.3em; }
        h2 { border-bottom: 1px solid #eee; padding-bottom: 0.2em; }
        p { margin-bottom: 1em; }
        ul, ol { margin-left: 2em; margin-bottom: 1em; }
        li { margin-bottom: 0.5em; }
        blockquote {
            margin-left: 2em;
            padding-left: 1em;
            border-left: 3px solid #eee;
            font-style: italic;
            color: #555;
        }
        code {
            background-color: #f4f4f4;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
        }
        pre code {
            display: block;
            padding: 1em;
            overflow-x: auto;
        }
        .katex-display { /* Style for KaTeX display math */
            display: block;
            margin-top: 1em;
            margin-bottom: 1em;
            overflow-x: auto; /* Handle long formulas */
            overflow-y: hidden;
        }
        .center { text-align: center; }
        .small { font-size: 0.8em; }
        .isbn { font-family: monospace; }
    </style>
</head>
<body>

<!-- START OF FILE 2023_02_11_18_47.md -->

<!-- Image omitted: image0010001 -->
<h1>SAVOIR & FAIRE EN PRÉPAS</h1>
<p>Collection dirigée par Karine Beaurpère</p>
<h2>MATHS MP/MP*</h2>
<p>Thierry Legay</p>
<ul>
<li>Les savoirs essentiels du programme</li>
<li>Les problématiques les plus classiques</li>
<li>Les méthodes de résolution les plus utiles</li>
<li>Des exercices d'entraînement minutieusement choisis</li>
<li>Des corrigés détaillés de tous les exercices</li>
</ul>
<!-- Image omitted: image0010003 -->
<h1>MATHS</h1>
<h2>MP/MP*</h2>

<p>JEGATHEESWARAN RUBINTHAN MP</p>

<h3>Contenu du cours</h3>
<h4>Algèbre</h4>
<ul>
<li>Groupes, anneaux et corps</li>
<li>Espaces vectoriels</li>
<li>Applications linéaires</li>
<li>Matrices et déterminants</li>
</ul>
<h4>Analyse</h4>
<ul>
<li>Suites et séries</li>
<li>Continuité et dérivabilité</li>
<li>Intégration</li>
<li>Séries entières et développements limités</li>
</ul>
<h4>Géométrie</h4>
<ul>
<li>Géométrie dans l'espace</li>
<li>Transformations géométriques</li>
<li>Géométrie projective</li>
</ul>
<h4>Probabilités et statistiques</h4>
<ul>
<li>Probabilités discrètes et continues</li>
<li>Variables aléatoires</li>
<li>Lois de probabilité</li>
<li>Estimation et tests d'hypothèses</li>
</ul>

<ol>
<li><strong>Dérivée d'une fonction</strong><br>
Si $f(x)$ est une fonction dérivable, alors la dérivée de $f$ en $x$ est donnée par:
$$
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$
</li>
<li><strong>Intégrale définie</strong><br>
L'intégrale définie de $f(x)$ de $a$ à $b$ est notée:
$$
\int_a^b f(x) \, dx
$$
</li>
<li><strong>Déterminant d'une matrice</strong><br>
Le déterminant d'une matrice $2 \times 2$ $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ est donné par:
$$
\det(A) = ad - bc
$$
</li>
<li><strong>Espérance mathématique</strong><br>
L'espérance d'une variable aléatoire discrète $X$ est donnée par:
$$
E(X) = \sum_{i} x_i P(X = x_i)
$$
</li>
</ol>

<p>Note: Ce document est un résumé des principaux sujets abordés en MP/MP*. Pour plus de détails, référez-vous aux manuels et aux notes de cours spécifiques.</p>

<!-- Image omitted: image0010005 -->
<h1>SAVOIR & FAIRE EN PRÉPAS</h1>
<p>COLLECTION DIRIGÉE PAR KARINE BEAURPÈRE</p>
<h2>MATHS</h2>
<h2>MP/MP*</h2>
<p>Thierry Legay<br>
Professeur de chaire supérieure au lycée d'Arsonval de Saint-Maur-des-Fossés<br>
Ancien élève de l'École normale supérieure de Saint-Cloud</p>
<!-- Image omitted: Ellipses logo -->
<p>Note : Le logo "Ellipses" en bas de l'image a été remplacé par un lien fictif vers une image. Vous pouvez remplacer ce lien par l'URL réelle de l'image si nécessaire.</p>

<!-- Image omitted: image0010006 -->
<h2>Dans la même collection :</h2>
<h3>1ère année</h3>
<h4>Classes préparatoires scientifiques</h4>
<p><strong>PCSI</strong></p>
<ul>
<li><em>Mathématiques</em>, Cécile Le Goff, 2015</li>
<li><em>Physique</em>, Florence Depaquit-Debieuvre, 2015</li>
<li><em>Chimie</em>, Julien Gérard, 2015</li>
</ul>
<p><strong>PTSI</strong></p>
<ul>
<li><em>Mathématiques</em>, Marie Virat, 2016</li>
</ul>
<p><strong>MPSI</strong></p>
<ul>
<li><em>Physique-Chimie</em>, Mathieu Hebding, 2016</li>
<li><em>Maths</em>, Lionel Béal, 2016</li>
</ul>
<p><strong>BCPST1</strong></p>
<ul>
<li><em>Mathématiques</em>, Sébastien Pellerin, 2015</li>
</ul>
<h4>Classes préparatoires économiques et commerciales</h4>
<p><strong>ECS1</strong></p>
<ul>
<li><em>Mathématiques</em>, Hervé Gras, 2015</li>
</ul>
<p><strong>ECS2</strong></p>
<ul>
<li><em>Mathématiques</em>, Hervé Gras, 2016</li>
</ul>
<p><strong>ECE1</strong></p>
<ul>
<li><em>Mathématiques</em>, Nicolas Damien, 2016</li>
</ul>
<p><strong>ECE2</strong></p>
<ul>
<li><em>Mathématiques</em>, Matthieu Fèvre, Anne Gorlier, Roland Gorlier, 2015</li>
</ul>
<h3>2ème année</h3>
<h4>Classes préparatoires scientifiques</h4>
<p><strong>MP/MP*</strong></p>
<ul>
<li><em>Mathématiques</em>, Thierry Legay, 2016</li>
<li><em>Physique-Chimie</em>, Marc Venturi, 2016</li>
</ul>
<p><strong>PSI/PSI*</strong></p>
<ul>
<li><em>Mathématiques</em>, Thierry Legay, 2015</li>
<li><em>Physique-Chimie</em>, Sylvie Berger, Gaëlle Ringot, 2016</li>
</ul>
<p><strong>PC/PC*</strong></p>
<ul>
<li><em>Physique</em>, Florence Depaquit-Debieuvre, 2016</li>
<li><em>Chimie</em>, Julien Gérard, 2016</li>
<li><em>Mathématiques</em>, Walter Damien, Rodolphe Garin, 2015</li>
</ul>
<p><strong>PT/PT*</strong></p>
<ul>
<li><em>Mathématiques</em>, Mustapha Boukhobza, 2015</li>
</ul>

<hr>
<p class="isbn">ISBN 9782340-014831</p>
<p>© Ellipses Édition Marketing S.A., 2016<br>
32, rue Bargue 75740 Paris cedex 15</p>
<p>Le Code de la propriété intellectuelle n’autorisant, aux termes de l’article $L.122-5 2^\circ$ et $3^\circ$ a), d’une part, que les « copies ou reproductions strictement réservées à l’usage privé du copiste et non destinées à une utilisation collective », et d’autre part, que les analyses et les courtes citations dans un but d’exemple et d’illustration, « toute représentation ou reproduction intégrale ou partielle faite sans le consentement de l’auteur ou de ses ayants droit ou ayants cause est illicite » (art. L. 122-4).<br>
Cette représentation ou reproduction, par quelque procédé que ce soit constituerait une contrefaçon sanctionnée par les articles L.335-2 et suivants du Code de la propriété intellectuelle.</p>
<p><a href="http://www.editions-ellipses.fr">www.editions-ellipses.fr</a></p>

<!-- Image omitted: image0010007 -->
<h1>Présentation de la collection</h1>
<p>La collection « Savoir et Faire en Prépas » est destinée aux étudiants des Classes Préparatoires aux Grandes Écoles.</p>
<p>Chaque ouvrage de la collection s'adresse à un niveau donné d'une filière précise, pour une matière spécifique.</p>
<p>Cette collection est conçue dans le but de pouvoir parfaitement utiliser son cours. C'est pourquoi cette collection met en avant les savoirs théoriques essentiels mais elle met surtout en lumière la façon dont sont utilisés ces savoirs au travers de nombreuses méthodes.</p>
<p>Cette collection démontre finalement que la résolution d'une grande quantité de problèmes se fait avec assez peu de savoirs théoriques mais beaucoup de savoir-faire.</p>
<p>Aussi chaque ouvrage de la collection se divise en trois grandes parties.</p>
<ul>
<li>La première partie liste les savoirs essentiels du cours. Une fois ces savoirs exposés, une ou des remarques peuvent venir compléter ce qui a été exposé. J'insiste sur le fait qu'un savoir sera toujours utilisé au moins une fois dans la partie « Savoir-faire ».</li>
<li>La seconde partie met en avant les savoir-faire à connaître impérativement, tout ceci s'organisant autour de problématiques bien identifiées et illustrées avec de nombreux exemples. Cette partie se termine systématiquement par des exercices d'entraînement qu'il est important de chercher activement afin que la lecture du corrigé ne soit pas inutile.</li>
<li>La troisième partie est consacrée aux corrigés des exercices. Ces corrigés sont suffisamment détaillés pour être bien compris de tous les étudiants.</li>
</ul>
<p>Finalement, travailler cet ouvrage est la garantie d'un apprentissage efficace du cours afin d'être le plus performant possible le jour du concours.</p>
<p>Karine Beaupère</p>

<!-- Image omitted: image0010008 -->
<h2>Comment utiliser ce livre ?</h2>
<p>Dans la partie « Savoirs », tous les savoirs sont identifiés à l'aide d'un codage et d'une appellation. Ce codage est le suivant : [S + numéro du thème + numéro du savoir dans ce thème].<br>
Par exemple, [S8.3] représente le savoir n°3 du thème 8.<br>
C'est ce codage qui sera utilisé dans la partie « Savoir-faire » pour faire référence à un savoir précis.</p>
<p>Dans la partie « Savoir-faire », on retrouve les mêmes thèmes que ceux de la partie « Savoirs », dans le même ordre.<br>
Tout y est organisé autour de problématiques.<br>
Une fois une ou plusieurs méthodes exposées rapidement pour une même problématique, on rentre dans le détail des différents savoir-faire liés à la problématique étudiée.<br>
Comme ci-dessus, les savoir-faire sont identifiés à l'aide d'un codage et d'une action décrivant ce savoir-faire.<br>
Par exemple, [SF3.5] représente le savoir-faire n°5 du thème 3.</p>
<p>Chaque savoir-faire est ensuite illustré d'un ou plusieurs exemple(s), toujours étudié(s) en détail et parfois complété(s) de différentes remarques.</p>
<p>La lecture de la table des matières, située en fin de livre, peut être précieuse car elle permet, d'un seul coup d'œil, de visualiser, par thème, toutes les problématiques et les savoir-faire qui y sont reliés.</p>
<p>Enfin, une fois toutes les problématiques exposées, c'est à vous de jouer !<br>
Pour cela, plusieurs exercices minutieusement choisis sont proposés.<br>
Le codage des exercices relève du même principe que ce qui a été vu plus haut. De plus, en haut à droite de l'énoncé sont précisés les savoir-faire utilisés dans la résolution de l'exercice.<br>
Il est absolument essentiel de chercher activement ces exercices afin que la lecture du corrigé ne soit pas inutile.</p>
<p>L'idéal est d'utiliser cet ouvrage régulièrement dans l'année, en particulier à la fin de chaque chapitre, afin de faire le point sur les savoirs et les savoir-faire.<br>
L'étude de la première partie permet en effet de voir si l'on a appris et retenu les « bons savoirs ».<br>
Puis, l'étude de la seconde partie permet de voir si l'on a bien cerné les différentes problématiques liées à un thème donné et surtout, cette étude permet de voir si l'on a bien en tête les savoir-faire répondant à une problématique donnée.<br>
Le travail sur les exercices proposés doit finalement permettre de vérifier que les savoirs et les savoir-faire sont bien acquis.</p>

<!-- Image omitted: image0010009 -->
<h2>Quelques conseils pour bien apprendre</h2>
<p>Il est important d'apprendre régulièrement son cours et d'être actif lors de cet apprentissage.<br>
Cela implique donc de reprendre ses notes très rapidement après le cours fait au lycée, pour ne pas accumuler de retard dans les apprentissages. Il est par exemple essentiel d'arriver à un cours en étant bien évidemment au point sur le (ou les) cours précédent(s).</p>
<p>Mais cela signifie aussi une vraie implication dans cet apprentissage. Pour cela, il faut se poser un certain nombre de questions afin de ne pas subir son cours. Voici quelques questions que vous pouvez vous poser régulièrement, notamment en relisant votre cours mais aussi en travaillant les corrigés des exercices et des devoirs.</p>
<ul>
<li>Ai-je bien tout compris à cette définition, cette propriété ou ce théorème ?</li>
<li>Cette définition est-elle essentielle en pratique ?</li>
<li>Cette propriété est-elle vraiment importante ?</li>
<li>Quelle est la problématique de la question posée dans l'exercice ?</li>
<li>Quelles sont les méthodes mises en œuvre pour répondre à cette question ?</li>
<li>Pourquoi ai-je échoué à cette question ?</li>
<li>Ce que j'écris est-il cohérent ? Si ce n'est pas le cas, il faut en être conscient et ne pas laisser les choses en l'état : soit vous trouvez l'origine de l'incohérence et vous arrivez à vous corriger soit vous ne voyez pas d'où cela vient mais vous mettez clairement en avant que vous êtes conscient de cette incohérence.</li>
</ul>
<p>N'hésitez pas à annoter vos cours, exercices et devoirs avec toutes les réponses aux questions que vous vous posez. C'est en faisant vivre vos notes que l'apprentissage se fait en profondeur.</p>
<p>Cet investissement actif prend du temps mais il est synonyme d'une assimilation véritable des connaissances et des pratiques donc cet investissement, à long terme, est très rentable.</p>
<p>Bon courage!</p>

<!-- Image omitted: image0010010 -->
<p>Je tiens à remercier chaleureusement $M^{me}$ Karine Beaurpère pour ses suggestions et sa relecture minutieuse de l'ouvrage, ainsi que les Éditions Ellipses pour le soin apporté à la mise en page de cet ouvrage et pour la patience dont elles ont fait preuve.</p>
<p>C'est avec plaisir que je recevrai toutes vos remarques, questions, critiques et suggestions; pour cela n'hésitez pas à m'écrire à l'adresse : livres@tlegay.fr</p>

<!-- Image omitted: image0010011 -->
<h1>SAVOIRS</h1>

<!-- Image omitted: image0010012 -->
<!-- Page 8 -->
<h1>Thème 1 - Structures algébriques usuelles</h1>
<h2>[S1.1] Magmas</h2>
<ul>
<li>Un magma est un couple $(E,*)$, où $E$ est un ensemble et $*$ une loi de composition interne sur $E$, c'est-à-dire une application de $E^2$ dans $E$.</li>
</ul>
<p>Si $(E,*)$ est un magma, une partie $F$ de $E$ est dite stable par la loi $*$ si :
$$\forall (x,y) \in F^2, x * y \in F. $$La restriction de la loi $*$ à $F^2$ s'appelle alors la loi induite sur $F$.</p>
<ul>
<li><strong>Associativité</strong><br>
Soit $(E,*)$ un magma. La loi $*$ est dite associative si :
$$\forall (x,y,z) \in E^3, (x * y) * z = x * (y * z). $$Si tel est le cas, pour tout élément $x$ de $E$, et tout entier $n$ de $\mathbb{N}^*$, on pourra définir l'itéré $n^e$ de $x$ par : $x^{(n)} = \underbrace{x * x * \ldots * x}_{n \text{ fois}}$ (noté $x^n$ si la loi est notée multiplicativement, ou $nx$ si la loi est notée additivement).<br>
On a facilement, pour tous $n$ et $m$ de $\mathbb{N}^*$ :
$$ x^{(n+m)} = x^{(n)} * x^{(m)} \text{ et } \left(x^{(n)}\right)^{(m)} = x^{(nm)}. $$</li>
<li><strong>Commutativité</strong><br>
Soit $(E,*)$ un magma. Deux éléments $x$ et $y$ de $E$ sont dits permutables (ou qu'ils commutent) si $x * y = y * x$.<br>
Si $x$ et $y$ sont deux éléments permutables d'un magma associatif $(E,*)$, alors :
<ul>
<li>pour tous entiers $n$ et $m$ de $\mathbb{N}^*$, $x^{(n)}$ et $y^{(m)}$ sont permutables ;</li>
<li>pour tout entier $n \in \mathbb{N}^*$, $x^{(n)} * y^{(n)} = (x * y)^{(n)}$.</li>
</ul>
La loi $*$ est dite commutative si tous les éléments commutent entre eux, soit :
$$\forall (x,y) \in E^2, x * y = y * x. $$</li>
<li><strong>Élément neutre</strong><br>
On appelle élément neutre d'un magma $(E,*)$ un élément $e \in E$ tel que :
$$\forall x \in E, x * e = e * x = x. $$Si $(E,*)$ possède un élément neutre, celui-ci est unique. Un magma qui possède un élément neutre est dit unifère.</li>
<li><strong>Symétrique</strong><br>
Soit $(E,*)$ un magma unifère d'élément neutre $e$, et $x$ un élément de $E$.<br>
On appelle symétrique à gauche (respectivement à droite) de $x$ un élément $x'$ (respectivement $x''$) de $E$, s'il existe, tel que :
$$ x' * x = e \quad (\text{respectivement } x * x'' = e). $$Si $x'$ (respectivement $x''$) existe, on dit que $x$ est symétrisable à gauche (respectivement à droite).</li>
</ul>

<!-- Image omitted: image0010013 -->
<!-- Page 9 -->
<ul>
<li>Soit $(E, *)$ un magma unifère d'élément neutre $e$, et $x$ un élément de $E$. On appelle symétrique de $x$ un élément $x'$ de $E$, s'il existe, tel que :
$$x' * x = x * x' = e.
$$
Si $x'$ existe, $x$ est dit symétrisable.<br>
Cela équivaut à dire que $x$ est symétrisable à droite et à gauche et que ses symétriques à droite et à gauche sont égaux.<br>
Exemple : dans l'ensemble $\mathcal{A}(E, E)$ des applications de $E$ dans $E$, muni de la loi $o$, les éléments symétrisables à droite sont les applications surjectives et les éléments symétrisables à gauche sont les applications injectives. Les éléments symétrisables sont donc les applications bijectives de $E$ sur $E$.</li>
<li>Soit $(E, *)$ un magma associatif et unifère ( $(E, *)$ s'appelle alors un monoïde).
<ul>
<li>Si un élément $x$ de $E$ admet un symétrique à droite et un symétrique à gauche, ceux-ci sont égaux (et $x$ est alors symétrisable).</li>
<li>Si un élément $x$ de $E$ est symétrisable, son symétrique est unique.</li>
<li>Si un élément $x$ de $E$ est symétrisable, de symétrique $x'$, alors $x'$ est symétrisable, de symétrique $x$.</li>
<li>Si $x$ et $y$ sont symétrisables (de symétriques respectifs $x'$ et $y'$) , il en est de même de $x * y$ et : $(x * y)' = y' * x'$.</li>
</ul>
✓ Lorsque la loi est notée multiplicativement, on parle d'inverse au lieu de symétrique. Lorsqu'elle est notée additivement, on parle d'opposé.</li>
<li>Soit $(E, *)$ un magma unifère, et $x$ un élément de $E$. On note : $x^{(0)} = e$. Si $x$ est symétrisable, de symétrique $x'$, pour tout $n \in \mathbb{N}^*$ On note $x^{(-n)}$ l'élément $x'^{(n)}$ (ainsi, $x' = x^{(-1)}$).<br>
On peut ainsi, lorsque $x$ est symétrisable, étendre la notation $x^{(n)}$ pour $n \in \mathbb{Z}$.</li>
<li>Soit $x$ un élément symétrisable de $E$. Alors, pour tous $n$ et $m$ de $\mathbb{Z}$, on a :
$$x^{(n+m)} = x^{(n)} * x^{(m)} \quad \text{et} \quad (x^{(n)})^{(m)} = x^{(nm)}.
$$</li>
<li>Soient $x$ et $y$ deux éléments permutables et symétrisables de $E$. Alors :
<ul>
<li>pour tous entiers $n$ et $m$ de $\mathbb{Z}$, $x^{(n)}$ et $y^{(m)}$ sont permutables;</li>
<li>pour tout entier $n \in \mathbb{Z}$, $x^{(n)} * y^{(n)} = (x * y)^{(n)}$.</li>
</ul>
</li>
<li><strong>Éléments réguliers</strong><br>
Soit $(E, *)$ un magma. Un élément $a$ de $E$ est dit régulier (ou simplifiable) à gauche (respectivement à droite) si :
$$\forall (x, y) \in E^2, a * x = a * y \Rightarrow x = y \quad (\text{respectivement } x * a = y * a \Rightarrow x = y).
$$
Un élément $a$ de $E$ est dit régulier s'il est à la fois régulier à gauche et à droite. Soit $(E, *)$ un magma associatif et unifère. Si un élément $a$ de $E$ est symétrisable, (à droite, à gauche), alors il est régulier (à droite, à gauche).<br>
✓ La réciproque de cette proposition est fausse en général. Par exemple, dans $(\mathbb{Z}, \times)$, 2 est régulier mais non symétrisable.</li>
</ul>

<!-- Image omitted: image0010014 -->
<!-- Page 10 -->
<h2>[S1.2] Morphismes de magmas</h2>
<ul>
<li>Soient $(E,*)$ et $(F,\square)$ deux magmas.<br>
On dit qu'une application $f: E \rightarrow F$ est un morphisme de $(E,*)$ dans $(F,\square)$ si :
$$\forall (x,y) \in E^2, \, f(x * y) = f(x) \square f(y).
$$
Un isomorphisme est un morphisme bijectif. Un endomorphisme est un morphisme de $(E,*)$ dans lui-même. Un automorphisme est un endomorphisme bijectif.<br>
Exemple : si $(E,*)$ est un magma associatif unifère et si $x$ est un élément symétrisable de $E$, l'application $n \mapsto x^{(n)}$ est un morphisme de $(\mathbb{Z},+)$ dans $(E,*)$.</li>
<li>Si $f$ est un morphisme de $(E,*)$ dans $(F,\square)$ et si $g$ un morphisme de $(F,\square)$ dans $(G,\triangle)$, la composée $g \circ f$ est un morphisme de $(E,*)$ dans $(G,\triangle)$.</li>
<li>Si $f$ est un isomorphisme de $(E,*)$ dans $(F,\square)$, alors son application réciproque $f^{-1}$ est un isomorphisme de $(F,\square)$ dans $(E,*)$.<br>
Exemple : l'application $x \mapsto e^x$ est un isomorphisme de $(\mathbb{R},+)$ sur $(\mathbb{R}_+^*,\times)$.</li>
</ul>
<h3>Transport de structure</h3>
<p>Soit $f: (E,*) \rightarrow (F,\square)$ un morphisme de magmas.</p>
<ul>
<li>L'image $f(E)$ de $E$ par $f$ est une partie stable de $(F,\square)$.</li>
<li>Si $*$ est commutative, alors $\square$ est commutative dans le magma $(f(E),\square)$.</li>
<li>Si $*$ est associative, alors $\square$ est associative dans le magma $(f(E),\square)$.</li>
<li>Si $e$ est l'élément neutre de $(E,*)$, alors $f(e)$ est l'élément neutre de $(f(E),\square)$.</li>
<li>Si $x$ est symétrisable dans $(E,*)$, de symétrique $x'$, alors $f(x)$ est symétrisable dans $(f(E),\square)$, de symétrique $f(x')$, et on a alors, pour tout $n \in \mathbb{Z}$, $f(x^{(n)}) = (f(x))^{(n)}$.</li>
</ul>
<h2>[S1.3] Groupes</h2>
<ul>
<li>On appelle groupe un magma $(G,*)$ tel que :
<ul>
<li>(i)$*$ est associative;</li>
<li>(ii)$*$ possède un élément neutre (généralement noté $e_G$);</li>
<li>(iii) tout élément de $G$ est symétrisable pour la loi $*$.</li>
</ul>
Le groupe est dit abélien (ou commutatif) si, de plus, la loi $*$ est commutative.<br>
Exemple : soit $E$ un ensemble non vide. Alors l'ensemble $S(E)$ des permutations de $E$ (c'est-à-dire l'ensemble des bijections de $E$ dans $E$) est un groupe pour la loi $\circ$; ce groupe n'est pas commutatif dès que $\text{Card}(E) \geq 3$.</li>
</ul>
<h3>Produit de groupes</h3>
<p>Soient $(G,*_G)$ et $(H,*_H)$ deux groupes. On peut alors munir l'ensemble produit $G \times H$ de la loi $\square$ définie par :
$$\forall (x_1,y_1),(x_2,y_2) \in (G \times H)^2, \, (x_1,y_1) \square (x_2,y_2) = (x_1 *_G x_2, y_1 *_H y_2).
$$
Alors $(G \times H, \square)$ est un groupe, appelé groupe produit de $G$ et $H$; son élément neutre est $(e_G, e_H)$.</p>

<!-- Image omitted: image0010015 -->
<!-- Page 11 -->
<p>On peut bien sûr étendre cette définition à un produit d'un nombre fini quelconque de groupes.</p>
<h2>[S1.4] Sous-groupe</h2>
<ul>
<li>Soit $(G, *)$ un groupe. On dit qu'une <em>partie</em> $H$ de $G$ est un sous-groupe de $G$ si $(H, *)$ est encore un groupe.<br>
Si $H$ est un sous-groupe de $G$, alors :
<ul>
<li>l'élément neutre de $H$ est celui de $G$;</li>
<li>si $x$ est un élément de $H$, son symétrique dans $H$ est le même que dans $G$.</li>
</ul>
</li>
</ul>
<h3>Caractérisation d'un sous-groupe</h3>
<p>Soit $(G, *)$ un groupe. Pour qu'une <em>partie</em> $H$ de $G$ soit un sous-groupe de $G$, il faut et il suffit que les trois conditions suivantes soient vérifiées :</p>
<ol type="i">
<li>$H \neq \emptyset$;</li>
<li>$H$ est stable par la loi $*$;</li>
<li>pour tout élément $x$ de $H$, son symétrique $x^{-1}$ est dans $H$.</li>
</ol>
<p>Ces trois conditions sont aussi équivalentes aux deux conditions suivantes :</p>
<ol type="i">
<li>$H \neq \emptyset$;</li>
<li>$\forall (x, y) \in H^2, x * y^{-1} \in H$.</li>
</ol>
<p>Lorsque la loi est notée additivement, cette dernière condition s'écrit : $\forall (x, y) \in H^2, x - y \in H$.</p>
<h3>Exemples</h3>
<ul>
<li>$\{\{-1, 1\}, \times\}$ est un sous-groupe de $(\mathbb{R}^*, \times)$.</li>
<li>L'ensemble $U$ des nombres complexes de module égal à 1 est un sous-groupe de $(\mathbb{C}^*, \times)$ (il s'agit du <em>cercle unité</em>).</li>
<li>Si $n$ est un entier non nul, l'ensemble $U_n$ des racines $n$-ièmes de l'unité est un sous-groupe de $(U, \times)$.</li>
</ul>
<h3>Théorème de Lagrange</h3>
<p>Si $G$ est un groupe fini, le cardinal de tout sous-groupe de $G$ est un diviseur de $\text{Card } G$.</p>
<h3>Intersection de sous-groupes</h3>
<p>Soit $(G, *)$ un groupe. L'intersection d'une famille $(H_i)_{i \in I}$ de sous-groupes de $G$ est encore un sous-groupe de $G$.<br>
$\checkmark$ La <em>réunion</em> de sous-groupes de $G$ n'est pas en général un sous-groupe de $G$. Plus précisément, si $H$ et $H'$ sont deux sous-groupes de $G$, $H \cup H'$ est encore un sous-groupe de $G$ si et seulement si $H \subset H'$Ou $H' \subset H$.</p>
<h3>Sous-groupe engendré</h3>
<ul>
<li>Soit $(G, *)$ un groupe, et $X$ une partie de $G$. L'intersection de tous les sous-groupes de $G$ contenant $X$ est un sous-groupe de $G$; c'est le plus petit sous-groupe de $G$ contenant $X$ (au sens de l'inclusion); on l'appelle sous-groupe engendré par $X$, et on le note $\text{gr}(X)$.</li>
</ul>

<!-- Image omitted: image0010016 -->
<!-- Page 12 -->
<ul>
<li>Si $X = \emptyset$, $\text{gr}(\emptyset) = \{e_G\}$. Sinon, $\text{gr}(X)$ est exactement l'ensemble des éléments de la forme :
$$x_1 * x_2 * \cdots * x_n
$$
où $n \in \mathbb{N}^*$ et où pour tout $i \in 〚 1 ; n 〛$, $x_i \in X$Ou $x_i^{-1} \in X$.</li>
</ul>
<h2>[S1.5] Morphismes de groupes</h2>
<ul>
<li>Un morphisme de groupes est (tout simplement) un morphisme entre deux groupes $(G, *)$ et $(H, \square)$.<br>
On définit de la même façon qu'auparavant les notions d'iso-, d'endo- et d'automorphisme de groupes.</li>
</ul>
<h3>Propriétés</h3>
<ul>
<li>Soit $f$ un morphisme d'un groupe $G$ vers un groupe $H$. Alors :
<ul>
<li>$f(e_G) = e_H$;</li>
<li>$\forall x \in G, \forall n \in \mathbb{Z}, f(x^{(n)}) = (f(x))^{(n)}$.</li>
</ul>
</li>
<li>La composée de deux morphismes de groupes est un morphisme de groupes.</li>
<li>Si $f$ est un isomorphisme de groupes, il en est de même de $f^{-1}$.</li>
<li>Si $G$ est un groupe, l'ensemble $\text{Aut}(G)$ des automorphismes de $G$ est un groupe pour la loi $\circ$; c'est un sous-groupe du groupe des permutations $(\mathfrak{S}(G), \circ)$.</li>
</ul>
<h3>Images directe et réciproque d'un sous-groupe</h3>
<ul>
<li>Soit $f$ un morphisme d'un groupe $(G, *)$ vers un groupe $(H, \square)$.
<ul>
<li>Si $G'$ est un sous-groupe de $(G, *)$, son image $f(G')$ est un sous-groupe de $(H, \square)$.</li>
<li>Si $H'$ est un sous-groupe de $(H, \square)$, son image réciproque par $f$, $f^{-1}(H')$, est un sous-groupe de $(G, *)$.</li>
</ul>
</li>
<li>Si $f$ est un morphisme d'un groupe $(G, *)$ vers un groupe $(H, \square)$, on appelle :
<ul>
<li>image de $f$, notée $\text{Im} f$, l'image de $G$ par $f$, soit : $\text{Im} f = \{f(x), x \in G\}$;</li>
<li>noyau de $f$, noté $\text{Ker} f$, l'image réciproque de $\{e_H\}$ par $f$, soit :
$$\text{Ker} f = \{x \in G \mid f(x) = e_H\}.
$$</li>
</ul>
On alors les résultats suivants.
<ul>
<li>$\text{Im} f$ est un sous-groupe de $H$, et : $f$ surjective $\Longleftrightarrow \text{Im} f = H$.</li>
<li>$\text{Ker} f$ est un sous-groupe de $G$, et : $f$ injective $\Longleftrightarrow \text{Ker} f = \{e_G\}$.</li>
</ul>
</li>
</ul>
<h3>Exemples</h3>
<ul>
<li>L'application « déterminant » est un morphisme du magma $(\mathcal{M}_n(\mathbb{K}), \times)$ dans le magma $(\mathbb{R}, \times)$.</li>
<li>C'est aussi un morphisme du groupe $(\text{GL}_n(\mathbb{K}), \times)$ dans le groupe $(\mathbb{R}^*, \times)$.</li>
<li>C'est aussi un morphisme du groupe orthogonal $(\mathcal{O}_n(\mathbb{R}), \times)$ dans $(\{-1, 1\}, \times)$, sous-groupe de $(\mathbb{R}^*, \times)$.<br>
Dans ce cas, son noyau est l'ensemble des matrices orthogonales de déterminant $+1$; c'est le groupe spécial orthogonal $\mathcal{O}_n^+(\mathbb{R})$.</li>
</ul>

<!-- Image omitted: image0010017 -->
<!-- Page 13 -->
<h2>[S1.6] Groupes monogènes et cycliques</h2>
<ul>
<li><strong>Sous-groupes de $(\mathbb{Z}, +)$</strong><br>
Les sous-groupes de $(\mathbb{Z}, +)$ sont exactement les ensembles $n\mathbb{Z}$Où $n \in \mathbb{N}$ (ensemble des multiples de $n$).<br>
$(n\mathbb{Z}, +)$ est le sous-groupe de $(\mathbb{Z}, +)$ engendré par $n$Ou par $-n$.</li>
<li><strong>Le groupe $(\mathbb{Z}/n\mathbb{Z}, +)$</strong>
<h3>Congruences</h3>
Soit $n \in \mathbb{N}$. On dit que deux entiers $x, y \in \mathbb{Z}$ sont congrus modulo $n$, et l'on note $x \equiv y \ (\text{mod} \ n)$, s'il existe $k \in \mathbb{Z}$ tel que $y = x + kn$.<br>
Cela équivaut à dire que $x - y$ appartient à $n\mathbb{Z}$, ou que (lorsque $n \geq 1$), $x$ et $y$Ont le même reste dans la division euclidienne par $n$.<br>
Il s'agit d'une relation d'équivalence sur $\mathbb{Z}$, compatible avec l'addition et la multiplication, c'est-à-dire que pour tous entiers $x, y, x', y'$ :
$$\begin{aligned}
x &\equiv y \ (\text{mod} \ n) \ \text{et} \ x' \equiv y' \ (\text{mod} \ n) \\
&\implies x + x' \equiv y + y' \ (\text{mod} \ n) \ \text{et} \ xx' \equiv yy' \ (\text{mod} \ n).
\end{aligned}
$$
En particulier, pour tout $(x, y) \in \mathbb{Z}^2$ et tout entier naturel $k \in \mathbb{N}$ :
$$x \equiv y \ (\text{mod} \ n) \implies x^k \equiv y^k \ (\text{mod} \ n).
$$</li>
<li>La classe d'équivalence de $x$ modulo $n$ est l'ensemble des $y \in \mathbb{Z}$ congrus à $x$ :
$$\overline{x} = \{y \in \mathbb{Z} \mid x \equiv y \ (\text{mod} \ n)\} = \{x\} + n\mathbb{Z}.
$$
L'ensemble de toutes les classes d'équivalence modulo $n$ se note $\mathbb{Z}/n\mathbb{Z}$.<br>
Si $n = 0$, $\mathbb{Z}/0\mathbb{Z} = \mathbb{Z}$; si $n = 1$, $\mathbb{Z}/\mathbb{Z} = \{0\}$; sinon, pour $n \geq 2$ (ce que l'on supposera pour la suite),
$$\mathbb{Z}/n\mathbb{Z} = \{\overline{0}, \overline{1}, \ldots, \overline{n-1}\}.
$$</li>
<li><strong>Addition dans $\mathbb{Z}/n\mathbb{Z}$</strong><br>
Elle est définie par :
$$\forall (\overline{x}, \overline{y}) \in (\mathbb{Z}/n\mathbb{Z})^2, \ \overline{x} + \overline{y} = \overline{x + y}.
$$
Muni de cette loi, l'ensemble $(\mathbb{Z}/n\mathbb{Z}, +)$ est un groupe abélien; l'application $\pi : \mathbb{Z} \longrightarrow \mathbb{Z}/n\mathbb{Z}$ est alors un morphisme de groupes surjectif, dont le noyau est $n\mathbb{Z}$. $\pi$ s'appelle la surjection canonique de $\mathbb{Z}$ sur $\mathbb{Z}/n\mathbb{Z}$.</li>
<li><strong>Groupe monogène, cyclique</strong><br>
Un groupe $(G, \cdot)$ est dit monogène s'il est engendré par un unique élément $a \in G$. Dans ce cas, en notant la loi multiplicativement :
$$G = \{a^k, k \in \mathbb{Z}\}.
$$
Un groupe est dit cyclique s'il est monogène et de cardinal fini.</li>
</ul>

<!-- Image omitted: image0010018 -->
<!-- Page 14 -->
<ul>
<li>Tout groupe monogène de cardinal infini est isomorphe à $(\mathbb{Z}, +)$.</li>
<li>Si $G$ est un groupe cyclique engendré par $a$ et de cardinal $n \in \mathbb{N}^*$, $G$ est isomorphe au groupe $(\mathbb{Z}/n\mathbb{Z}, +)$. De plus, on a alors : $a^n = e_G$.</li>
</ul>
<h3>Générateurs d'un groupe cyclique</h3>
<p>Soit $n \geq 2$. Les générateurs du groupe $(\mathbb{Z}/n\mathbb{Z}, +)$ sont les éléments $\bar{k}$Où $k$ est premier avec $n$.<br>
Le nombre de ces générateurs, c'est-à-dire le nombre d'entiers de $[\![0; n-1]\!]$ premiers avec $n$ se note $\varphi(n)$; $\varphi$ s'appelle l'indicateur d'Euler.<br>
Par isomorphisme, il en résulte que si $(G, *)$ est un groupe cyclique engendré par $a$ et de cardinal $n$, les générateurs de $G$ sont exactement les éléments $a^k$Où $k \in \mathbb{Z}$ est un entier premier avec $n$.<br>
Exemple : l'ensemble $U_n$ des racines $n^{\text{ème}}$ de l'unité est un sous-groupe cyclique de $(\mathbb{C}^*, \times)$.<br>
Ses générateurs sont les nombres complexes de la forme $e^{\frac{2ik\pi}{n}}$ avec $k \in [\![0; n-1]\!]$ premier avec $n$. Ces nombres sont appelés les racines primitives $n^{\text{ème}}$ de l'unité.</p>
<h2>[S1.7] Ordre d'un élément dans un groupe</h2>
<ul>
<li>Un élément $a$ d'un groupe $(G, *)$ est dit d'ordre fini si le groupe engendré par $a$,
$$\text{gr}(a) = \{a^k, k \in \mathbb{Z}\},
$$
est de cardinal fini $d$. $d$ s'appelle alors l'ordre de $a$.<br>
Si $a$ est d'ordre fini, l'ordre de $a$ est le plus petit entier $n \in \mathbb{N}^*$ tel que $a^n = e_G$; plus précisément, si $d$ est l'ordre de $a$ :
$$a^n = e_G \iff n \text{ multiple de } d.
$$</li>
<li>D'après le théorème de Lagrange, si $(G, *)$ est un groupe fini de cardinal $n$, tout élément $a \in G$ est d'ordre fini et son ordre $d$ divise le cardinal de $G$ ($d \mid n$). En particulier, pour tout $a \in G$, on a $a^n = e_G$.</li>
<li>En regroupant alors les éléments du groupe $(\mathbb{Z}/n\mathbb{Z}, +)$ selon leur ordre, on obtient la propriété suivante de l'indicateur d'Euler $\varphi$ :
$$n = \sum_{d \text{ divise } n} \varphi(d).
$$</li>
</ul>
<h2>[S1.8] Le groupe symétrique $\mathfrak{S}_n$</h2>
<ul>
<li>Soit $n \in \mathbb{N}^*$. On appelle groupe symétrique d'ordre $n$, noté $\mathfrak{S}_n$, l'ensemble des permutations de l'ensemble $[\![1; n]\!]$.<br>
$\mathfrak{S}_n$ est un groupe pour la loi $\circ$ de composition des applications. C'est un groupe fini de cardinal $n!$, non commutatif dès que $n \geq 3$.</li>
</ul>

<!-- Image omitted: image0010019 -->
<!-- Page 15 -->
<h3>Cycles, transpositions</h3>
<ul>
<li>Soit $n \geqslant 2$ et $p \in 〚 2 ; n 〛$. On appelle cycle de longueur $p$ toute permutation $\sigma \in \mathfrak{S}_n$ telle qu'il existe $p$ éléments distincts $a_1, a_2, \ldots, a_p$ de $〚 1 ; n 〛$ tels que :
$$\sigma(a_1) = a_2, \sigma(a_2) = a_3, \ldots, \sigma(a_{p-1}) = a_p, \sigma(a_p) = a_1
$$
et $\forall x \notin \{a_1, \ldots, a_p\}, \sigma(x) = x$.<br>
L'ensemble $\{a_1, a_2, \ldots, a_p\}$ s'appelle le support du cycle.</li>
<li>Un cycle de longueur $p$ est un élément d'ordre $p$ du groupe $(\mathfrak{S}_n, \circ)$.</li>
<li>Un cycle de longueur 2 s'appelle une transposition.</li>
</ul>
<h3>Générateurs du groupe $(\mathfrak{S}_n, \circ)$</h3>
<ul>
<li>Toute permutation de $\mathfrak{S}_n$ peut s'écrire comme composée commutative de cycles à supports disjoints. Cette décomposition est unique à l'ordre près des facteurs.</li>
<li>Toute permutation de $\mathfrak{S}_n$ peut s'écrire comme composée (non commutative et non unique) de transpositions.</li>
</ul>
<h3>Signature d'une permutation</h3>
<ul>
<li>Soit $\sigma \in \mathfrak{S}_n$ ($n \geqslant 2$). On appelle inversion de $\sigma$ tout couple $(i, j) \in 〚 1 ; n 〛^2$ tel que : $i < j$ et $\sigma(i) > \sigma(j)$.<br>
On appelle alors signature de $\sigma$ le nombre $\varepsilon(\sigma) = (-1)^{\nu(\sigma)}$Où $\nu(\sigma)$ est le nombre d'inversions dans $\sigma$. Une permutation est dite paire si $\varepsilon(\sigma) = 1$ et impaire sinon.</li>
<li>Si $\sigma$ est un cycle de longueur $p$, $\varepsilon(\sigma) = (-1)^{p-1}$. En particulier, la signature d'une transposition est égale à $-1$.</li>
<li>La fonction $\varepsilon$ est un morphisme du groupe $(\mathfrak{S}_n, \circ)$ sur le groupe $(\{-1, 1\}, \times)$ :
$$\forall \sigma, \sigma' \in \mathfrak{S}_n, \varepsilon(\sigma \circ \sigma') = \varepsilon(\sigma) \varepsilon(\sigma').
$$
Son noyau, c'est-à-dire l'ensemble des permutations paires, est un sous-groupe de $(\mathfrak{S}_n, \circ)$, noté $A_n$ et appelé groupe alterné d'ordre $n$.</li>
</ul>
<h2>[S1.9] Anneaux</h2>
<ul>
<li>Un anneau $(A, +, \times)$ est un triplet formé d'un ensemble $A$, de deux lois de compositions internes $+$ et $\times$, tel que :
<ul>
<li>(i)$(A, +)$ est un groupe abélien (dont l'élément neutre sera noté $0_A$);</li>
<li>(ii)$\times$ est associative et possède un élément neutre (noté $1_A$ et aussi appelé élément unité);</li>
<li>(iii)$\times$ est distributive à gauche et à droite par rapport à $+$, c'est-à-dire :
$$\forall (x, y, z) \in A^3, x(y + z) = xy + xz \text{ et } (y + z)x = yx + zx.
$$</li>
</ul>
L'anneau est dit commutatif si la loi $\times$ est commutative.<br>
$\checkmark$ Une loi peut être distributive à droite mais pas à gauche; c'est le cas par exemple des lois $+$ et $\circ$ dans $\mathcal{A}(\mathbb{R}, \mathbb{R})$</li>
</ul>

<!-- Image omitted: image0010020 -->
<!-- Page 16 -->
<h3>Règles de calcul dans un anneau</h3>
<p>Soit $(A, +, \times)$ un anneau.</p>
<ul>
<li>$\forall x \in A, 0_A \times x = x \times 0_A = 0_A$.</li>
<li>$\forall (x, y) \in A^2, x(-y) = (-x)y = -(x \times y)$ et $(-x)(-y) = xy$.</li>
<li>$\forall (x, y, z) \in A^3, x(y - z) = xy - xz$ et $(y - z)x = yx - zx$.</li>
<li>Si $x$ et $y$ sont deux éléments permutables de $A$, alors, pour tout $n \in \mathbb{N}$ :
$$(x + y)^n = \sum_{k=0}^{n} \binom{n}{k} x^k y^{n-k} \quad \text{(formule du binôme)}.$$</li>
<li>Si $x$ et $y$ sont deux éléments permutables de $A$, alors, pour tout $n \in \mathbb{N}^*$ :
$$x^n - y^n = (x - y)(x^{n-1} + x^{n-2}y + \cdots + xy^{n-2} + y^{n-1}).$$En particulier, pour tout $x \in A$ :
$$1_A - x^n = (1_A - x)(1_A + x + \cdots + x^{n-1}).$$</li>
</ul>
<h3>Groupe des éléments inversibles</h3>
<p>Soit $(A, +, \times)$ un anneau.<br>
L'ensemble $\mathcal{U}(A)$ des éléments inversibles (pour la loi $\times$) de l'anneau $A$ est un groupe pour la loi $\times$ (d'élément neutre $1_A$).</p>
<h3>Sous-anneaux</h3>
<ul>
<li>Soit $(A, +, \times)$ un anneau, et $B$ une partie de $A$.<br>
On dit que $B$ est un sous-anneau de $A$ si $(B, +, \times)$ est encore un anneau, de même élément unité que $A$.</li>
</ul>
<h4>Caractérisation d'un sous-anneau</h4>
<p>Soit $(A, +, \times)$ un anneau, et $B$ une partie de $A$. Pour que $B$ soit un sous-anneau de $A$, il faut et il suffit que les trois conditions suivantes soient vérifiées :</p>
<ol type="i">
<li>$\forall (x, y) \in B^2, x - y \in B$;</li>
<li>$\forall (x, y) \in B^2, xy \in B$;</li>
<li>$1_A \in B$.</li>
</ol>
<h3>Anneau produit</h3>
<p>Si $(A, +, \cdot)$ et $(B, +, \cdot)$ sont deux anneaux, l'ensemble $A \times B$ est muni d'une structure d'anneau par les lois définies, pour tous $(a, b), (a', b')$ de $A \times B$, par :
$$(a, b) + (a', b') = (a + a', b + b') \quad \text{et} \quad (a, b) \times (a', b') = (a \cdot a', b \cdot b').$$</p>
<h3>Morphismes d'anneaux</h3>
<ul>
<li>Soient $A$ et $A'$ deux anneaux (dont les lois sont notées de façon identique).<br>
Une application $f$ de $A$ dans $A'$ est appelée un morphisme d'anneaux si :
<ol type="i">
<li>$f(1_A) = 1_{A'}$;</li>
<li>$\forall (x, y) \in A^2, f(x + y) = f(x) + f(y)$ et $f(xy) = f(x)f(y)$.</li>
</ol>
</li>
</ul>

<!-- Image omitted: image0010021 -->
<!-- Page 17 -->
<ul>
<li>Soit $f : A \rightarrow A'$ un morphisme d'anneaux.
<ul>
<li>Si $B$ est un sous-anneau de $A$, $f(B)$ est un sous-anneau de $A'$.</li>
<li>Si $B'$ est un sous-anneau de $A'$, son image réciproque $f^{-1}(B')$ est un sous-anneau de $A$.</li>
</ul>
</li>
</ul>
<h2>Anneau intègre</h2>
<ul>
<li>Un anneau $(A, +, \times)$ est dit intègre s'il n'est pas réduit à $\{0\}$ et si :
$$
\forall (a, b) \in A^2, \, ab = 0 \implies a = 0 \text{ ou } b = 0.
$$
Ainsi, un anneau $A$ est intègre si et seulement si le produit de deux éléments de $A$ différents de $0_A$ est différent de $0_A$.<br>
Dans un anneau intègre, tous les éléments sont réguliers pour la loi $\times$.</li>
</ul>
<h2>Exemples</h2>
<ul>
<li>Si $\mathbb{K}$ est un corps, l'anneau $\mathbb{K}[X]$ des polynômes à coefficients dans $\mathbb{K}$ est intègre.</li>
<li>Pour $n \geq 2$, l'anneau des matrices carrées $\mathcal{M}_n(\mathbb{K})$ n'est pas intègre.</li>
</ul>
<h2>[S1.10] Idéaux d'un anneau commutatif</h2>
<ul>
<li>Soit $(A, +, \times)$ un anneau commutatif. Une partie $I$ de $A$ est un idéal de $A$ si c'est un sous-groupe additif et s'il est absorbant pour la loi $\times$, c'est-à-dire :
<ol type="i">
<li>$(I, +)$ est un sous-groupe de $(A, +)$;</li>
<li>$\forall a \in A, \forall x \in I, \, ax \in I$.</li>
</ol>
</li>
</ul>
<h3>Caractérisation d'un idéal</h3>
<p>Une partie $I$ d'un anneau commutatif $(A, +, \times)$ est un idéal de $A$ si et seulement si :</p>
<ol type="i">
<li>$I \neq \emptyset$;</li>
<li>$\forall (x, y) \in I^2, \, x + y \in I$;</li>
<li>$\forall a \in A, \forall x \in I, \, ax \in I$.</li>
</ol>
<ul>
<li>L'intersection d'une famille quelconque d'idéaux d'un anneau commutatif $A$ est encore un idéal de $A$.</li>
<li>Si $I_1, \ldots, I_p$ sont des idéaux d'un d'un anneau commutatif $A$, l'ensemble :
$$
I_1 + \cdots + I_p = \{x_1 + \cdots + x_p \mid \forall k \in 〚 1 ; p 〛, \, x_k \in I_k\}
$$
est encore un idéal de $A$.</li>
</ul>
<h3>Idéal engendré</h3>
<ul>
<li>Soit $X$ une partie quelconque d'un anneau commutatif $A$.<br>
L'intersection de tous les idéaux de $A$ qui contiennent $X$ est encore un idéal de $A$; c'est le plus petit idéal de $A$ contenant $X$ (au sens de l'inclusion); on l'appelle idéal engendré par $X$, et on le note $\text{id}(X)$.</li>
</ul>

<!-- Image omitted: image0010022 -->
<!-- Page 18 -->
<ul>
<li>Soit $X$ une partie non vide d'un anneau commutatif $A$. $\text{id}(X)$ est alors l'ensemble de tous les éléments de la forme :
$$a_1 x_1 + \ldots a_n x_n \quad \text{avec} \ n \in \mathbb{N}^*, \ a_i \in A, \ x_i \in X.$$</li>
<li>En particulier, l'idéal engendré par un élément $\{x\}$ est l'ensemble :
$$\text{id}(\{x\}) = \{ax, \ a \in A\}.$$
On le note aussi : $Ax$, ou $xA$ (ces deux écritures désignent le même ensemble puisque l'anneau $A$ est supposé commutatif; sinon, il faudrait distinguer les notions d'idéal à gauche et d'idéal à droite).</li>
<li><strong>Exemples dans l'anneau $\mathbb{Z}$</strong>
<ul>
<li>Si $n \in \mathbb{N}$, $\text{id}(\{n\}) = n\mathbb{Z}$.</li>
<li>Soient $a, b \in \mathbb{Z}$;
$$\text{id}(\{a, b\}) = a\mathbb{Z} + b\mathbb{Z} = d\mathbb{Z}, \ \text{avec} \ d = \text{pgcd}(a, b) \ (\text{noté} \ a \wedge b);$$$$a\mathbb{Z} \cap b\mathbb{Z} = m\mathbb{Z}, \ \text{avec} \ m = \text{ppcm}(a, b) \ (\text{noté} \ a \vee b).$$</li>
</ul>
</li>
<li>Un idéal $I$ d'un anneau commutatif $A$ est dit principal s'il est engendré par un seul élément (c'est-à-dire s'il existe $x \in A$ tel que $I = Ax$).<br>
Un anneau commutatif $A$ est dit principal s'il est intègre et si tous ses idéaux sont principaux.<br>
<strong>Exemples</strong>
<ul>
<li>L'anneau $\mathbb{Z}$ est principal : les idéaux de l'anneau $\mathbb{Z}$ sont les ensembles $n\mathbb{Z}, \ n \in \mathbb{N}$.</li>
<li>Si $\mathbb{K}$ est un corps, l'anneau $\mathbb{K}[X]$ est principal (voir ci-après).</li>
</ul>
</li>
<li><strong>Morphismes d'anneaux</strong><br>
Soient $A$ et $A'$ deux anneaux commutatifs (dont les lois sont notées de la même façon), et $f : A \to A'$ un morphisme d'anneaux.<br>
(i) Si $I$ est un idéal de $A$, son image $f(I)$ est un idéal du sous-anneau $f(A)$.<br>
(ii) Si $I'$ est un idéal de $A'$, son image réciproque $f^{-1}(I')$ est un idéal de $A$.<br>
En particulier, le noyau de $f$ :
$$\text{Ker} \ f = \{x \in A \ | \ f(x) = 0_{A'}\},$$
est un idéal de $A$.</li>
</ul>
<h2>[S1.11] L'anneau $\mathbb{Z}/n\mathbb{Z} \ (n \geq 2)$</h2>
<ul>
<li>La congruence modulo $n$ étant compatible avec la multiplication dans $\mathbb{Z}$, on peut définir dans l'ensemble quotient $\mathbb{Z}/n\mathbb{Z}$ une multiplication interne par :
$$\forall \ (\overline{x}, \overline{y}) \in (\mathbb{Z}/n\mathbb{Z})^2, \ \overline{x} \cdot \overline{y} = \overline{xy}.$$
L'ensemble $(\mathbb{Z}/n\mathbb{Z}, +, \cdot)$ est un anneau commutatif.</li>
</ul>

<!-- Image omitted: image0010023 -->
<!-- Page 19 -->
<h3>Éléments inversibles</h3>
<p>Les éléments inversibles de l'anneau $\mathbb{Z}/n\mathbb{Z}$ sont les $\bar{x}$ avec $x$ premier avec $n$. Leur nombre est égal à $\varphi(n)$, où $\varphi$ est l'indicateur d'Euler. Le groupe des éléments inversibles $(\mathcal{U}(\mathbb{Z}/n\mathbb{Z}), \times)$ étant de cardinal $\varphi(n)$, on a :
$$\forall a \in \mathbb{Z}, a \wedge n = 1 \implies a^{\varphi(n)} \equiv 1 \ (\text{mod} \ n)
$$
(c'est le théorème d'Euler).</p>
<h3>Théorème des restes chinois</h3>
<ul>
<li>Si $x \in \mathbb{Z}$, nous noterons $\bar{x}_{(n)}$ sa classe d'équivalence modulo $n$. Si $n$ et $m$ sont deux entiers premiers entre eux, l'application :
$$\phi : \begin{cases}
\mathbb{Z}/nm\mathbb{Z} & \longrightarrow \mathbb{Z}/n\mathbb{Z} \times \mathbb{Z}/m\mathbb{Z} \\
\bar{x}_{(nm)} & \longmapsto (\bar{x}_{(n)}, \bar{x}_{(m)})
\end{cases}
$$
est un isomorphisme d'anneaux.</li>
<li><strong>Conséquence : système de congruences</strong><br>
Soient $n$ et $m$ deux entiers premiers entre eux, et $a, b \in \mathbb{Z}$. L'ensemble des solutions du système :
$$\begin{cases}
x \equiv a \ (\text{mod} \ n) \\
x \equiv b \ (\text{mod} \ m)
\end{cases}
$$
est de la forme $\mathcal{S} = \{x_0 + kmn, k \in \mathbb{Z}\}$Où $x_0$ est une solution particulière appartenant à $[[0; mn - 1]]$.</li>
</ul>
<h3>Calcul de l'indicateur d'Euler</h3>
<p>La restriction de l'isomorphisme $\phi$ ci-dessus est un isomorphisme entre le groupe $(\mathcal{U}(\mathbb{Z}/nm\mathbb{Z}), \times)$ et le groupe produit $(\mathcal{U}(\mathbb{Z}/n\mathbb{Z}), \times) \times (\mathcal{U}(\mathbb{Z}/m\mathbb{Z}), \times)$ des éléments inversibles.<br>
Il en résulte que, si $\varphi$ désigne l'indicateur d'Euler :
$$n \wedge m = 1 \implies \varphi(nm) = \varphi(n)\varphi(m),
$$
puis que, si $n = p_1^{\alpha_1} p_2^{\alpha_2} \ldots p_r^{\alpha_r}$ est la décomposition de $n$ en facteurs premiers :
$$\varphi(n) = n \left(1 - \frac{1}{p_1}\right) \left(1 - \frac{1}{p_2}\right) \times \ldots \times \left(1 - \frac{1}{p_r}\right).
$$</p>
<h2>[S1.12] L'anneau $\mathbb{K}[X]$</h2>
<p>$\mathbb{K}$ désigne ici un sous-corps de $\mathbb{C}$. Les résultats de $1^{re}$ année concernant les polynômes à coefficients réels ou complexes s'étendent au cas des polynômes à coefficients dans $\mathbb{K}$. Nous rappelons brièvement les principaux résultats.</p>
<h3>Construction</h3>
<ul>
<li>Soit $\mathbb{K}$ un sous-corps de $\mathbb{C}$. On appelle polynôme à coefficients dans $\mathbb{K}$ toute suite $P = (a_n)_{n \in \mathbb{N}}$ d'éléments de $\mathbb{K}$ qui est nulle à partir d'un certain rang. Leur ensemble est noté $\mathbb{K}[X]$.</li>
<li>Pour les lois usuelles sur les suites (addition et multiplication externe), $\mathbb{K}[X]$ est un sous-espace vectoriel de $\mathbb{K}^{\mathbb{N}}$.<br>
On définit de plus une multiplication interne dans $\mathbb{K}[X]$ de la façon suivante :</li>
</ul>

<!-- Image omitted: image0010024 -->
<!-- Page 20 -->
<p>Si $P = (a_n)_{n \in \mathbb{N}}$ et $Q = (b_n)_{n \in \mathbb{N}}$, alors $PQ = (c_n)_{n \in \mathbb{N}}$Où :
$$\forall n \in \mathbb{N}, c_n = \sum_{k=0}^{n} a_k b_{n-k}.
$$
Si l'on note $1$ le polynôme $1 = (1, 0, 0, \ldots)$ et $X$ le polynôme $X = (0, 1, 0, 0, \ldots)$, alors tout polynôme $P \in \mathbb{K}[X]$ s'écrit de façon unique sous la forme :
$$P = \sum_{k=0}^{+\infty} a_k X^k,
$$
où les $a_k$ sont nuls à partir d'un certain rang.<br>
Lorsque $P$ n'est pas le polynôme nul, on peut définir son degré :
$$\deg P = \max \{k \in \mathbb{N} \mid a_k \neq 0\}.
$$
Par convention, le degré du polynôme nul est $-\infty$.<br>
Un polynôme est dit unitaire s'il est non nul et son coefficient dominant est égal à $1$; un polynôme est dit normalisé s'il est nul ou unitaire.<br>
$(\mathbb{K}[X], +, \times)$ est alors un anneau commutatif et intègre. Ses éléments inversibles sont les polynômes constants non nuls.</p>
<h3>Division euclidienne dans $\mathbb{K}[X]$</h3>
<p>Soient $A, B \in \mathbb{K}[X]$ avec $B \neq 0$. Il existe un et un seul couple $(Q, R) \in \mathbb{K}[X]^2$ tel que :
$$A = BQ + R \text{ avec } \deg R < \deg B.
$$</p>
<h3>Idéaux de $\mathbb{K}[X]$</h3>
<p>Tout idéal de l'anneau $\mathbb{K}[X]$ est principal. Plus précisément, si $I$ est un idéal de $\mathbb{K}[X]$, il existe un et un seul polynôme normalisé $A$ tel que :
$$I = A \mathbb{K}[X] = \{AQ \mid Q \in \mathbb{K}[X]\}.
$$</p>
<h3>Racines d'un polynôme</h3>
<p>Si $P = \sum_{k=0}^{d} a_k X^k$ est un polynôme à coefficients dans $\mathbb{K}$, on peut lui associer la fonction polynôme (encore notée $P$ par abus de notation) :
$$P : x \in \mathbb{K} \longmapsto \sum_{k=0}^{d} a_k x^k.
$$
Un élément $a \in \mathbb{K}$ s'appelle une racine de $P$ si $P(a) = 0$, ce qui équivaut à dire que le polynôme $X - a$ divise $P$ dans $\mathbb{K}[X]$.<br>
Plus généralement, $a$ est appelé racine de $P$ d'ordre $k \in \mathbb{N}$ si $(X - a)^k$ divise $P$ et $(X - a)^{k+1}$ ne divise pas $P$.</p>
<h3>Formule de Taylor</h3>
<p>Soit $P \in \mathbb{K}[X]$, de degré $n$, et soit $a \in \mathbb{K}$. Alors :
$$P = \sum_{k=0}^{n} \frac{P^{(k)}(a)}{k!} (X - a)^k.
$$</p>

<!-- Image omitted: image0010025 -->
<!-- Page 21 -->
<p>Consequence : $a \in \mathbb{K}$ est racine d'ordre $k \in \mathbb{N}^*$ de $P$ si et seulement si :
$$P(a) = \cdots = P^{(k-1)}(a) = 0 \quad \text{et} \quad P^{(k)}(a) \neq 0.
$$</p>
<h3>Relations coefficients-racines</h3>
<p>Soit $P = \sum_{k=0}^{n} a_k X^k \in \mathbb{K}[X]$ un polynôme scindé de degré $n \in \mathbb{N}^*$. Notons $x_1, \ldots, x_n$ ses racines, distinctes ou non.<br>
On définit alors les fonctions symétriques élémentaires $\sigma_1, \ldots, \sigma_n$ des racines de $P$ par :
$$\forall k \in 〚 1; n 〛, \sigma_k = \sum_{1 \leq i_1 < i_2 < \cdots < i_k \leq n} x_{i_1} x_{i_2} \cdots x_{i_k}.
$$
En particulier, $\sigma_1$ désigne la somme des $n$ racines et $\sigma_n$ leur produit.<br>
On a alors les relations :
$$\forall k \in 〚 1; n 〛, \sigma_k = (-1)^k \frac{a_{n-k}}{a_n}.
$$</p>
<h3>Polynômes d'interpolation de Lagrange</h3>
<p>Soit $n \in \mathbb{N}$ et soient $a_0, a_1, \ldots, a_n$ $n+1$ scalaires deux à deux distincts.<br>
Pour tout $(b_0, b_1, \ldots, b_n) \in \mathbb{K}^{n+1}$, il existe un et un seul polynôme $P$ de degré inférieur ou égal à $n$ tel que $P(a_i) = b_i$ pour tout $i \in 〚 0; n 〛$.<br>
$P$ s'appelle le polynôme interpolateur de Lagrange relatif aux points $(a_i, b_i)$.<br>
Si l'on note, pour tout $j \in 〚 0; n 〛$ :
$$L_j = \prod_{\substack{i=0 \\ i \neq j}}^n \left( \frac{X - a_i}{a_j - a_i} \right),
$$
on a les propriétés suivantes :</p>
<ul>
<li>$\forall (i, j) \in 〚 0; n 〛^2, L_j(a_i) = \delta_{ij}$;</li>
<li>les polynômes $(L_j)_{0 \leq j \leq n}$ forment une base de $\mathbb{K}_n[X]$;</li>
<li>l'unique polynôme $P$ de $\mathbb{K}_n[X]$ qui vérifie $P(a_i) = b_i$ pour tout $i \in 〚 0; n 〛$ est le polynôme $P = \sum_{j=0}^n b_j L_j$.</li>
</ul>
<h2>[S1.13] Arithmétique dans $\mathbb{K}[X]$</h2>
<p>Le fait que $\mathbb{K}[X]$ soit un anneau principal permet d'étendre à $\mathbb{K}[X]$ les notions d'arithmétique dans $\mathbb{Z}$ vues en $1^{\text{ère}}$ année.</p>
<h3>Divisibilité dans $\mathbb{K}[X]$</h3>
<ul>
<li>On dit qu'un polynôme $A$ divise un polynôme $B$ (ou que $B$ est un multiple de $A$) si et seulement si il existe $Q \in \mathbb{K}[X]$ tel que $B = AQ$.<br>
On note alors : $A \mid B$.<br>
Cela équivaut à dire que l'idéal $\mathbb{K}[X].B$ engendré par $B$ est inclus dans l'idéal $\mathbb{K}[X].A$ engendré par $A$.</li>
</ul>

<!-- Image omitted: image0010026 -->
<!-- Page 22 -->
<ul>
<li>On dit que $A$ et $B$ sont associés si $A$ divise $B$ et si $B$ divise $A$, ce qui équivaut à dire qu'il existe un scalaire $\lambda$ non nul tel que $B = \lambda A$. En particulier, tout polynôme est associé à un polynôme normalisé et un seul.</li>
</ul>
<h3>PGCD et PPCM</h3>
<h4>PGCD de deux polynômes</h4>
<p>Soient $A, B \in \mathbb{K}[X]$. L'idéal engendré par $A$ et $B$ est l'ensemble :
$$A.\mathbb{K}[X] + B.\mathbb{K}[X] = \{AU + BV \mid (U, V) \in \mathbb{K}[X]^2\}.$$
$\mathbb{K}[X]$ étant principal, il existe un polynôme normalisé $D$ et un seul tel que $A.\mathbb{K}[X] + B.\mathbb{K}[X] = D.\mathbb{K}[X]$.<br>
Par définition, $D$ s'appelle le pgcd de $A$ et $B$; il est noté $A \wedge B$.<br>
Ainsi, si $D = A \wedge B$, il existe $U$ et $V \in \mathbb{K}[X]$ tels que $AU + BV = D$ (identité de Bézout), et réciproquement, tout polynôme de la forme $AU + BV$ est un multiple de $D$.</p>
<h4>Caractérisation du pgcd</h4>
<p>$D$ est le pgcd de $A$ et de $B$ si et seulement si :</p>
<ol type="i">
<li>$D$ est normalisé;</li>
<li>$D|A$ et $D|B$;</li>
<li>$\forall P \in \mathbb{K}[X], P|A$ et $P|B \implies P|D$.</li>
</ol>
<h4>PPCM de deux polynômes</h4>
<p>Soient $A, B \in \mathbb{K}[X]$. Le ppcm de $A$ et $B$ est l'unique polynôme normalisé $M$ qui engendre l'idéal $A.\mathbb{K}[X] \cap B.\mathbb{K}[X]$ :
$$A.\mathbb{K}[X] \cap B.\mathbb{K}[X] = M.\mathbb{K}[X].$$
On note $M = A \vee B$.</p>
<h4>Caractérisation du ppcm</h4>
<p>$M$ est le ppcm de $A$ et de $B$ si et seulement si :</p>
<ol type="i">
<li>$M$ est normalisé;</li>
<li>$A|M$ et $B|M$;</li>
<li>$\forall P \in \mathbb{K}[X], A|P$ et $B|P \implies M|P$.</li>
</ol>
<h4>Généralisation</h4>
<p>Si $A_1, \ldots, A_n$ sont des polynômes, on peut définir leur pgcd $D$ et leur ppcm $M$ comme étant les polynômes normalisés tels que :
$$\sum_{i=1}^{n} A_i.\mathbb{K}[X] = D.\mathbb{K}[X] \quad \text{et} \quad \bigcap_{i=1}^{n} A_i.\mathbb{K}[X] = M.\mathbb{K}[X].$$
Les lois « pgcd » et « ppcm » sont associatives; autrement dit, on peut calculer les pgcd et ppcm de $n$ polynômes en calculant de proche en proche :
$$\bigwedge_{i=1}^{n} A_i = \left( \bigwedge_{i=1}^{n-1} A_i \right) \wedge A_n \quad \text{et} \quad \bigvee_{i=1}^{n} A_i = \left( \bigvee_{i=1}^{n-1} A_i \right) \vee A_n.$$</p>

<!-- Image omitted: image0010027 -->
<!-- Page 23 -->
<ul>
<li><strong>Polynômes premiers entre eux</strong>
<ul>
<li>Deux polynômes $A$ et $B$ de $\mathbb{K}[X]$ sont dits premiers entre eux si $A \wedge B = 1$.<br>
Exemple : les polynômes $X - a$ et $X - b$ sont premiers entre eux si et seulement si $a$ est différent de $b$.</li>
<li><strong>Théorème de Bezout</strong><br>
Deux polynômes $A$ et $B$ sont premiers entre eux si et seulement si il existe deux polynômes $U$ et $V$ tels que : $AU + BV = 1$.</li>
<li><strong>Théorème de Gauss</strong><br>
Soient $A, B, C \in \mathbb{K}[X]$.<br>
Si $A$ divise le produit $BC$ et est premier avec $B$, alors $A$ divise $C$.</li>
<li><strong>Propriétés</strong>
<ul>
<li>$(A \wedge B = 1 \text{ et } A \wedge C = 1) \Longleftrightarrow A \wedge BC = 1$.</li>
<li>On en déduit, par récurrence :
<ul>
<li>si $A$ est premier avec $B_1, B_2, \ldots, B_n$, il est premier avec leur produit;</li>
<li>si $A \wedge B = 1$, alors $A^n \wedge B^m = 1$ pour tout $(n, m) \in \mathbb{N}^2$.</li>
</ul>
</li>
<li>Si $A$ est divisible par $B$ et divisible par $C$, et si $B$ et $C$ sont premiers entre eux, alors $A$ est divisible par le produit $BC$ (ce résultat se généralise au cas où $A$ est divisible par $n$ polynômes premiers entre eux deux à deux).<br>
Exemple : si $a_1, \ldots, a_n$ sont $n$ racines deux à deux distinctes d'un polynôme $P$, alors le produit $(X - a_1) \cdots (X - a_n)$ divise $P$.</li>
</ul>
</li>
<li><strong>Généralisation</strong><br>
Des polynômes $A_1, \ldots, A_n$ sont dits premiers entre eux dans leur ensemble si leur pgcd est égal à 1.<br>
Cela équivaut à dire qu'il existe des polynômes $U_1, \ldots, U_n$ tels que $\sum_{i=1}^{n} A_i U_i = 1$ (théorème de Bezout).<br>
Si $n$ polynômes $A_1, \ldots, A_n$ sont dits premiers entre eux deux à deux, ils sont premiers entre eux dans leur ensemble.<br>
$\checkmark$ La réciproque de cette propriété est fausse : considérer par exemple les polynômes $(X - 1)(X - 2)$, $(X - 2)(X - 3)$ et $(X - 1)(X - 3)$.</li>
<li><strong>Polynômes irréductibles</strong>
<ul>
<li>Un polynôme $P \in \mathbb{K}[X]$ est dit irréductible si et seulement si :
<ol type="i">
<li>$P$ n'est pas constant;</li>
<li>les seuls diviseurs de $P$ dans $\mathbb{K}[X]$ sont les polynômes constants et les polynômes associés à $P$.</li>
</ol>
Exemples :
<ul>
<li>si $a \in \mathbb{K}$, le polynôme $X - a$ est irréductible dans $\mathbb{K}[X]$;</li>
<li>le polynôme $X^2 - 2$ est irréductible dans $\mathbb{Q}[X]$ mais pas dans $\mathbb{R}[X]$;</li>
<li>le polynôme $X^2 + 1$ est irréductible dans $\mathbb{R}[X]$ mais pas dans $\mathbb{C}[X]$.</li>
</ul>
</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010028 -->
<!-- Page 24 -->
<h3>Propriétés</h3>
<ul>
<li>Soit $P$ un polynôme irréductible de $\mathbb{K}[X]$ et $A \in \mathbb{K}[X]$.<br>
Alors $A$ et $P$ sont premiers entre eux si et seulement si $A$ n'est pas multiple de $P$.</li>
<li>Si $P$ et $Q$ sont deux polynômes irréductibles, $P$ et $Q$ sont sont premiers entre eux si et seulement si ils ne sont pas associés.</li>
<li>Si $P$ est irréductible et divise le produit $AB$, alors il divise $A$Ou il divise $B$ (ce résultat se généralise au cas où $P$ divise un produit $B_1 B_2 \cdots B_n$).</li>
</ul>
<h3>Décomposition en produit de polynômes irréductibles</h3>
<p>Soit $P$ un polynôme non constant de $\mathbb{K}[X]$.<br>
Alors $P$ peut s'écrire de manière unique (à l'ordre près des facteurs) sous la forme :
$$P = \lambda P_1^{\alpha_1} \cdots P_r^{\alpha_r},
$$
où $\lambda$ est un scalaire non nul, $P_1, \ldots, P_r$ sont des polynômes irréductibles deux à deux distincts et les $\alpha_i$ sont des entiers strictement positifs. $\alpha_i$ s'appelle la valuation de $P_i$ dans $P$.</p>
<h3>Décomposition en produit de polynômes irréductibles dans $\mathbb{C}[X]$</h3>
<p>Les polynômes irréductibles de $\mathbb{C}[X]$ sont les polynômes de degré 1.<br>
Tout polynôme $P$ non constant de $\mathbb{C}[X]$ peut donc s'écrire de façon unique (à l'ordre près) sous la forme :
$$P = \lambda \prod_{i=1}^{r} (X - a_i)^{\alpha_i},
$$
où $\lambda \in \mathbb{C}^*$, les $a_i$ sont des complexes distincts deux à deux et les $\alpha_i$ des entiers naturels non nuls.</p>
<h3>Décomposition en produit de polynômes irréductibles dans $\mathbb{R}[X]$</h3>
<p>Les polynômes irréductibles de $\mathbb{R}[X]$ sont les polynômes de degré 1 et les polynômes de degré 2 à discriminant strictement négatif.<br>
Tout polynôme $P$ non constant de $\mathbb{R}[X]$ peut donc s'écrire de façon unique (à l'ordre près) sous la forme :
$$P = \lambda \prod_{i=1}^{r} (X - a_i)^{\alpha_i} \prod_{j=1}^{s} (X^2 + b_j X + c_j)^{\beta_j},
$$
où $\lambda \in \mathbb{R}^*$, les $a_i$ sont des réels distincts deux à deux, les $X^2 + b_j X + c_j$ des trinômes distincts deux à deux et à discriminant strictement négatif, et les $\alpha_i$, $\beta_j$ des entiers naturels non nuls.</p>
<h2>[S1.14] Corps</h2>
<ul>
<li>Un corps est un anneau commutatif, non réduit à $\{0\}$ dans lequel tout élément non nul est inversible.<br>
Autrement dit, un triplet $(\mathbb{K}, +, \times)$ est un corps si et seulement si :</li>
</ul>

<!-- Image omitted: image0010029 -->
<!-- Page 25 -->
<ol type="i">
<li>$(\mathbb{K}, +)$ est un groupe abélien ;</li>
<li>$(\mathbb{K}^*, \times)$ est un groupe abélien (où $\mathbb{K}^* = \mathbb{K} \setminus \{0_K\}$) ;</li>
<li>$\times$ est distributive par rapport à $+$.</li>
</ol>
<ul>
<li>Si $(\mathbb{K}, +, \times)$ est un corps, l'anneau $(\mathbb{K}, +, \times)$ est intègre.</li>
<li>Un anneau commutatif $(A, +, \times)$ non réduit à $\{0\}$ est un corps si et seulement si ses seuls idéaux sont $\{0\}$ et $A$.<br>
$\checkmark$ Ce résultat montre que la notion d'idéal n'a pas d'intérêt dans un corps !</li>
<li>Tout anneau intègre et fini (non réduit à $\{0\}$) est un corps.</li>
<li>L'anneau $(\mathbb{Z}/p\mathbb{Z}, +, \times)$ est un corps si et seulement si $p$ est un nombre premier. Dans ce cas, $\mathcal{U}(\mathbb{Z}/p\mathbb{Z}) = \mathbb{Z}/p\mathbb{Z} \setminus \{\overline{0}\}$ est un groupe multiplicatif d'ordre $p - 1$. On en déduit le petit théorème de Fermat :<br>
si $p$ est premier et si $a \in \mathbb{Z}$ n'est pas multiple de $p$, $a^{p-1} \equiv 1 \ (\text{mod} \ p)$.<br>
Plus généralement, si $p$ est premier, on a :
$\forall a \in \mathbb{Z}, \ a^p \equiv a \ (\text{mod} \ p)$.</li>
<li><strong>Sous-corps</strong><br>
Soient $(\mathbb{K}, +, \times)$ un corps, et $L$ une partie de $\mathbb{K}$. On dit que $L$ est un sous-corps de $\mathbb{K}$ si $L$ est un sous-anneau de $\mathbb{K}$ qui, pour les lois induites, est aussi un corps. Pour qu'une partie $L$ d'un corps $(\mathbb{K}, +, \times)$ soit un sous-corps de $\mathbb{K}$, il faut et il suffit que :
<ol type="i">
<li>$1_K \in L$ ;</li>
<li>$\forall (x, y) \in L^2, \ x - y \in L$ et $xy \in L$ ;</li>
<li>$\forall x \in L \setminus \{0_K\}, \ x^{-1} \in L$.</li>
</ol>
Exemple : $\mathbb{Q} + \sqrt{3}\mathbb{Q} = \{a + b\sqrt{3} \ ; \ (a, b) \in \mathbb{Q}^2\}$ est un sous-corps de $\mathbb{R}$.</li>
<li><strong>Morphismes de corps</strong><br>
Soient $\mathbb{K}$ et $\mathbb{K}'$ deux corps (dont les lois sont notées de façon identique). Une application $f$ de $\mathbb{K}$ dans $\mathbb{K}'$ est appelée un morphisme de corps si $f$ est un morphisme pour les structures d'anneaux sous-jacentes, c'est-à-dire :
<ol type="i">
<li>$f(1_K) = 1_{K'}$ ;</li>
<li>$\forall (x, y) \in \mathbb{K}^2, \ f(x + y) = f(x) + f(y)$ et $f(xy) = f(x)f(y)$.</li>
</ol>
Si $f : \mathbb{K} \to \mathbb{K}'$ est un morphisme de corps, alors $f$ est injectif.</li>
</ul>
<h2>[S1.15] Algèbres</h2>
<ul>
<li>Soit $(\mathbb{K}, +, \times)$ un corps.<br>
Une algèbre sur $\mathbb{K}$ est un quadruplet $(E, +, \cdot, \times)$, où $E$ est un ensemble muni de deux lois de composition internes $+$ et $\times$ et d'une loi de composition externe $\cdot$ de domaine d'opérateurs $\mathbb{K}$, telles que :</li>
</ul>

<!-- Image omitted: image0010030 -->
<!-- Page 26 -->
<ol type="i">
<li>$(E, +, \times)$ est un anneau ;</li>
<li>$(E, +, \cdot)$ est un $\mathbb{K}$-espace vectoriel (les espaces vectoriels seront étudiés en détail dans le thème 2) ;</li>
<li>$\forall \lambda \in \mathbb{K}, \forall (x, y) \in E^2, \lambda \cdot (x \times y) = (\lambda \cdot x) \times y = x \times (\lambda \cdot y)$.</li>
</ol>
<p>L'algèbre $(E, +, \cdot, \times)$ est dite commutative lorsque l'anneau $(E, +, \times)$ l'est, c'est-à-dire si la loi $\times$ est commutative.<br>
<strong>Exemples</strong></p>
<ul>
<li>L'ensemble des polynômes $\mathbb{K}[X]$ à coefficients dans $\mathbb{K}$ est une $\mathbb{K}$-algèbre commutative et intègre (pour les lois habituelles, rappelées en [S1.12]).</li>
<li>Si $E$ est un $\mathbb{K}$-espace vectoriel, l'ensemble $(\mathscr{L}(E), +, \cdot, \circ)$ des endomorphismes de $E$ est une $\mathbb{K}$-algèbre, non commutative, non intègre, dès que $\dim E \geqslant 2$.</li>
<li>Si $n \in \mathbb{N}^*$, l'ensemble $\mathcal{M}_n(\mathbb{K})$ des matrices carrées d'ordre $n$ à coefficients dans $\mathbb{K}$ est une $\mathbb{K}$-algèbre pour les lois usuelles sur les matrices.</li>
</ul>
<h3>Sous-algèbre</h3>
<p>Soit $E$ une $\mathbb{K}$-algèbre. Une partie $F$ de $E$ est une sous-algèbre de $E$ si $(F, +, \cdot, \times)$ est encore une $\mathbb{K}$-algèbre.<br>
Cela équivaut à dire que $F$ est à la fois un sous-anneau de $(E, +, \times)$ et un sous-espace vectoriel de $(E, +, \cdot)$.</p>
<h4>Caractérisation d'une sous-algèbre</h4>
<p>Soit $E$ une $\mathbb{K}$-algèbre. Une partie $F$ de $E$ est une sous-algèbre de $E$ si et seulement si :</p>
<ol type="i">
<li>$1_E \in F$ ;</li>
<li>$\forall (x, y) \in F^2, x + y \in F$ et $xy \in F$ ;</li>
<li>$\forall \lambda \in \mathbb{K}, \forall x \in F, \lambda \cdot x \in F$.</li>
</ol>
<h3>Morphisme d'algèbres</h3>
<ul>
<li>Soit $(\mathbb{K}, +, \times)$ un corps et $E$ et $E'$ deux $\mathbb{K}$-algèbres (dont les lois sont notées de façon identique).<br>
Une application $f$ de $E$ dans $E'$ est appelée un morphisme d'algèbres si :
<ol type="i">
<li>$f(1_E) = 1_{E'}$ ;</li>
<li>$\forall (x, y) \in E^2, f(x + y) = f(x) + f(y)$ et $f(xy) = f(x)f(y)$ ;</li>
<li>$\forall \lambda \in \mathbb{K}, \forall x \in E, f(\lambda \cdot x) = \lambda \cdot f(x)$.</li>
</ol>
$f$ est donc à la fois un morphisme d'espaces vectoriels (c'est-à-dire une application linéaire), et un morphisme d'anneaux.</li>
</ul>
<p>Soit $f : A \to A'$ un morphisme de $\mathbb{K}$-algèbres. Alors :</p>
<ul>
<li>si $B$ est une sous-algèbre de $A$, $f(B)$ est une sous-algèbre de $A'$ ;</li>
<li>si $B'$ est une sous-algèbre de $A'$, son image réciproque $f^{-1}(B')$ est une sous-algèbre de $A$.</li>
</ul>

<!-- Image omitted: image0010031 -->
<!-- Page 27 -->
<h1>Thème 2 - Espaces vectoriels et applications linéaires</h1>
<p>Dans tout le chapitre, $\mathbb{K}$ désigne un sous-corps de $\mathbb{C}$, le plus souvent $\mathbb{R}$Ou $\mathbb{C}$.</p>
<h2>[S2.1] Définition d'un espace vectoriel</h2>
<p>Un $\mathbb{K}$-espace vectoriel est un triplet $(E, +, .)$, où $E$ est un ensemble muni :</p>
<ul>
<li>d'une loi de composition interne sur $E$, notée $+$, telle que $(E, +)$ soit un groupe abélien ;</li>
<li>d'une loi de composition externe sur $\mathbb{K}$, notée . , c'est-à-dire une application
$$
\begin{cases}
\mathbb{K} \times E \longrightarrow E \\
(\lambda, x) \longmapsto \lambda . x
\end{cases}
$$
vérifiant les propriétés suivantes :
$$
\forall (\lambda, \mu) \in \mathbb{K}^2, \forall (x, y) \in E^2,
\begin{cases}
\text{(i)} & 1_{\mathbb{K}} . x = x ; \\
\text{(ii)} & \lambda . (x + y) = \lambda . x + \lambda . y ; \\
\text{(iii)} & (\lambda + \mu) . x = \lambda . x + \mu . x ; \\
\text{(iv)} & \lambda . (\mu . x) = (\lambda \mu) . x .
\end{cases}
$$</li>
</ul>
<h3>Exemples</h3>
<p>Il est important de connaître un certain nombre d'espaces vectoriels classiques.</p>
<ul>
<li>Si $D$ est un ensemble quelconque et $E$ un $\mathbb{K}$-espace vectoriel, l'ensemble $\mathcal{A}(D, E)$ des applications de $D$ dans $E$ peut être muni d'une structure de $\mathbb{K}$-espace vectoriel pour les lois $+$ et . définies par :
$$
(f, g) \mapsto f + g \text{ avec : } \forall x \in D, (f + g)(x) = f(x) + g(x) ;
$$
$$
(\lambda, f) \mapsto \lambda . f \text{ avec : } \forall x \in D, (\lambda . f)(x) = \lambda . f(x).
$$
En particulier, l'ensemble $E^{\mathbb{N}}$ des suites à valeurs dans $E$ est un $\mathbb{K}$-espace vectoriel.</li>
<li>$\mathbb{K}[X]$, ensemble des polynômes à coefficients dans $\mathbb{K}$, est un $\mathbb{K}$-espace vectoriel.</li>
<li>$\mathcal{M}_{p, q}(\mathbb{K})$, ensemble des matrices à $p$ lignes et $q$ colonnes à coefficients dans $\mathbb{K}$, est un $\mathbb{K}$-espace vectoriel.</li>
</ul>
<h2>[S2.2] Espace vectoriel produit</h2>
<p>Soient $E_1, E_2, \ldots, E_p$ $p$ $\mathbb{K}$-espaces vectoriels.<br>
L'ensemble produit $E = E_1 \times E_2 \times \ldots \times E_p$ est un $\mathbb{K}$-espace vectoriel pour les lois définies par :
$$
(x_1, x_2, \ldots, x_p) + (y_1, y_2, \ldots, y_p) = (x_1 + y_1, x_2 + y_2, \ldots, x_p + y_p) ;
$$
$$
\lambda . (x_1, x_2, \ldots, x_p) = (\lambda x_1, \lambda x_2, \ldots, \lambda x_p).
$$
Muni de ces lois, $E$ s'appelle l'espace vectoriel produit des $E_i$. Le vecteur nul de $E$ est le $p$-uplet $(0_{E_1}, \ldots, 0_{E_p})$.<br>
Exemple : $\mathbb{K}^p$ est un $\mathbb{K}$-espace vectoriel pour les lois définies ci-dessus.</p>

<!-- Image omitted: image0010032 -->
<!-- Page 28 -->
<h3>Sous-espace vectoriel</h3>
<ul>
<li>Soit $(E, +, .)$ un $\mathbb{K}$-espace vectoriel. Par définition, une partie $F$ de $E$ est un sous-espace vectoriel de $E$ si $(F, +, .)$ est encore un $\mathbb{K}$-espace vectoriel.</li>
<li>Cela équivaut à dire que $F$ est une partie non vide de $E$ stable par combinaisons linéaires, soit :
$$F \subset E, \quad F \neq \emptyset \quad \text{et} \quad \forall \lambda \in \mathbb{K}, \forall (x, y) \in F^2 : x + y \in F \quad \text{et} \quad \lambda x \in F$$
ou encore :
$$F \subset E, \quad F \neq \emptyset \quad \text{et} \quad \forall \lambda \in \mathbb{K}, \forall (x, y) \in F^2 : \lambda x + y \in F.$$</li>
</ul>
<h3>Exemples</h3>
<p>Il existe un certain nombre de sous-espaces vectoriels classiques (donc d'espaces vectoriels) à connaître.</p>
<ul>
<li>$\{0_E\}$ et $E$ sont des sous-espaces vectoriels de $E$ (dits triviaux).</li>
<li>Si $n \in \mathbb{N}$, $\mathbb{K}_n[X]$, ensemble des polynômes de $\mathbb{K}[X]$ de degré inférieur ou égal à $n$, est un sous-espace vectoriel de $\mathbb{K}[X]$.<br>
<em>Attention : l'ensemble des polynômes de degré exactement $n$ n'est PAS un sous-espace vectoriel de $\mathbb{K}[X]$; en effet, il ne contient même pas le polynôme nul!</em></li>
<li>Si $P \in \mathbb{K}[X]$, l'ensemble $\mathbb{K}[X] \cdot P$ des multiples de $P$ est un sous-espace vectoriel de $\mathbb{K}[X]$ (c'est l'idéal engendré par $P$).</li>
<li>Si $I$ est un intervalle de $\mathbb{R}$, l'ensemble $\mathscr{C}(I, \mathbb{R})$ des fonctions continues sur $I$ à valeurs réelles est un sous-espace vectoriel de $\mathscr{A}(I, \mathbb{R})$.</li>
</ul>
<h2>[S2.4] Intersection de sous-espaces vectoriels</h2>
<p>Si $(E_i)_{i \in I}$ est une famille quelconque de sous-espaces vectoriels de $E$, leur intersection $\bigcap_{i \in I} E_i$ est encore un sous-espace vectoriel de $E$.<br>
<em>La réunion de sous-espaces vectoriels de $E$ n'est pas, en général, un sous-espace vectoriel de $E$.</em><br>
Plus précisément, on peut démontrer que si $A$ et $B$ sont deux sous-espaces vectoriels d'un espace vectoriel $E$, $A \cup B$ est encore un sous-espace vectoriel de $E$ si et seulement si $A \subset B$Ou $B \subset A$.</p>
<h2>[S2.5] Sous-espace vectoriel engendré, combinaisons linéaires</h2>
<ul>
<li>Soit $X$ une partie quelconque d'un $\mathbb{K}$-espace vectoriel $E$. L'intersection de tous les sous-espaces vectoriels de $E$ qui contiennent $X$ est encore un sous-espace vectoriel de $E$; c'est en fait le plus petit sous-espace vectoriel de $E$ contenant $X$ (au sens de l'inclusion); on l'appelle sous-espace vectoriel engendré par $X$, et on le note $\text{Vect}(X)$.</li>
</ul>

<!-- Image omitted: image0010033 -->
<!-- Page 29 -->
<ul>
<li>Si $X = \emptyset$, $\text{Vect}(\emptyset) = \{0_E\}$. Sinon, $\text{Vect}(X)$ est exactement l'ensemble des combinaisons linéaires des éléments de $X$, c'est-à-dire les éléments de la forme :
$$\sum_{i=1}^{n} \lambda_i x_i \quad \text{avec} \quad n \in \mathbb{N}^*, \lambda_i \in \mathbb{K} \quad \text{et} \quad x_i \in X \quad \text{pour tout} \quad i.
$$</li>
<li>Par définition, une combinaison linéaire est toujours une somme <em>finie</em>.</li>
<li>Le résultat précédent montre, en particulier, que l'ensemble des combinaisons linéaires des vecteurs d'une partie de $E$ est un sous-espace vectoriel de $E$. Cela peut être utile pour abréger certaines démonstrations.</li>
</ul>
<h2>[S2.6] Famille génératrice</h2>
<p>Une famille $(x_i)_{i \in I}$ d'un $\mathbb{K}$-espace vectoriel $E$ est dite génératrice de $E$ si le sous-espace vectoriel qu'elle engendre est égal à l'espace $E$ tout entier :
$$E = \text{Vect}((x_i)_{i \in I}),
$$
c'est-à-dire si tout vecteur de $E$ est combinaison linéaire des $x_i$. Toute famille contenant une famille génératrice de $E$ est encore génératrice de $E$.</p>
<h2>[S2.7] Dépendance et indépendance linéaire</h2>
<ul>
<li>Une famille finie $(x_i)_{1 \leq i \leq n}$ d'un $\mathbb{K}$-espace vectoriel $E$ est dite libre s'il n'existe pas de relation de dépendance linéaire non triviale entre ces vecteurs, c'est-à-dire si, pour toute famille $(\lambda_i)_{1 \leq i \leq n}$ d'éléments de $\mathbb{K}$ :
$$\sum_{i=1}^{n} \lambda_i x_i = 0_E \implies \forall i \in 〚 1; n 〛, \lambda_i = 0_{\mathbb{K}}
$$
(on dit aussi que les $x_i$ sont linéairement indépendants).</li>
<li>Une famille quelconque $(x_i)_{i \in I}$ d'un $\mathbb{K}$-espace vectoriel $E$ est dite libre si toutes ses sous-familles finies sont libres.<br>
Exemple : dans $\mathbb{K}[X]$, toute famille de polynômes non nuls de degrés distincts est libre.</li>
<li>Une famille $(x_i)_{i \in I}$ d'un $\mathbb{K}$-espace vectoriel $E$ est dite liée si elle n'est pas libre, c'est-à-dire s'il existe une famille $(\lambda_i)_{i \in I}$ d'éléments de $\mathbb{K}$, à support fini (c'est-à-dire tous nuls sauf un nombre fini), non tous nuls, telle que $\sum_{i \in I} \lambda_i x_i = 0_E$ (on dit aussi que les $x_i$ sont linéairement dépendants).</li>
</ul>
<h3>Propriétés</h3>
<ul>
<li>Une famille réduite à un élément $\{x\}$ est libre si et seulement si $x \neq 0_E$.</li>
<li>Toute famille contenant $0_E$ est liée.</li>
<li>Toute sous-famille d'une famille libre est libre. Toute famille contenant une famille liée est liée.</li>
<li>Une famille $(x_i)_{i \in I}$ est liée si et seulement si il existe $j \in I$ tel que $x_j$ soit combinaison linéaire des $(x_i)_{i \in I, i \neq j}$.<br>
Une famille $(x_i)_{i \in I}$ est libre si aucun des $x_i$ n'est combinaison linéaire des autres.</li>
</ul>

<!-- Image omitted: image0010034 -->
<!-- Page 30 -->
<h2>[S2.8] Bases</h2>
<p>Une famille $\mathcal{B}$ d'un $\mathbb{K}$-espace vectoriel $E$ s'appelle une base de $E$ si elle est à la fois libre et génératrice de $E$.<br>
Cela équivaut à dire que tout vecteur $x$ de $E$ s'écrit de manière unique comme combinaison linéaire des éléments de $\mathcal{B}$. Les coefficients de cette combinaison linéaire sont les coordonnées de $x$ dans la base $\mathcal{B}$.</p>
<h2>[S2.9] Somme de sous espaces vectoriels</h2>
<p>Soit $(E_i)_{1 \leq i \leq n}$ une famille de $n$ sous-espaces vectoriels de $E$ ($n \in \mathbb{N}^*$).<br>
On appelle somme des $E_i$, et on note $\sum_{i=1}^{n} E_i$, l'ensemble des vecteurs de la forme $\sum_{i=1}^{n} x_i$, où $x_i \in E_i$ pour tout $i \in 〚 1 ; n 〛$.<br>
C'est aussi le sous-espace vectoriel de $E$ engendré par la réunion des $E_i$, c'est-à-dire le plus petit sous-espace vectoriel de $E$ qui contient tous les $E_i$ :
$$\sum_{i=1}^{n} E_i = \text{Vect} \left( \bigcup_{1 \leq i \leq n} E_i \right).
$$
On prendra soin de ne pas confondre la <em>réunion</em>, qui est une notion ensembliste, et la <em>somme</em> de sous-espaces vectoriels.<br>
Exemple : si $x_1, \ldots, x_n$ sont $n$ vecteurs de $E$, $\sum_{i=1}^{n} \mathbb{K} x_i = \text{Vect}(x_1, \ldots, x_n)$.</p>
<h2>[S2.10] Somme directe de sous-espaces vectoriels</h2>
<ul>
<li>Soit $(E_i)_{1 \leq i \leq n}$ une famille de $n$ sous-espaces vectoriels de $E$.<br>
On dit que la somme $\sum_{i=1}^{n} E_i$ est directe lorsque pour tout $x \in \sum_{i=1}^{n} E_i$ l'écriture $x = \sum_{i=1}^{n} x_i$ avec $x_i \in E_i$ pour tout $i$ est <em>unique</em>. On note alors : $\sum_{i=1}^{n} E_i = \bigoplus_{i=1}^{n} E_i$.<br>
Exemple : si $x_1, \ldots, x_n$ sont $n$ vecteurs <em>non nuls</em> de $E$, dire que la somme $\sum_{i=1}^{n} \mathbb{K} x_i$ est directe équivaut à dire que la famille $(x_1, \ldots, x_n)$ est libre.</li>
<li><strong>Caractérisations d'une somme directe</strong><br>
Pour que la somme $\sum_{i=1}^{n} E_i$ soit directe, il faut et il suffit que pour toute famille $(x_i)_{1 \leq i \leq n}$ avec $x_i \in E_i$ pour tout $i$On ait :
$$\sum_{i=1}^{n} x_i = 0 \implies \forall i \in 〚 1 ; n 〛, x_i = 0.
$$
Il s'agit de la caractérisation la plus simple à utiliser pour montrer que $n$ sous-espaces vectoriels sont en somme directe dès que $n \geq 3$.</li>
</ul>

<!-- Image omitted: image0010035 -->
<!-- Page 31 -->
<ul>
<li>Pour que la somme $\sum_{i=1}^{n} E_i$ soit directe, il faut et il suffit qu'il existe $j \in 〚 1 ; n 〛$ tel que $\sum_{1 \leq i \leq n} E_i$ soit directe et que les sous-espaces $\sum_{\substack{1 \leq i \leq n \\ i \neq j}} E_i$ et $E_j$ soient en somme directe (et dans ce cas, cette propriété est vraie pour tout $j \in 〚 1 ; n 〛$).<br>
Cette caractérisation est moins utilisée, mais elle peut être pratique dans des démonstrations par récurrence.</li>
<li><strong>Somme directe de deux sous-espaces</strong><br>
Dans le cas de deux sous-espaces vectoriels (et dans ce cas seulement), on a une caractérisation plus simple : pour que la somme de deux sous-espaces vectoriels $E_1$ et $E_2$ de $E$ soit directe, il faut et il suffit que $E_1 \cap E_2 = \{0_E\}$.</li>
<li><strong>Sous-espaces supplémentaires</strong><br>
Les sous-espaces vectoriels $E_i$ ($1 \leq i \leq n$) de $E$ sont dits supplémentaires si $E = \bigoplus_{i=1}^{n} E_i$. Cela équivaut à dire que tout vecteur $x \in E$ s'écrit de manière unique sous la forme $x = \sum_{i=1}^{n} x_i$ avec $x_i \in E_i$ pour tout $i$.<br>
Exemple : si $x_1, \ldots, x_n$ sont $n$ vecteurs non nuls de $E$, dire que $E = \bigoplus_{i=1}^{n} \mathbb{K}.x_i$ équivaut à dire que la famille $(x_1, \ldots, x_n)$ est une base de $E$.</li>
<li><strong>Base adaptée à une décomposition en somme directe</strong>
<ul>
<li>Soit $(E_i)_{1 \leq i \leq n}$ une famille de sous-espaces vectoriels de $E$ telle que $E = \bigoplus_{i=1}^{n} E_i$. Si, pour tout $i \in 〚 1 ; n 〛$, $\mathcal{B}_i$ est une base de $E_i$, alors les $\mathcal{B}_i$ sont deux à deux disjointes et $\mathcal{B} = \bigcup_{i=1}^{n} \mathcal{B}_i$ est une base de $E$.</li>
<li>Soit $\mathcal{B}$ une base de $E$, et $(\mathcal{B}_i)_{1 \leq i \leq n}$ une partition de $\mathcal{B}$. Si, pour tout $i \in 〚 1 ; n 〛$, $E_i = \text{Vect}(\mathcal{B}_i)$, alors $E = \bigoplus_{i=1}^{n} E_i$.</li>
</ul>
</li>
</ul>
<h2>[S2.11] Espaces vectoriels de dimension finie</h2>
<ul>
<li>On dit qu'un $\mathbb{K}$-espace vectoriel est de dimension finie s'il possède une famille génératrice finie.</li>
<li><strong>Théorème de la base incomplète</strong><br>
Soient $L$ une famille libre et $G$ une famille génératrice de $E$, espace vectoriel de dimension finie.<br>
Alors il existe une base $\mathcal{B}$ de $E$ telle que : $L \subset \mathcal{B} \subset L \cup G$ (autrement dit, on peut compléter $L$ à l'aide de vecteurs de $G$ pour obtenir une base).<br>
On en déduit que tout espace vectoriel de dimension finie $E$ possède (au moins) une base (dans le cas où $E = \{0\}$, on peut convenir qu'une base de $E$ est $\emptyset$).</li>
<li><strong>Dimension</strong><br>
Toutes les bases d'un espace vectoriel $E$ de dimension finie ont le même nombre d'éléments, appelé la dimension de $E$, notée $\text{dim}_{\mathbb{K}} E$Ou plus simplement $\text{dim} E$.</li>
</ul>

<!-- Image omitted: image0010036 -->
<!-- Page 32 -->
<h3>Caractérisation des bases</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n$.</p>
<ul>
<li>Toute famille libre de $E$ a au plus $n$ éléments; et si elle en a exactement $n$, c'est une base de $E$.</li>
<li>Toute famille génératrice de $E$ a au moins $n$ éléments; et si elle en a exactement $n$, c'est une base de $E$.</li>
</ul>
<p>Exemple : toute famille de polynômes de degrés échelonnés de $0$ à $n$ est une base de $\mathbb{K}_n[X]$.</p>
<h3>Dimension d'un espace vectoriel produit</h3>
<p>Soient $E_1, E_2, \ldots, E_p$ $p$ $\mathbb{K}$-espaces vectoriels de dimensions finies.<br>
Alors l'espace vectoriel produit $E = E_1 \times E_2 \times \ldots \times E_p$ est de dimension finie et $\dim E = \sum_{i=1}^p \dim(E_i)$.</p>
<h2>[S2.12] Dimension d'un sous-espace vectoriel</h2>
<ul>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie, et soit $F$ un sous-espace vectoriel de $E$. Alors :
<ul>
<li>$F$ est de dimension finie, et $\dim F \leq \dim E$;</li>
<li>de plus, si $\dim E = \dim F$, alors $F = E$.</li>
</ul>
</li>
</ul>
<h3>Rang d'une famille</h3>
<p>Soit $(x_i)_{i \in I}$ une famille d'éléments d'un $\mathbb{K}$-espace vectoriel $E$.<br>
Si le sous-espace vectoriel qu'elle engendre est de dimension finie, on définit le rang de la famille $(x_i)_{i \in I}$ par :
$$\text{rg}((x_i)_{i \in I}) = \dim(\text{Vect}((x_i)_{i \in I})).
$$</p>
<h3>Propriétés</h3>
<ul>
<li>Si $E$ est de dimension finie, alors : $\text{rg}((x_i)_{i \in I}) \leq \dim(E)$.</li>
<li>Si $(x_1, x_2, \ldots, x_p)$ est une famille finie, alors : $\text{rg}(x_1, x_2, \ldots, x_p) \leq p$.</li>
</ul>
<h2>[S2.13] Dimension de la somme de deux sous-espaces</h2>
<h3>Dimension d'un supplémentaire</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie.</p>
<ul>
<li>Tout sous-espace vectoriel de $E$ possède (au moins) un supplémentaire.</li>
<li>Si $E = F \oplus G$, $\dim F + \dim G = \dim E$.</li>
</ul>
<h3>Formule de Grassmann</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel, et soient $F$ et $G$ deux sous-espaces vectoriels de dimensions finies de $E$. Alors $F + G$ est de dimension finie et :
$$\dim(F + G) = \dim F + \dim G - \dim(F \cap G).
$$</p>

<!-- Image omitted: image0010037 -->
<!-- Page 33 -->
<h3>Caractérisation de deux sous-espaces vectoriels supplémentaires</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie, et soient $F$ et $G$ deux sous-espaces vectoriels de $E$.<br>
Pour que $F$ et $G$ soient supplémentaires, il faut et il suffit que :
$$F + G = E \quad \text{et} \quad \dim E = \dim F + \dim G
$$
ou bien que :
$$F \cap G = \{0_E\} \quad \text{et} \quad \dim E = \dim F + \dim G.
$$</p>
<h2>[S2.14] Dimension de la somme de $n$ sous-espaces</h2>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie, et $(E_i)_{1 \leq i \leq n}$ une famille de sous-espaces vectoriels de $E$. On a l'inégalité :
$$\dim \left( \sum_{i=1}^{n} E_i \right) \leq \sum_{i=1}^{n} \dim E_i,
$$
et il y a égalité si et seulement si la somme est directe.</p>
<h3>Caractérisation de sous-espaces vectoriels supplémentaires</h3>
<p>Pour que les $E_i$ soient supplémentaires, il faut et il suffit que :<br>
la somme $\sum_{i=1}^{n} E_i$ soit directe et que $\dim E = \sum_{i=1}^{n} \dim (E_i)$<br>
ou bien que :
$$E = \sum_{i=1}^{n} E_i \quad \text{et} \quad \dim E = \sum_{i=1}^{n} \dim (E_i).
$$</p>
<h2>[S2.15] Applications linéaires</h2>
<p>Soient $E$ et $F$ deux $\mathbb{K}$-espaces vectoriels. Une application $u : E \to F$ est dite linéaire si :
$$\forall \lambda \in \mathbb{K}, \forall (x, y) \in E^2, u(x + y) = u(x) + u(y) \quad \text{et} \quad u(\lambda x) = \lambda u(x).
$$
On peut remplacer cette définition par :
$$\forall \lambda \in \mathbb{K}, \forall (x, y) \in E^2, u(\lambda x + y) = \lambda u(x) + u(y).
$$
Si $u$ est linéaire, on aura plus généralement, pour toute famille de scalaires $(\lambda_i)_{1 \leq i \leq n}$ et toute famille $(x_i)_{1 \leq i \leq n}$ de vecteurs de $E$ :
$$u \left( \sum_{i=1}^{n} \lambda_i x_i \right) = \sum_{i=1}^{n} \lambda_i u(x_i).
$$</p>
<ul>
<li>Un isomorphisme de $E$ sur $F$ est une application linéaire bijective de $E$ sur $F$; un endomorphisme de $E$ est une application linéaire de $E$ dans lui-même; un automorphisme de $E$ est un endomorphisme de $E$ bijectif.</li>
</ul>

<!-- Image omitted: image0010038 -->
<!-- Page 34 -->
<ul>
<li>On note $\mathscr{L}(E, F)$ l'ensemble des applications linéaires de $E$ dans $F$, $\mathscr{L}(E)$ l'ensemble des endomorphismes de $E$ et $\text{GL}(E)$ celui des automorphismes de $E$.</li>
</ul>
<h3>Exemples</h3>
<p>De même qu'il faut connaître certains espaces vectoriels de référence, il faut aussi connaître certaines applications linéaires classiques. Citons-en quelques-unes.</p>
<ul>
<li>L'application nulle de $E$ dans $F$, qui à tout $x \in E$ associe $0_F$.</li>
<li>L'application identique de $E$ : $\forall x \in E, \text{Id}_E(x) = x$.</li>
<li>L'homothétie de rapport $\lambda \in \mathbb{K}$ : $h_{\lambda} : x \mapsto \lambda x \quad (h_{\lambda} = \lambda . \text{Id}_E)$.</li>
<li>L'application $P \mapsto P'$ de $\mathbb{K}[X]$ dans $\mathbb{K}[X]$.</li>
<li>L'application $f \mapsto f(a)$ de $\mathscr{A}(D, E)$ dans $E$, avec $a \in D$.</li>
<li>L'application $f \mapsto \int_a^b f(t) \, dt$ de $\mathscr{C}([a, b], \mathbb{R})$ dans $\mathbb{R}$.</li>
</ul>
<h2>[S2.16] Opérations sur les applications linéaires</h2>
<ul>
<li>Soient $E$ et $F$ deux $\mathbb{K}$-espaces vectoriels. Alors $\mathscr{L}(E, F)$ est un sous-espace vectoriel de $\mathscr{A}(E, F)$. Autrement dit, si $u$ et $v$ sont deux applications linéaires de $E$ dans $F$ et si $\lambda$ est un scalaire, l'application $\lambda u + v$ est linéaire.<br>
De plus, lorsque $E$ et $F$ sont de dimensions finies, on a :
$$\dim \mathscr{L}(E, F) = \dim E \times \dim F.
$$</li>
<li>Soient $E$, $F$ et $G$ trois $\mathbb{K}$-espaces vectoriels.<br>
Si $u \in \mathscr{L}(E, F)$ et $v \in \mathscr{L}(F, G)$, alors $v \circ u \in \mathscr{L}(E, G)$.</li>
<li>Si $u$ est un isomorphisme de $E$ sur $F$, l'application réciproque $u^{-1}$ est encore linéaire (ce sera donc un isomorphisme de $F$ sur $E$).</li>
<li>$(\mathscr{L}(E), +, \cdot, \circ)$ est une algèbre (non commutative, non intègre, dès que $\dim E \geq 2$).<br>
$(\text{GL}(E), \circ)$ est un groupe; c'est le groupe des éléments inversibles de l'anneau $(\mathscr{L}(E), +, \circ)$.</li>
</ul>
<h2>[S2.17] Image directe et image réciproque d'un sous-espace, noyau</h2>
<ul>
<li>Soient $E$ et $F$ deux $\mathbb{K}$-espaces vectoriels, et $u \in \mathscr{L}(E, F)$.
<ul>
<li>Si $E'$ est un sous-espace vectoriel de $E$, son image $u(E')$ par $u$ est un sous-espace vectoriel de $F$.</li>
<li>Si $F'$ est un sous-espace vectoriel de $F$, son image réciproque par $u$ :
$$u^{-1}(F') = \{x \in E \mid u(x) \in F'\},
$$
est un sous-espace vectoriel de $E$.</li>
</ul>
</li>
<li>Soit $u \in \mathscr{L}(E, F)$. L'ensemble image de $E$ par $u$ est un sous-espace vectoriel de $F$, appelé image de $u$, et noté $\text{Im } u$. Ainsi :
$$\text{Im } u = \{u(x) \mid x \in E\}.
$$</li>
</ul>

<!-- Image omitted: image0010039 -->
<!-- Page 35 -->
<ul>
<li>Soit $u \in \mathscr{L}(E, F)$. L'image réciproque de $\{0_F\}$ par $u$ est un sous-espace vectoriel de $E$, appelé noyau de $u$, et noté $\text{Ker } u$. Ainsi :
$$\text{Ker } u = u^{-1}(\{0_F\}) = \{x \in E \mid u(x) = 0_F\}.
$$</li>
<li><strong>Théorème</strong><br>
Soit $u \in \mathscr{L}(E, F)$. Alors :
$$u \text{ surjective } \Longleftrightarrow \text{Im } u = F \quad ; \quad u \text{ injective } \Longleftrightarrow \text{Ker } u = \{0_E\}.
$$</li>
<li><strong>Deux propriétés simples mais importantes</strong><br>
Soient $E$, $F$ et $G$ trois $\mathbb{K}$-espaces vectoriels, $u \in \mathscr{L}(E, F)$ et $v \in \mathscr{L}(F, G)$. Alors :
$$\text{Im}(v \circ u) \subset \text{Im } v \quad \text{et} \quad \text{Ker}(v \circ u) \supset \text{Ker } u.
$$
$$v \circ u = 0 \Longleftrightarrow \text{Im } u \subset \text{Ker } v.
$$</li>
<li><strong>Noyau d'une restriction</strong><br>
Si $u \in \mathscr{L}(E, F)$ et si $E_1$ est un sous-espace vectoriel de $E$, la restriction de $u$ à $E_1$ a pour noyau :
$$\text{Ker}(u|_{E_1}) = \text{Ker } u \cap E_1.
$$</li>
<li><strong>Théorème d'isomorphisme</strong><br>
Soit $u \in \mathscr{L}(E, F)$. La restriction de $u$ à tout supplémentaire de $\text{Ker } u$ dans $E$ est un isomorphisme de ce supplémentaire sur $\text{Im } u$.</li>
</ul>
<h2>[S2.18] Détermination d'une application linéaire</h2>
<ul>
<li>Soit $(E_i)_{1 \leq i \leq n}$ une famille de sous-espaces vectoriels d'un $\mathbb{K}$-espace vectoriel $E$, telle que $E = \bigoplus_{i=1}^{n} E_i$, et soit $F$ un $\mathbb{K}$-espace vectoriel.<br>
Pour tout $i \in 〚 1 ; n 〛$, soit $u_i$ une application linéaire de $E_i$ dans $F$. Alors il existe une et une seule application linéaire $u \in \mathscr{L}(E, F)$ telle que pour tout $i \in 〚 1 ; n 〛$, $u_i = u|_{E_i}$.<br>
(autrement dit, une application linéaire est entièrement déterminée par ses restrictions à des sous-espaces vectoriels supplémentaires).</li>
<li>Soient $E$ et $F$ deux $\mathbb{K}$-espaces vectoriels, $(e_i)_{i \in I}$ une base de $E$ et $(b_i)_{i \in I}$ une famille de vecteurs de $F$.<br>
Alors il existe une et une seule application linéaire $u \in \mathscr{L}(E, F)$ telle que $u(e_i) = b_i$ pour tout $i \in I$.<br>
(autrement dit, une application linéaire est entièrement déterminée par les images des vecteurs d'une base).</li>
<li>Soient $E$ et $F$ deux $\mathbb{K}$-espaces vectoriels et $u \in \mathscr{L}(E, F)$.<br>
$u$ est injective (respectivement surjective, respectivement bijective) si et seulement si l'image d'une base par $u$ est une famille libre dans $F$ (respectivement génératrice de $F$, respectivement est une base de $F$) et, lorsque cela est le cas, cela vaut pour toutes les bases.</li>
<li>Il est important de retenir que, si $u \in \mathscr{L}(E, F)$ et si $(e_i)_{i \in I}$ est une base de $E$, alors $\text{Im } u$ est le sous-espace vectoriel de $F$ engendré par les vecteurs $u(e_i)$.</li>
</ul>

<!-- Image omitted: image0010040 -->
<!-- Page 36 -->
<h2>[S2.19] Rang d'une application linéaire</h2>
<ul>
<li>Soient $E$ et $F$ deux $\mathbb{K}$-espaces vectoriels et $u \in \mathscr{L}(E, F)$. Si $\operatorname{Im} u$ est de dimension finie, sa dimension s'appelle le rang de $u$, noté $\operatorname{rg} u$.<br>
Si $(e_i)_{i \in I}$ est une base de $E$, le rang de $u$ est aussi celui de la famille $(u(e_i))_{i \in I}$.</li>
<li><strong>Théorème du rang</strong><br>
Soient $E$ et $F$ deux $\mathbb{K}$-espaces vectoriels, avec $E$ de dimension finie, et soit $u \in \mathscr{L}(E, F)$. Alors $\operatorname{Im} u$ est de dimension finie et :
$$\operatorname{dim}(\operatorname{Im} u) + \operatorname{dim}(\operatorname{Ker} u) = \operatorname{dim} E.
$$</li>
<li><strong>Conséquences</strong><br>
$u$ désigne ici une application linéaire de $E$ dans $F$.
<ul>
<li>Si $E$ est de dimension finie :
$$\operatorname{rg} u \leq \operatorname{dim} E \text{ et } \operatorname{rg} u = \operatorname{dim} E \Longleftrightarrow u \text{ injective}.
$$</li>
<li>Si $F$ est de dimension finie :
$$\operatorname{rg} u \leq \operatorname{dim} F \text{ et } \operatorname{rg} u = \operatorname{dim} F \Longleftrightarrow u \text{ surjective}.
$$</li>
<li>Si $E$ et $F$ sont de dimensions finies et si $\operatorname{dim} E = \operatorname{dim} F$, on a :
$$u \text{ injective } \Longleftrightarrow u \text{ surjective } \Longleftrightarrow u \text{ bijective}.
$$</li>
<li>Soient $E$ et $F$ de dimensions finies, avec $\operatorname{dim} E = \operatorname{dim} F$. Soient $u \in \mathscr{L}(E, F)$ et $v \in \mathscr{L}(F, E)$ tels que $v \circ u = \operatorname{Id}_E$ (ou $u \circ v = \operatorname{Id}_F$). Alors $u$ et $v$ sont des isomorphismes, réciproques l'un de l'autre.<br>
$\checkmark$ Attention : les deux dernières propriétés ci-dessus ne sont plus vraies lorsque $E$ et $F$ ne sont pas de même dimension, ou bien lorsque l'un d'eux n'est pas de dimension finie.</li>
</ul>
</li>
<li><strong>Rang de la composée</strong><br>
Soient $E$, $F$ et $G$ trois $\mathbb{K}$-espaces vectoriels, $E$ et $F$ étant de dimensions finies. Soient $u \in \mathscr{L}(E, F)$ et $v \in \mathscr{L}(F, G)$. Alors on a :
<ul>
<li>$\operatorname{rg}(v \circ u) \leq \operatorname{rg} v$ et, si $u$ est surjective, il y a égalité.</li>
<li>$\operatorname{rg}(v \circ u) \leq \operatorname{rg} u$ et, si $v$ est injective, il y a égalité.</li>
</ul>
En particulier, il y a invariance du rang par la composition avec un isomorphisme.</li>
</ul>
<h2>[S2.20] Formes linéaires, hyperplans</h2>
<ul>
<li><strong>Espace dual</strong>
<ul>
<li>On rappelle qu'une forme linéaire sur un $\mathbb{K}$-espace vectoriel $E$ est une application linéaire de $E$ dans $\mathbb{K}$.<br>
L'espace vectoriel $\mathscr{L}(E, \mathbb{K})$ des formes linéaires sur $E$ s'appelle l'espace vectoriel dual de $E$, et est noté $E^*$.</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010041 -->
<!-- Page 37 -->
<ul>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie $n \in \mathbb{N}^*$, et $\mathcal{B} = (e_1, \ldots, e_n)$ une base de $E$.<br>
Pour tout $i \in 〚 1 ; n 〛$, l'application $e_i^*$ qui à tout $x = \sum_{i=1}^n x_i e_i$ de $E$, associe $e_i^*(x) = x_i$, est une forme linéaire sur $E$, appelée $i^e$ forme linéaire coordonnée dans $\mathcal{B}$.<br>
$e_i^*$ est l'unique forme linéaire sur $E$ telle que :
$$\forall j \in 〚 1 ; n 〛, e_i^*(e_j) = \delta_{ij}.
$$
La famille $\mathcal{B}^* = (e_1^*, \ldots, e_n^*)$ est une base de $E^*$, appelée base duale de $\mathcal{B}$.</li>
</ul>
<h3>Hyperplan</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel. Un hyperplan $H$ de $E$ est un sous-espace vectoriel de $E$ qui admet pour supplémentaire une droite vectorielle $D$ : $E = D \oplus H$. Si $H$ est un hyperplan, alors, pour toute droite vectorielle $D'$ non incluse dans $H$, on a $E = D' \oplus H$.<br>
Lorsque $E$ est de dimension finie $n \geq 1$, un sous-espace vectoriel $H$ de $E$ est un hyperplan si et seulement si $\dim H = n - 1$.</p>
<h3>Lien avec les formes linéaires</h3>
<ul>
<li>Un sous-espace vectoriel $H$ de $E$ est un hyperplan si et seulement si il existe une forme linéaire $\varphi$ sur $E$, non nulle, telle que $H = \text{Ker} \varphi$.</li>
<li>Si $\varphi$ et $\psi$ sont deux formes linéaires non nulles sur $E$ telles que $\text{Ker} \varphi = \text{Ker} \psi$, alors il existe $\lambda \in \mathbb{K}$ tel que $\psi = \lambda \varphi$.<br>
Pour tout hyperplan $H$, il existe donc une forme linéaire non nulle $\varphi$ telle que $H = \{x \in E, \varphi(x) = 0\}$. La relation $\varphi(x) = 0$ s'appelle une équation de $H$.</li>
</ul>
<h3>Expression analytique</h3>
<p>Lorsque $E$ est de dimension finie $n$, muni d'une base $\mathcal{B} = (e_1, \ldots, e_n)$, toute forme linéaire $\varphi$ sur $E$ est de la forme :
$$\varphi = \sum_{i=1}^n a_i e_i^*, \text{ avec } a_i = \varphi(e_i) \in \mathbb{K}.
$$
Cela équivaut à dire que pour tout $x = \sum_{i=1}^n x_i e_i \in E$, $\varphi(x) = \sum_{i=1}^n a_i x_i$.<br>
Il en résulte que tout hyperplan possède une équation de la forme :
$$\sum_{i=1}^n a_i x_i = 0 \quad (a_i \text{ scalaires non tous nuls}).
$$</p>
<h3>Intersection d'hyperplans</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie $n \in \mathbb{N}^*$.<br>
Si $(\varphi_1, \ldots, \varphi_p)$ est une famille de $p$ formes linéaires linéairement indépendantes sur $E$ ($p \in \mathbb{N}^*$), le sous-espace vectoriel :
$$F = \bigcap_{i=1}^p \text{Ker} \varphi_i = \{x \in E \mid \forall i \in 〚 1 ; p 〛, \varphi_i(x) = 0\}
$$</p>

<!-- Image omitted: image0010042 -->
<!-- Page 38 -->
<p>est un sous-espace vectoriel de $E$ de dimension $n - p$. L'ensemble des $p$ équations $\varphi_i(x) = 0$ $(1 \leq i \leq p)$ s'appelle un système d'équations de $F$.</p>
<h2>[S2.21] Projections, projecteurs, symétries</h2>
<h3>Définitions géométriques</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel, et $F$, $G$ deux sous-espaces vectoriels supplémentaires de $E$ : $E = F \oplus G$.<br>
Tout vecteur $x \in E$ s'écrit donc de manière unique sous la forme : $x = x_F + x_G$ avec $(x_F, x_G) \in F \times G$.<br>
La projection sur $F$ parallèlement à $G$ est l'application $p : \begin{cases} E \longrightarrow E \\ x \longmapsto x_F \end{cases}$.<br>
La symétrie par rapport à $F$ et parallèlement à $G$ est l'application $s : \begin{cases} E \longrightarrow E \\ x \longmapsto x_F - x_G \end{cases}$.<br>
<!-- Image omitted: Diagramme de projection/symétrie -->
</p>
<h3>Propriétés</h3>
<ul>
<li>$s = 2p - \text{Id}_E$.</li>
<li>$s$ et $p$ sont deux endomorphismes de $E$.</li>
<li>$p^2 = p \circ p = p$.</li>
<li>$s^2 = s \circ s = \text{Id}_E$ ($s$ est donc bijectif, et $s^{-1} = s$).</li>
<li>$\text{Ker}(p - \text{Id}_E) = \text{Im} \, p = F$ et $\text{Ker} \, p = G$.</li>
<li>$\text{Ker}(s - \text{Id}_E) = F$ et $\text{Ker}(s + \text{Id}_E) = G$.</li>
<li>Si $p$ est la projection sur $F$ parallèlement à $G$, et $q$ est la projection sur $G$ parallèlement à $F$, $p + q = \text{Id}_E$ ($p$ et $q$ sont dits associés).</li>
</ul>
<h3>Projecteurs</h3>
<p>On appelle projecteur de $E$ tout endomorphisme $p$ de $E$ tel que $p^2 = p$.</p>

<!-- Image omitted: image0010043 -->
<!-- Page 39 -->
<p>Si $p$ est un projecteur, alors :</p>
$$E = \text{Im} p \oplus \text{Ker} p \quad ; \quad \text{Im} p = \text{Ker}(p - \text{Id}_E)$$
<p>et $p$ est la projection sur $\text{Im} p$ parallèlement à $\text{Ker} p$.<br>
$\checkmark$ Pour un projecteur $p$, il est vraiment important de retenir la propriété $\text{Im} p = \text{Ker}(p - \text{Id}_E)$, qui exprime que les vecteurs de l'image de $p$ sont exactement les vecteurs invariants par $p$.<br>
$\checkmark$ Il est également utile de retenir que la décomposition de tout vecteur $x \in E$ selon les sous-espaces vectoriels supplémentaires $\text{Im} p$ et $\text{Ker} p$ est :
$$x = \underbrace{p(x)}_{\in \text{Im} p} + \underbrace{x - p(x)}_{\in \text{Ker} p}.$$</p>
<h3>Endomorphismes involutifs</h3>
<p>Si $s$ est un endomorphisme involutif de $E$ ($s^2 = \text{Id}_E$), alors :</p>
$$E = \text{Ker}(s - \text{Id}_E) \oplus \text{Ker}(s + \text{Id}_E),$$
<p>et $s$ est la symétrie par rapport à $\text{Ker}(s - \text{Id}_E)$ parallèlement à $\text{Ker}(s + \text{Id}_E)$.<br>
$\checkmark$ Il peut être utile de retenir la décomposition suivante :
$$\forall x \in E, x = \underbrace{\frac{x + s(x)}{2}}_{\in \text{Ker}(s - \text{Id}_E)} + \underbrace{\frac{x - s(x)}{2}}_{\in \text{Ker}(s + \text{Id}_E)}.$$</p>
<h3>Projecteurs associés à une décomposition en somme directe</h3>
<p>Soit $(E_i)_{1 \leq i \leq n}$ une famille de sous-espaces vectoriels de $E$ telle que $E = \bigoplus_{i=1}^n E_i$.<br>
Pour tout $x \in E$, il existe donc une unique famille $(x_i)_{1 \leq i \leq n} \in \prod_{i=1}^n E_i$ telle que :
$$x = \sum_{i=1}^n x_i.$$
Considérons, pour tout $i \in 〚 1; n 〛$, l'application $p_i : \begin{cases} E \rightarrow E \\ x \mapsto x_i \end{cases}$.<br>
On a alors :</p>
<ul>
<li>$\forall i \in 〚 1; n 〛, p_i \circ p_i = p_i$.</li>
<li>$\forall i \in 〚 1; n 〛, p_i$ est la projection sur $E_i$ parallèlement à $\bigoplus_{\substack{j=1 \\ j \neq i}}^n E_j$.</li>
<li>$\sum_{i=1}^n p_i = \text{Id}_E$.</li>
<li>$\forall (i, j) \in 〚 1; n 〛^2, i \neq j \implies p_i \circ p_j = 0_{\mathcal{L}(E)}$.</li>
</ul>
<p>On dit que $(p_i)_{1 \leq i \leq n}$ est la famille de projecteurs canoniquement associée à la décomposition de $E$ en somme directe $E = \bigoplus_{i=1}^n E_i$.</p>

<!-- Image omitted: image0010044 -->
<!-- Page 40 -->
<h1>Thème 3 - Calcul matriciel</h1>
<p>Dans toute la suite, $\mathbb{K}$ désigne un sous-corps de $\mathbb{C}$ (le plus souvent $\mathbb{R}$Ou $\mathbb{C}$), et $p, q$ et $n$ désignent des entiers naturels non nuls. On note :</p>
<ul>
<li>$\mathcal{M}_{p,q}(\mathbb{K})$ l'ensemble des matrices de type $(p,q)$ à coefficients dans $\mathbb{K}$.</li>
<li>$\mathcal{M}_n(\mathbb{K})$ l'ensemble des matrices carrées d'ordre $n$.</li>
</ul>
<h2>[S3.1] Opérations sur les matrices</h2>
<h3>Structure de $\mathbb{K}$-espace vectoriel</h3>
<p>Soient $A = (a_{ij})_{\substack{1 \leq i \leq p \\ 1 \leq j \leq q}} \in \mathcal{M}_{p,q}(\mathbb{K})$ et $B = (b_{ij})_{\substack{1 \leq i \leq p \\ 1 \leq j \leq q}} \in \mathcal{M}_{p,q}(\mathbb{K})$ deux matrices de même type $(p,q)$, et soit $\lambda \in \mathbb{K}$ un scalaire.<br>
La somme des matrices $A$ et $B$ est définie par :
$$A + B = (a_{ij} + b_{ij})_{\substack{1 \leq i \leq p \\ 1 \leq j \leq q}}
$$
et la multiplication externe de $A$ par $\lambda$ est définie par :
$$\lambda A = (\lambda a_{ij})_{\substack{1 \leq i \leq p \\ 1 \leq j \leq q}}
$$
Muni de ces lois, $(\mathcal{M}_{p,q}(\mathbb{K}), +, \cdot)$ est un $\mathbb{K}$-espace vectoriel.<br>
L'élément neutre pour l'addition dans $\mathcal{M}_{p,q}(\mathbb{K})$ est la matrice nulle (dont tous les termes sont nuls), que l'on note $0_{p,q}$, ou $0$ s'il n'y a pas d'ambiguïté.</p>
<h3>Produit de matrices</h3>
<p>Soient $A = (a_{ij})_{\substack{1 \leq i \leq n \\ 1 \leq j \leq p}} \in \mathcal{M}_{n,p}(\mathbb{K})$ et $B = (b_{jk})_{\substack{1 \leq j \leq p \\ 1 \leq k \leq q}} \in \mathcal{M}_{p,q}(\mathbb{K})$ deux matrices.<br>
On définit le produit matriciel $AB = (c_{ik})_{\substack{1 \leq i \leq n \\ 1 \leq k \leq q}} \in \mathcal{M}_{n,q}(\mathbb{K})$ par :</p>
$$\forall (i,k) \in 〚 1;n 〛 \times 〚 1;q 〛, c_{ik} = \sum_{j=1}^{p} a_{ij} b_{jk}
$$
<p>Pour tout $k \in 〚 1;q 〛$, la $k^e$ colonne de $AB$ est égale à $AC_k$, où $C_k$ désigne la $k^e$ colonne de $B$. De même, pour tout $i \in 〚 1;n 〛$, la $i^e$ ligne de $AB$ est égale à $L_i B$, où $L_i$ désigne la $i^e$ ligne de $A$.<br>
Le produit matriciel $AB$ n'est défini que si le nombre de colonnes de $A$ est égal au nombre de lignes de $B$.</p>
<h3>Propriétés</h3>
<ul>
<li>$\forall A \in \mathcal{M}_{p,q}(\mathbb{K}), \forall B \in \mathcal{M}_{q,r}(\mathbb{K}), \forall C \in \mathcal{M}_{r,s}(\mathbb{K}), A(BC) = (AB)C$.</li>
<li>$\forall A \in \mathcal{M}_{p,q}(\mathbb{K}), \forall B_1, B_2 \in \mathcal{M}_{q,r}(\mathbb{K}), A(B_1 + B_2) = AB_1 + AB_2$.</li>
<li>$\forall A_1, A_2 \in \mathcal{M}_{p,q}(\mathbb{K}), \forall B \in \mathcal{M}_{q,r}(\mathbb{K}), (A_1 + A_2)B = A_1 B + A_2 B$.</li>
<li>$\forall A \in \mathcal{M}_{p,q}(\mathbb{K}), \forall B \in \mathcal{M}_{q,r}(\mathbb{K}), \forall \lambda \in \mathbb{K}, (\lambda A)B = A(\lambda B) = \lambda (AB)$.</li>
</ul>

<!-- Image omitted: image0010045 -->
<!-- Page 41 -->
<p>On peut résumer ces propriétés en disant que la multiplication des matrices est une opération associative et bilinéaire.</p>
<ul>
<li><strong>Base canonique de $M_{p,q}(\mathbb{K})$</strong>
<ul>
<li>Pour tout $(k,\ell) \in 〚 1 ; p 〛 \times 〚 1 ; q 〛$, on note $E_{k\ell}$ la matrice de type $(p,q)$ dont tous les termes sont nuls, sauf celui d'indice $(k,\ell)$ qui vaut 1. Autrement dit,
$$E_{k\ell} = (\delta_{ki}\delta_{\ell j})_{\substack{1 \leq i \leq p \\ 1 \leq j \leq q}} \in M_{p,q}(\mathbb{K}).
$$</li>
<li>La famille des matrices $(E_{k\ell})_{(k,\ell) \in 〚 1 ; p 〛 \times 〚 1 ; q 〛}$ forme une base de $M_{p,q}(\mathbb{K})$, appelée base canonique.</li>
<li>On en déduit que : $\dim(M_{p,q}(\mathbb{K})) = pq$.</li>
<li>Pour toute matrice $A = (a_{ij})_{\substack{1 \leq i \leq p \\ 1 \leq j \leq q}} \in M_{p,q}(\mathbb{K})$, $A = \sum_{\substack{1 \leq i \leq p \\ 1 \leq j \leq q}} a_{ij} E_{ij}$, c'est-à-dire que les coordonnées de $A$ dans la base canonique sont les $a_{ij}$.</li>
<li>Soient $(E_{ij})_{\substack{1 \leq i \leq p \\ 1 \leq j \leq q}}$, $(E'_{jk})_{\substack{1 \leq j \leq q \\ 1 \leq k \leq r}}$ et $(E''_{k\ell})_{\substack{1 \leq k \leq r \\ 1 \leq \ell \leq r}}$ les bases canoniques respectives de $M_{p,q}(\mathbb{K})$, de $M_{q,r}(\mathbb{K})$ et de $M_{p,r}(\mathbb{K})$. Alors :
$$\forall (i,j) \in 〚 1 ; p 〛 \times 〚 1 ; q 〛, \forall (k,\ell) \in 〚 1 ; q 〛 \times 〚 1 ; r 〛, E_{ij}E'_{k\ell} = \delta_{jk}E''_{i\ell}.
$$</li>
</ul>
</li>
</ul>
<h2>[S3.2] Opérations par blocs</h2>
<ul>
<li>Soit $A \in M_{p,q}(\mathbb{K})$. On appelle bloc de $A$ toute matrice $(a_{ij})_{\substack{i \in I \\ j \in J}}$, où $I$ et $J$ sont respectivement des parties de $〚 1 ; p 〛$ et de $〚 1 ; q 〛$ formées d'entiers consécutifs (lorsque les indices ne sont pas consécutifs on parle de matrice extraite).</li>
<li><strong>Combinaison linéaire par blocs</strong><br>
Soient $A, B \in M_{p,q}(\mathbb{K})$ décomposées en blocs avec le même découpage :
<!-- Note: Visual representation of block matrices is difficult in text -->
$$A = \begin{bmatrix}
A_{11} & \cdots & A_{1m} \\
\vdots & \ddots & \vdots \\
A_{\ell 1} & \cdots & A_{\ell m}
\end{bmatrix} \quad \text{et} \quad B = \begin{bmatrix}
B_{11} & \cdots & B_{1m} \\
\vdots & \ddots & \vdots \\
B_{\ell 1} & \cdots & B_{\ell m}
\end{bmatrix}$$
(où $p_1, \ldots, p_\ell$ sont les hauteurs des blocs et $q_1, \ldots, q_m$ les largeurs).
Alors, pour tout $\lambda \in \mathbb{K}$, la matrice $\lambda A + B$ s'écrit, par blocs :
$$\lambda A + B = \begin{bmatrix}
\lambda A_{11} + B_{11} & \cdots & \lambda A_{1m} + B_{1m} \\
\vdots & \ddots & \vdots \\
\lambda A_{\ell 1} + B_{\ell 1} & \cdots & \lambda A_{\ell m} + B_{\ell m}
\end{bmatrix}$$</li>
</ul>

<!-- Image omitted: image0010046 -->
<!-- Page 42 -->
<h3>Produit par blocs</h3>
<p>Soient $A \in M_{p,q}(\mathbb{K})$ et $B \in M_{q,r}(\mathbb{K})$, décomposées en blocs comme suit :
$$A = \begin{bmatrix}
A_{11} & \cdots & A_{1m} \\
\vdots & & \vdots \\
A_{\ell 1} & \cdots & A_{\ell m}
\end{bmatrix} \quad (\text{hauteurs } p_1, \ldots, p_\ell)
$$
$$B = \begin{bmatrix}
B_{11} & \cdots & B_{1n} \\
\vdots & & \vdots \\
B_{m1} & \cdots & B_{mn}
\end{bmatrix} \quad (\text{hauteurs } q_1, \ldots, q_m)
$$
et soit $C = AB \in M_{p,r}(\mathbb{K})$, décomposée en blocs :
$$C = \begin{bmatrix}
C_{11} & \cdots & C_{1n} \\
\vdots & & \vdots \\
C_{\ell 1} & \cdots & C_{\ell n}
\end{bmatrix} \quad (\text{hauteurs } p_1, \ldots, p_\ell)
$$
Alors : $\forall (i,k) \in 〚 1 ; \ell 〛 \times 〚 1 ; n 〛, C_{ik} = \sum_{j=1}^{m} A_{ij} B_{jk}$.</p>
<h2>[S3.3] Transposition</h2>
<p>Soit $A = (a_{ij})_{\substack{1 \leqslant i \leqslant p \\ 1 \leqslant j \leqslant q}} \in M_{p,q}(\mathbb{K})$.<br>
On appelle transposée de $A$ la matrice $^tA = (a'_{ji})_{\substack{1 \leqslant j \leqslant q \\ 1 \leqslant i \leqslant p}} \in M_{q,p}(\mathbb{K})$ définie par :
$$\forall (i,j) \in 〚 1 ; p 〛 \times 〚 1 ; q 〛, a'_{ji} = a_{ij}.
$$
La transposée de $A$ peut aussi être notée $A^T$.</p>
<h3>Propriétés</h3>
<ul>
<li>$\forall A \in M_{p,q}(\mathbb{K}), ^t(^tA) = A$.</li>
<li>$\forall A, B \in M_{p,q}(\mathbb{K}), ^t(A + B) = ^tA + ^tB$.</li>
<li>$\forall A \in M_{p,q}(\mathbb{K}), \forall \lambda \in \mathbb{K}, ^t(\lambda A) = \lambda ^tA$.</li>
<li>$\forall A \in M_{p,q}(\mathbb{K}), \forall B \in M_{q,r}(\mathbb{K}), ^t(AB) = ^tB ^tA$.</li>
<li>L'application $$\begin{cases}
M_{p,q}(\mathbb{K}) \longrightarrow M_{q,p}(\mathbb{K}) \\
A \longmapsto ^tA
\end{cases}$$ est un isomorphisme d'espaces vectoriels.</li>
</ul>
<h3>Transposition par blocs</h3>
<p>Soit $A \in M_{p,q}(\mathbb{K})$. Si
$$A = \begin{bmatrix}
A_{11} & \cdots & A_{1m} \\
\vdots & & \vdots \\
A_{\ell 1} & \cdots & A_{\ell m}
\end{bmatrix} \quad (\text{hauteurs } p_1, \ldots, p_\ell)
$$
alors
$$^tA = \begin{bmatrix}
^tA_{11} & \cdots & ^tA_{\ell 1} \\
\vdots & & \vdots \\
^tA_{1m} & \cdots & ^tA_{\ell m}
\end{bmatrix} \quad (\text{hauteurs } q_1, \ldots, q_m)
$$</p>

<!-- Image omitted: image0010047 -->
<!-- Page 43 -->
<h2>[S3.4] L'algèbre des matrices carrées</h2>
<ul>
<li>On sait que $(M_n(\mathbb{K}), +, \cdot)$ est un $\mathbb{K}$-espace vectoriel de dimension $n^2$. De plus, dans $M_n(\mathbb{K})$, la multiplication matricielle $\times$ est une loi de composition interne, et $(M_n(\mathbb{K}), +, \cdot, \times)$ est une $\mathbb{K}$-algèbre.<br>
L'élément neutre pour la loi $\times$ est la matrice identité $I_n$, dont les termes diagonaux valent 1, et dont les termes non diagonaux sont nuls.<br>
L'algèbre $M_n(\mathbb{K})$ n'est pas commutative et n'est pas intègre dès que $n \geq 2$.</li>
</ul>
<h3>Matrices inversibles</h3>
<p>On note $GL_n(\mathbb{K})$ l'ensemble des matrices inversibles d'ordre $n$. Il s'agit d'un groupe pour la multiplication matricielle, appelé groupe linéaire d'ordre $n$. C'est le groupe des éléments inversibles de l'anneau $(M_n(\mathbb{K}), +, \times)$.<br>
Si $A$ est inversible, il en est de même de $^tA$, et $(^tA)^{-1} = ^t(A^{-1})$.</p>
<ul>
<li>Soit $A \in M_n(\mathbb{K})$. Alors les propriétés suivantes sont équivalentes :
<ol type="i">
<li>$A$ est inversible,</li>
<li>$A$ est inversible à droite $(\exists B \in M_n(\mathbb{K}) \mid AB = I_n)$,</li>
<li>$A$ est inversible à gauche $(\exists B \in M_n(\mathbb{K}) \mid BA = I_n)$,</li>
<li>$A$ est régulière à droite $(\forall M, N \in M_n(\mathbb{K}), MA = NA \implies M = N)$,</li>
<li>$A$ est régulière à gauche $(\forall M, N \in M_n(\mathbb{K}), AM = AN \implies M = N)$.</li>
</ol>
</li>
</ul>
<h2>[S3.5] Matrices carrées remarquables</h2>
<h3>Matrices triangulaires</h3>
<ul>
<li>Soit $A = (a_{ij}) \in M_n(\mathbb{K})$. $A$ est dite triangulaire supérieure (respectivement inférieure) si pour tout $(i, j) \in 〚 1 ; n 〛^2$ :
$i > j \implies a_{ij} = 0$ (respectivement $i < j \implies a_{ij} = 0$).<br>
On note respectivement $T_n^+(\mathbb{K})$ et $T_n^-(\mathbb{K})$ l'ensemble des matrices triangulaires supérieures et l'ensemble des matrices triangulaires inférieures d'ordre $n$.</li>
</ul>
<h3>Propriétés</h3>
<ul>
<li>Soient $A = (a_{ij}) \in T_n^+(\mathbb{K})$ et $B = (b_{ij}) \in T_n^+(\mathbb{K})$ deux matrices triangulaires supérieures. Alors $C = AB$ est triangulaire supérieure et ses coefficients diagonaux sont les scalaires $c_{ii} = a_{ii}b_{ii}$.</li>
<li>Soit $A = (a_{ij}) \in T_n^+(\mathbb{K})$. Alors :
$A$ inversible $\iff \forall i \in 〚 1 ; n 〛, a_{ii} \neq 0$.
De plus, dans ce cas, $A^{-1}$ est triangulaire supérieure et ses termes diagonaux sont les scalaires $\frac{1}{a_{ii}}$.</li>
<li>$T_n^+(\mathbb{K})$ est une sous-algèbre de $M_n(\mathbb{K})$, de dimension $\frac{n(n+1)}{2}$.</li>
</ul>

<!-- Image omitted: image0010048 -->
<!-- Page 44 -->
<ul>
<li>L'application $$\begin{cases}
\mathcal{T}_n^+(\mathbb{K}) & \longrightarrow \mathcal{T}_n^-(\mathbb{K}) \\
A & \longmapsto ^tA
\end{cases}$$ est un anti-isomorphisme d'algèbres.<br>
Il en résulte que les propriétés précédentes (produit, inverse et dimension) sont encore vraies pour les matrices triangulaires inférieures.</li>
</ul>
<h3>Matrices diagonales</h3>
<ul>
<li>Une matrice carrée $A = (a_{ij}) \in \mathcal{M}_n(\mathbb{K})$ est dite diagonale si :
$\forall (i,j) \in 〚 1 ; n 〛^2, i \neq j \Longrightarrow a_{ij} = 0$.<br>
On note $\mathcal{D}_n(\mathbb{K}) = \mathcal{T}_n^+(\mathbb{K}) \cap \mathcal{T}_n^-(\mathbb{K})$ l'ensemble des matrices diagonales d'ordre $n$. $\mathcal{D}_n(\mathbb{K})$ est une sous-algèbre commutative de $\mathcal{M}_n(\mathbb{K})$ et $\dim \mathcal{D}_n(\mathbb{K}) = n$.<br>
Pour tout $(\lambda_1, \ldots, \lambda_n) \in \mathbb{K}^n$, on note $\text{diag}(\lambda_1, \ldots, \lambda_n)$ la matrice diagonale $A = (a_{ij}) \in \mathcal{M}_n(\mathbb{K})$ telle que : $\forall i \in 〚 1 ; n 〛, a_{ii} = \lambda_i$.</li>
</ul>
<h3>Propriétés</h3>
<ul>
<li>Soient $A = \text{diag}(a_{11}, \ldots, a_{nn}) \in \mathcal{D}_n(\mathbb{K})$ et $B = \text{diag}(b_{11}, \ldots, b_{nn}) \in \mathcal{D}_n(\mathbb{K})$ deux matrices diagonales. Alors $AB = \text{diag}(a_{11} b_{11}, \ldots, a_{nn} b

  ```html
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MATHS MP/MP* - Savoir & Faire en Prépas (Suite)</title>
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css">
    <!-- KaTeX JS -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"></script>
    <!-- KaTeX auto-render extension -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 2em;
            max-width: 800px; /* Limit width for readability */
            margin-left: auto;
            margin-right: auto;
        }
        h1, h2, h3, h4, h5, h6 {
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }
        h1 { border-bottom: 2px solid #ccc; padding-bottom: 0.3em; }
        h2 { border-bottom: 1px solid #eee; padding-bottom: 0.2em; }
        p { margin-bottom: 1em; }
        ul, ol { margin-left: 2em; margin-bottom: 1em; }
        li { margin-bottom: 0.5em; }
        blockquote {
            margin-left: 2em;
            padding-left: 1em;
            border-left: 3px solid #eee;
            font-style: italic;
            color: #555;
        }
        code {
            background-color: #f4f4f4;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
        }
        pre code {
            display: block;
            padding: 1em;
            overflow-x: auto;
        }
       .katex-display { /* Style for KaTeX display math */
            display: block;
            margin-top: 1em;
            margin-bottom: 1em;
            overflow-x: auto; /* Handle long formulas */
            overflow-y: hidden;
        }
        .center { text-align: center; }
        .small { font-size: 0.8em; }
        .isbn { font-family: monospace; }
        table {
            border-collapse: collapse;
            margin-bottom: 1em;
            width: 100%;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 0.5em;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>

<!-- Continuation from page 44 -->
<ul>
<li>Soit $A = \text{diag}(a_{11}, \ldots, a_{nn}) \in \mathcal{D}_n(\mathbb{K})$. Alors :
$A$ inversible $\Longleftrightarrow \forall i \in 〚 1 ; n 〛, a_{ii} \neq 0$.<br>
De plus, dans ce cas, $A^{-1} = \text{diag} \left( \frac{1}{a_{11}}, \ldots, \frac{1}{a_{nn}} \right)$.</li>
<li>Soit $D \in \mathcal{D}_n(\mathbb{K})$ une matrice diagonale dont les termes diagonaux sont deux à deux distincts. Soit $A \in \mathcal{M}_n(\mathbb{K})$. Alors :
$AD = DA \Longleftrightarrow A$ diagonale.<br>
$\checkmark$ Autrement dit, si une matrice commute avec une matrice diagonale à éléments diagonaux distincts, elle est diagonale. Ce résultat, très utile, ne figure pas au programme officiel et est démontré en [SF3.1].</li>
</ul>
<h3>Matrices scalaires</h3>
<ul>
<li>Une matrice carrée $A = (a_{ij}) \in \mathcal{M}_n(\mathbb{K})$ est dite scalaire s'il existe $\lambda \in \mathbb{K}$ tel que $A = \text{diag}(\lambda, \ldots, \lambda) = \lambda I_n$.</li>
<li>Soit $A \in \mathcal{M}_n(\mathbb{K})$. Alors :
$A$ scalaire $\Longleftrightarrow \forall M \in \mathcal{M}_n(\mathbb{K}), AM = MA$.<br>
$\checkmark$ Autrement dit, les matrices carrées qui commutent avec toutes les autres sont les matrices scalaires. Ce résultat classique ne figure pas au programme officiel et est démontré en [SF3.2].</li>
</ul>
<h3>Matrices symétriques, matrices antisymétriques</h3>
<p>Soit $A = (a_{ij}) \in \mathcal{M}_n(\mathbb{K})$ une matrice carrée.<br>
$A$ est dite symétrique si $^tA = A$, et antisymétrique si $^tA = -A$.<br>
$\checkmark$ Les éléments diagonaux d'une matrice antisymétrique sont nécessairement nuls (c'est une conséquence de la définition).</p>

<!-- Image omitted: image0010049 -->
<!-- Page 45 -->
<p>On note respectivement $\mathcal{S}_n(\mathbb{K})$ et $\mathcal{A}_n(\mathbb{K})$ l'ensemble des matrices symétriques et l'ensemble des matrices antisymétriques d'ordre $n$.<br>
Ce sont des sous-espaces vectoriels supplémentaires de $\mathcal{M}_n(\mathbb{K})$ :
$$\mathcal{M}_n(\mathbb{K}) = \mathcal{S}_n(\mathbb{K}) \oplus \mathcal{A}_n(\mathbb{K}).
$$
Plus précisément, la décomposition unique d'une matrice $A \in \mathcal{M}_n(\mathbb{K})$ est :
$$A = \underbrace{\frac{1}{2}(A + ^tA)}_{\in \mathcal{S}_n(\mathbb{K})} + \underbrace{\frac{1}{2}(A - ^tA)}_{\in \mathcal{A}_n(\mathbb{K})}.
$$
Enfin, on a : $\dim(\mathcal{S}_n(\mathbb{K})) = \frac{n(n+1)}{2}$ et $\dim(\mathcal{A}_n(\mathbb{K})) = \frac{n(n-1)}{2}$.</p>
<h2>[S3.6] Matrice d'une famille de vecteurs</h2>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n$, rapporté à une base $\mathcal{B} = (e_1, \ldots, e_n)$ et soient $x_1, \ldots, x_p$ des vecteurs de $E$.<br>
Pour tout $j \in [1; p]$, le vecteur $x_j$ s'écrit dans la base $\mathcal{B}$ sous la forme $x_j = \sum_{i=1}^n a_{ij} e_i$ avec $a_{ij} \in \mathbb{K}$.<br>
La matrice $A = (a_{ij})_{\substack{1 \leq i \leq n \\ 1 \leq j \leq p}} \in \mathcal{M}_{n,p}(\mathbb{K})$ s'appelle la matrice de la famille $(x_1, \ldots, x_p)$ dans la base $\mathcal{B}$.<br>
Il s'agit donc de la matrice obtenue en écrivant dans chaque colonne les coordonnées dans $\mathcal{B}$ des vecteurs de la famille.</p>
<h2>[S3.7] Matrice d'une application linéaire</h2>
<ul>
<li>Soient :
<ul>
<li>$E$ un $\mathbb{K}$-espace vectoriel de dimension $q$, rapporté à une base $\mathcal{B}_E = (e_1, \ldots, e_q)$;</li>
<li>$F$ un $\mathbb{K}$-espace vectoriel de dimension $p$, rapporté à une base $\mathcal{B}_F = (e'_1, \ldots, e'_p)$;</li>
<li>$u \in \mathcal{L}(E, F)$.</li>
</ul>
On appelle matrice de $u$ dans les bases $\mathcal{B}_E$ et $\mathcal{B}_F$ la matrice dans $\mathcal{B}_F$ du système de vecteurs $(u(e_1), \ldots, u(e_q))$. On la note généralement $M_{\mathcal{B}_E}^{\mathcal{B}_F}(u)$.<br>
Ainsi : $M_{\mathcal{B}_E}^{\mathcal{B}_F}(u) = (a_{ij}) \in \mathcal{M}_{p,q}(\mathbb{K})$, où $a_{ij}$ désigne la $i^e$ coordonnée de $u(e_j)$ dans la base $\mathcal{B}_F$ soit :
$$\forall j \in [1; q], u(e_j) = \sum_{i=1}^p a_{ij} e'_i.
$$
L'écriture de la matrice de $u$ dans les bases $\mathcal{B}_E$ et $\mathcal{B}_F$ peut être visualisée ainsi :
<!-- Visual representation omitted -->
</li>
</ul>

<!-- Image omitted: image0010050 -->
<!-- Page 46 -->
<p>$\checkmark$ Il est important de remarquer que si $u$ est une application linéaire d'un espace de dimension $q$ vers un espace de dimension $p$ alors $M_{\mathcal{B}_E}^{\mathcal{B}_F}(u)$ est une matrice de type $(p, q)$.</p>
<ul>
<li>L'application $\begin{cases} \mathcal{L}(E, F) \longrightarrow \mathcal{M}_{p, q}(\mathbb{K}) \\ \quad \quad \quad u \longmapsto M_{\mathcal{B}_E}^{\mathcal{B}_F}(u) \end{cases}$ est un isomorphisme d'espaces vectoriels, c'est-à-dire qu'elle est bijective et que :
$\forall (u, v) \in \mathcal{L}(E, F)^2, \forall \lambda \in \mathbb{K}, M_{\mathcal{B}_E}^{\mathcal{B}_F}(\lambda u + v) = \lambda M_{\mathcal{B}_E}^{\mathcal{B}_F}(u) + M_{\mathcal{B}_E}^{\mathcal{B}_F}(v)$.<br>
En particulier, pour toute matrice $A \in \mathcal{M}_{p, q}(\mathbb{K})$, il existe une unique application linéaire $a \in \mathcal{L}(\mathbb{K}^q, \mathbb{K}^p)$ telle que la matrice de $a$ dans les bases canoniques de $\mathbb{K}^q$ et $\mathbb{K}^p$ soit égale à $A$. L'application $a$ ainsi définie est appelée l'application linéaire canoniquement associée à $A$.</li>
<li><strong>Matrice d'un endomorphisme</strong><br>
Dans le cas où $u$ est un endomorphisme d'un espace vectoriel $E$ de dimension $n$ muni d'une base $\mathcal{B}$, on appelle matrice de $u$ dans la base $\mathcal{B}$ la matrice $M_{\mathcal{B}}^{\mathcal{B}}(u) \in \mathcal{M}_n(\mathbb{K})$, et on la note simplement $M_{\mathcal{B}}(u)$.<br>
Pour toute base $\mathcal{B}$ de $E$, on a $M_{\mathcal{B}}(\text{Id}_E) = I_n$. Plus généralement, si $u$ est une homothétie de $E$ de rapport $\lambda, M_{\mathcal{B}}(u) = \lambda I_n$ : c'est une matrice scalaire.</li>
<li><strong>Matrice d'une composée d'applications linéaires</strong>
<ul>
<li>Soient $E, F$ et $G$ des $\mathbb{K}$-espaces vectoriels de bases respectives $\mathcal{B}_E, \mathcal{B}_F$ et $\mathcal{B}_G$. Soient $u \in \mathcal{L}(E, F)$ et $v \in \mathcal{L}(F, G)$ deux applications linéaires. Alors :
$M_{\mathcal{B}_E}^{\mathcal{B}_G}(v \circ u) = M_{\mathcal{B}_F}^{\mathcal{B}_G}(v) \times M_{\mathcal{B}_E}^{\mathcal{B}_F}(u)$.</li>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n$ muni d'une base $\mathcal{B}$, et soit $u \in \mathcal{L}(E)$. On note $A = M_{\mathcal{B}}(u)$. Alors :
$u \in \text{GL}(E) \Longleftrightarrow A \in \text{GL}_n(\mathbb{K})$.<br>
De plus, dans ce cas, $A^{-1} = M_{\mathcal{B}}(u^{-1})$.</li>
</ul>
</li>
<li><strong>Expression analytique d'une application linéaire</strong><br>
Soient :
<ul>
<li>$E$ un $\mathbb{K}$-espace vectoriel de dimension $q$, rapporté à une base $\mathcal{B}_E = (e_1, \ldots, e_q)$;</li>
<li>$F$ un $\mathbb{K}$-espace vectoriel de dimension $p$, rapporté à une base $\mathcal{B}_F = (e_1', \ldots, e_p')$;</li>
<li>$u \in \mathcal{L}(E, F)$;</li>
<li>$A = M_{\mathcal{B}_E}^{\mathcal{B}_F}(u) = (a_{ij}) \in \mathcal{M}_{p, q}(\mathbb{K})$ la matrice de $u$ dans les bases $\mathcal{B}_E$ et $\mathcal{B}_F$.</li>
</ul>
Soit $x = \sum_{j=1}^q x_j e_j \in E$, et soit $y = u(x) = \sum_{i=1}^p y_i e_i' \in F$ son image par $u$. Alors :
$\forall i \in 〚 1; p 〛, y_i = \sum_{j=1}^q a_{ij} x_j$ (expression analytique de $u$ dans $\mathcal{B}_E$ et $\mathcal{B}_F$).<br>
Cela s'écrit matriciellement : $\boxed{Y = AX}$, où :
<ul>
<li>$X = ^t (x_1 \ldots x_q)$ est la matrice colonne des coordonnées de $x$ dans $\mathcal{B}_E$;</li>
<li>$Y = ^t (y_1 \ldots y_p)$ est la matrice colonne des coordonnées de $y$ dans $\mathcal{B}_F$.</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010051 -->
<!-- Page 47 -->
<h2>[S3.8] Matrices par blocs et sous-espaces stables</h2>
<ul>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel, et $u$ un endomorphisme de $E$. Un sous-espace vectoriel $F$ de $E$ est dit stable par $u$ si $u(F) \subset F$, c'est-à-dire si :
$$\forall x \in F, u(x) \in F.
$$
On peut alors définir l'endomorphisme $u_F$ induit par $u$ sur $F$ par :
$$\forall x \in F, u_F(x) = u(x).
$$
Il ne faut pas confondre l'endomorphisme induit $u_F$, qui n'est défini que si $F$ est stable par $u$ et qui est une application linéaire de $F$ dans $F$, avec la restriction $u|_{F}$, qui est une application linéaire de $F$ dans $E$.</li>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n \geq 1$ et $F$ un sous-espace de $E$, de dimension $p$. Soit $\mathscr{B}_F$ une base de $F$, que l'on complète en une base $\mathscr{B}_E$ de $E$. Soit $u$ un endomorphisme de $E$ et $A = M_{\mathscr{B}_E}(u)$ sa matrice dans $\mathscr{B}_E$. Alors $F$ est stable par $u$ si et seulement si $A$ est de la forme :
$$\begin{bmatrix}
A_1 & B \\
0_{n-p,p} & A_2
\end{bmatrix}
\quad \text{avec } A_1 \in M_p(\mathbb{K}).
$$
Une telle matrice est dite triangulaire supérieure par blocs.<br>
Dans ce cas, $A_1 = M_{\mathscr{B}_F}(u_F)$Où $u_F$ est l'endomorphisme induit par $u$ sur $F$.</li>
</ul>
<h3>Généralisation</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n \geq 1$, $E_1, \ldots, E_p$ $p$ sous-espaces vectoriels supplémentaires de $E$ et $\mathscr{B}_E = \bigcup_{i=1}^{p} \mathscr{B}_i$ une base adaptée à la décomposition en somme directe $E = \bigoplus_{i=1}^{p} E_i$.<br>
Soit $u$ un endomorphisme de $E$. Alors les sous-espaces $E_i$ sont stables par $u$ si et seulement si la matrice $A$ de $u$ dans $\mathscr{B}_E$ est diagonale par blocs, c'est-à-dire de la forme :
$$A = \begin{bmatrix}
A_1 & 0 & \cdots & 0 \\
0 & A_2 & \ddots & \vdots \\
\vdots & \ddots & \ddots & 0 \\
0 & \cdots & 0 & A_p
\end{bmatrix}
\quad \text{avec } A_i \in M_{n_i}(\mathbb{K}) \text{ où } n_i = \dim E_i.
$$
Pour tout $i \in 〚 1 ; p 〛$, $A_i$ est alors la matrice dans $\mathscr{B}_i$ de l'endomorphisme $u_i$ induit par $u$ sur $E_i$.</p>
<h2>[S3.9] Rang d'une matrice</h2>
<p>Le rang d'une matrice $A \in M_{p,n}(\mathbb{K})$ est, par définition, le rang de ses vecteurs colonnes (éléments de $M_{p,1}(\mathbb{K})$, que l'on peut assimiler à des éléments de $\mathbb{K}^p$). On le note $\text{rg } A$. D'autres interprétations sont possibles :</p>

<!-- Image omitted: image0010052 -->
<!-- Page 48 -->
<ul>
<li>si $A = M_{\mathcal{B}_F}(x_1, \ldots, x_q)$ est la matrice d'une famille de vecteurs $(x_1, \ldots, x_q)$ de $F$ relativement à une base $\mathcal{B}_F$, alors :
$$\operatorname{rg} A = \operatorname{rg}(x_1, \ldots, x_q) = \dim (\operatorname{Vect}(x_1, \ldots, x_q)) ;
$$</li>
<li>si $A = M_{\mathcal{B}_E}^{\mathcal{B}_F}(u)$ est la matrice d'une application linéaire $u \in \mathcal{L}(E, F)$ relativement à des bases $\mathcal{B}_E$ et $\mathcal{B}_F$, alors $\operatorname{rg} A = \operatorname{rg} u$.</li>
</ul>
<ul>
<li>Soit $A \in \mathcal{M}_{p, q}(\mathbb{K})$. Alors on a les résultats suivants.
<ul>
<li>$\operatorname{rg} A \leq \min(p, q)$.</li>
<li>$\operatorname{rg} A = \operatorname{rg}(^t A)$.</li>
</ul>
</li>
<li>Si $A = M_{\mathcal{B}_E}^{\mathcal{B}_F}(u)$ est la matrice d'une application linéaire $u \in \mathcal{L}(E, F)$ dans des bases $\mathcal{B}_E$ et $\mathcal{B}_F$, alors :
$$\operatorname{rg} A = p \Longleftrightarrow u \text{ surjective} ; \quad \operatorname{rg} A = q \Longleftrightarrow u \text{ injective}.
$$</li>
<li>En particulier, pour $A \in \mathcal{M}_n(\mathbb{K})$, on a :
$$A \text{ inversible} \Longleftrightarrow \operatorname{rg} A = n.
$$</li>
<li><strong>Rang d'un produit</strong>. Soient $A \in \mathcal{M}_{p, q}(\mathbb{K})$ et $B \in \mathcal{M}_{q, r}(\mathbb{K})$. On a les résultats suivants.
<ul>
<li>$\operatorname{rg}(AB) \leq \min(\operatorname{rg} A, \operatorname{rg} B)$.</li>
<li>Si $p = q$ et $A \in \operatorname{GL}_p(\mathbb{K})$, alors $\operatorname{rg}(AB) = \operatorname{rg}(B)$.</li>
<li>Si $q = r$ et $B \in \operatorname{GL}_q(\mathbb{K})$, alors $\operatorname{rg}(AB) = \operatorname{rg}(A)$.</li>
</ul>
</li>
<li><strong>Caractérisation des matrices de rang $r$</strong><br>
Le rang d'une matrice non nulle $A \in \mathcal{M}_{p, q}(\mathbb{K})$ est le plus grand entier $r \geq 1$ tel que l'on puisse extraire de $A$ une matrice carrée inversible d'ordre $r$.</li>
</ul>
<h2>[S3.10] Formules de changement de base</h2>
<h3>Matrices de passage</h3>
<ul>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n$, et $\mathcal{B} = (e_1, \ldots, e_n)$ et $\mathcal{B}' = (e'_1, \ldots, e'_n)$ deux bases de $E$.<br>
On appelle matrice de passage de $\mathcal{B}$ à $\mathcal{B}'$ la matrice du système $(e'_1, \ldots, e'_n)$ dans la base $\mathcal{B}$. On la note généralement $P_{\mathcal{B}}^{\mathcal{B}'}$.<br>
Ainsi, pour former la matrice de passage de $\mathcal{B} \text{ à } \mathcal{B}'$, on écrit dans les colonnes les coordonnées dans $\mathcal{B}$ des vecteurs de $\mathcal{B}'.$</li>
</ul>
<h3>Propriétés</h3>
<ul>
<li>Si $\mathcal{B}$, $\mathcal{B}'$ et $\mathcal{B}''$ sont trois bases de $E$, alors $P_{\mathcal{B}}^{\mathcal{B}''} = P_{\mathcal{B}}^{\mathcal{B}'} \times P_{\mathcal{B}'}^{\mathcal{B}''}$. (Note : il semble y avoir une inversion d'indice dans le markdown original, corrigée ici pour $P_{\mathcal{B}}^{\mathcal{B}''} = P_{\mathcal{B}}^{\mathcal{B}'} P_{\mathcal{B}'}^{\mathcal{B}''}$).</li>
<li>$P_{\mathcal{B}}^{\mathcal{B}'}$ est inversible et $\left(P_{\mathcal{B}}^{\mathcal{B}'}\right)^{-1} = P_{\mathcal{B}'}^{\mathcal{B}}$.</li>
</ul>

<!-- Image omitted: image0010053 -->
<!-- Page 49 -->
<h3>Formule de changement de base pour un vecteur</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n$, $\mathcal{B}$ et $\mathcal{B'}$ deux bases de $E$, et $P$ la matrice de passage de $\mathcal{B}$ à $\mathcal{B'}$.<br>
Soit $x$ un vecteur de $E$, $X$ la matrice colonne de ses coordonnées dans $\mathcal{B}$ et $X'$ celle de ses coordonnées dans $\mathcal{B'}$.<br>
On a alors la relation :</p>
$$X = PX'
$$
<p>(Note : la formule $X' = PX$ indiquée dans le markdown original est incorrecte. La formule correcte exprime les anciennes coordonnées $X$ en fonction des nouvelles $X'$ à l'aide de la matrice de passage $P$ de l'ancienne base $\mathcal{B}$ à la nouvelle base $\mathcal{B}'$.)</p>
<h3>Formule de changement de bases pour une application linéaire</h3>
<p>Soient :</p>
<ul>
<li>$E$ un $\mathbb{K}$-espace vectoriel de dimension $q$, muni de deux bases $\mathcal{B}_E$ et $\mathcal{B'}_E$;</li>
<li>$F$ un $\mathbb{K}$-espace vectoriel de dimension $p$, muni de deux bases $\mathcal{B}_F$ et $\mathcal{B'}_F$;</li>
<li>$P \in \text{GL}_q(\mathbb{K})$ la matrice de passage de $\mathcal{B}_E$ à $\mathcal{B'}_E$;</li>
<li>$Q \in \text{GL}_p(\mathbb{K})$ la matrice de passage de $\mathcal{B}_F$ à $\mathcal{B'}_F$;</li>
<li>$u \in \mathcal{L}(E, F)$, $A = M_{\mathcal{B}_E}^{\mathcal{B}_F}(u)$ la matrice de $u$ dans les bases $\mathcal{B}_E$ et $\mathcal{B}_F$, $A' = M_{\mathcal{B'}_E}^{\mathcal{B'}_F}(u)$ celle dans les bases $\mathcal{B'}_E$ et $\mathcal{B'}_F$ ($A, A' \in \mathcal{M}_{p,q}(\mathbb{K})$).</li>
</ul>
<p>On a alors la relation :</p>
$$A' = Q^{-1}AP
$$
<h3>Formule de changement de base pour un endomorphisme</h3>
<p>Soient :</p>
<ul>
<li>$E$ un $\mathbb{K}$-espace vectoriel de dimension $n$, muni de deux bases $\mathcal{B}_E$ et $\mathcal{B'}_E$;</li>
<li>$P \in \text{GL}_n(\mathbb{K})$ la matrice de passage de $\mathcal{B}_E$ à $\mathcal{B'}_E$;</li>
<li>$u \in \mathcal{L}(E)$, $A = M_{\mathcal{B}_E}(u)$ sa matrice dans la base $\mathcal{B}$ et $A' = M_{\mathcal{B'}_E}(u)$ celle dans la base $\mathcal{B'}$ ($A, A' \in \mathcal{M}_n(\mathbb{K})$).</li>
</ul>
<p>On a alors la relation :</p>
$$A' = P^{-1}AP
$$
<h2>[S3.11] Matrices équivalentes</h2>
<ul>
<li>On dit qu'une matrice $A' \in \mathcal{M}_{p,q}(\mathbb{K})$ est équivalente à la matrice $A \in \mathcal{M}_{p,q}(\mathbb{K})$ s'il existe $Q \in \text{GL}_p(\mathbb{K})$ et $P \in \text{GL}_q(\mathbb{K})$ telles que $A' = Q^{-1}AP$.</li>
<li>On vérifie facilement qu'il s'agit bien d'une relation d'équivalence dans $\mathcal{M}_{p,q}(\mathbb{K})$.</li>
<li>$A$ et $A'$ sont équivalentes si et seulement si ce sont les matrices, dans des couples de bases différentes, d'une même application linéaire $u$ d'un espace vectoriel de dimension $q$ dans un espace vectoriel de dimension $p$.</li>
</ul>
<h3>Caractérisation des matrices de rang $r$</h3>
<p>Une matrice $A \in \mathcal{M}_{p,q}(\mathbb{K})$ est de rang $r$ si et seulement si elle est équivalente à la matrice :
$$J_{p,q,r} = \begin{bmatrix}
I_r & 0_{r,q-r} \\
0_{p-r,r} & 0_{p-r,q-r}
\end{bmatrix}
$$</p>
<ul>
<li>Deux matrices $A$ et $A' \in \mathcal{M}_{p,q}(\mathbb{K})$ sont équivalentes si et seulement si elles ont même rang.</li>
</ul>

<!-- Image omitted: image0010054 -->
<!-- Page 50 -->
<h2>[S3.12] Matrices semblables, trace</h2>
<h3>Matrices semblables</h3>
<p>Soient $A$ et $A'$ deux matrices carrées de $\mathcal{M}_n(\mathbb{K})$. On dit que $A$ et $A'$ sont semblables s'il existe $P \in \text{GL}_n(\mathbb{K})$ telle que $A' = P^{-1}AP$.<br>
$\checkmark$ Dire que $A$ et $A'$ sont semblables signifie donc que ce sont les matrices dans deux bases différentes d'un même endomorphisme $u$ d'un espace vectoriel de dimension $n$. Cette interprétation est importante à retenir.<br>
On appelle trace d'une matrice carrée $A = (a_{ij}) \in \mathcal{M}_n(\mathbb{K})$ le scalaire :
$$\text{tr} A = \sum_{i=1}^{n} a_{ii} \quad \text{(somme des éléments diagonaux)}$$</p>
<ul>
<li>L'application $\text{tr} : \mathcal{M}_n(\mathbb{K}) \to \mathbb{K}$ est une forme linéaire.</li>
<li>$\forall A \in \mathcal{M}_{n,p}(\mathbb{K}), \forall B \in \mathcal{M}_{p,n}(\mathbb{K}), \text{tr}(AB) = \text{tr}(BA)$.</li>
<li>Deux matrices semblables ont même trace.</li>
</ul>
<h3>Trace d'un endomorphisme</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n \geq 1$, et soit $u \in \mathscr{L}(E)$. Pour toute base $\mathscr{B}$ de $E$, le scalaire $\text{tr}(M_{\mathscr{B}}(u))$ ne dépend pas de la base $\mathscr{B}$ choisie. Ce scalaire s'appelle la trace de l'endomorphisme $u$, notée $\text{tr} u$.</p>
<ul>
<li>L'application $\text{tr} : \mathscr{L}(E) \to \mathbb{K}$ est une forme linéaire.</li>
<li>$\forall u, v \in \mathscr{L}(E), \text{tr}(v \circ u) = \text{tr}(u \circ v)$.</li>
</ul>
<h3>Trace d'un projecteur</h3>
<p>Si $p$ est un projecteur de $E$, alors $\text{tr} p = \text{rg} p$.</p>
<h2>[S3.13] Opérations élémentaires sur les lignes d'une matrice</h2>
<ul>
<li>Soit $A \in \mathcal{M}_{p,q}(\mathbb{K})$, de lignes notées $L_i (1 \leq i \leq p)$ et de colonnes notées $C_j (1 \leq j \leq q)$.<br>
On appelle opération élémentaire sur les lignes de $A$ l'une des opérations suivantes :
<ul>
<li>échange des lignes $L_i$ et $L_j$, notée $L_i \leftrightarrow L_j$;</li>
<li>ajout à la ligne $L_i$ de la ligne $L_j$ multipliée par un scalaire $\lambda \in \mathbb{K}$, où $i \neq j$, notée $L_i \leftarrow L_i + \lambda L_j$;</li>
<li>multiplication de la ligne $L_i$ par un scalaire $\lambda \in \mathbb{K}^*$, notée $L_i \leftarrow \lambda L_i$.</li>
</ul>
On définit de manière analogue les opérations élémentaires sur les colonnes.</li>
</ul>
<h3>Matrices élémentaires</h3>
<p>Soit $(E_{ij})_{1 \leq i,j \leq p}$ la base canonique de $\mathcal{M}_p(\mathbb{K})$, et soit $A \in \mathcal{M}_{p,q}(\mathbb{K})$.</p>
<ul>
<li>Si $A'$ est la matrice obtenue à partir de $A$ par l'opération :
$$ L_i \leftarrow L_i + \lambda L_j \ (i \neq j), $$alors $A' = T_{i,j}(\lambda)A$, où $T_{i,j}(\lambda) = I_p + \lambda E_{ij}$ (matrice de transvection).</li>
</ul>

<!-- Image omitted: image0010055 -->
<!-- Page 51 -->
<ul>
<li>Si $A'$ est la matrice obtenue à partir de $A$ par l'opération :
$$L_i \leftarrow \lambda L_i \ (\lambda \in \mathbb{K}^*)
$$
alors $A' = D_i(\lambda)A$, où $D_i(\lambda) = I_p + (\lambda - 1)E_{ii}$ (matrice de dilatation).</li>
<li>Si $A'$ est la matrice obtenue à partir de $A$ par l'opération
$$L_i \leftrightarrow L_j
$$
alors $A' = P_{i,j}A$, où $P_{i,j} = I_p - E_{ii} - E_{jj} + E_{ij} + E_{ji}$ (matrice de transposition).</li>
<li>Les matrices $T_{i,j}(\lambda)$ avec $i \neq j$, $D_i(\lambda)$ avec $\lambda \neq 0$ et $P_{i,j}$ sont appelées matrices élémentaires. Ce sont des matrices carrées inversibles.<br>
$\checkmark$On obtient des résultats similaires pour les opérations élémentaires sur les colonnes : il faut alors multiplier $A$ à droite par une matrice élémentaire $Q \in \text{GL}_q(\mathbb{K})$.<br>
$\checkmark$ Si $A'$ est obtenue à partir de $A$ par une suite d'opérations élémentaires sur les lignes ou les colonnes, $A$ et $A'$ sont équivalentes.</li>
</ul>
<h3>Matrices échelonnées</h3>
<ul>
<li>Soit $A \in \mathcal{M}_{p,q}(\mathbb{K})$. Pour tout $i \in 〚 1 ; p 〛$, on note :
$$m_i = \begin{cases}
\min \{ j \in 〚 1 ; q 〛 \mid a_{ij} \neq 0 \} & \text{si } L_i \neq 0, \\
q + i & \text{si } L_i = 0.
\end{cases}
$$
Si la ligne $L_i$ est non nulle, $m_i$ désigne donc le numéro de colonne du premier terme non nul de $L_i$ en partant de la gauche. Le terme $a_{ij}$ correspondant s'appelle un pivot.</li>
<li>On dit que $A$ est échelonnée par lignes si la suite finie $(m_i)_{1 \leq i \leq p}$ est strictement croissante.<br>
Ainsi, dans une matrice échelonnée par lignes, la position des pivots croît strictement avec $i$ jusqu'aux lignes nulles éventuelles, et si une ligne est nulle, alors toutes les lignes suivantes sont nulles.</li>
<li>Une matrice $A$ échelonnée par lignes est dite échelonnée réduite par lignes si $A$ est nulle ou si tous les pivots de $A$ valent 1 et sont les seuls termes non nuls de leur colonne.</li>
<li>On définit de manière analogue les notions de matrice échelonnée par colonnes et de matrice échelonnée réduite par colonnes.</li>
</ul>
<h3>Théorème de Gauss-Jordan</h3>
<p>Soit $A \in \mathcal{M}_{p,q}(\mathbb{K})$.</p>
<ul>
<li>Il existe une unique matrice $R \in \mathcal{M}_{p,q}(\mathbb{K})$ échelonnée réduite par lignes telle que $A \sim R$. De plus, il existe $U \in \text{GL}_p(\mathbb{K})$, produit de matrices élémentaires, telle que $UA = R$.</li>
<li>Il existe une unique matrice $R' \in \mathcal{M}_{p,q}(\mathbb{K})$ échelonnée réduite par colonnes telle que $A \sim R'$. De plus, il existe $V \in \text{GL}_q(\mathbb{K})$, produit de matrices élémentaires, telle que $AV = R'$.</li>
</ul>

<!-- Image omitted: image0010056 -->
<!-- Page 52 -->
<p>√ La méthode du pivot permet ainsi de retrouver le résultat mentionné en [S3.11] : toute matrice de $M_{p,q}(\mathbb{K})$ de rang $r$ est équivalente à $J_{p,q,r}$.</p>
<ul>
<li><strong>Utilisation des opérations élémentaires</strong>
<ul>
<li>Calcul du rang d'une matrice<br>
Si deux matrices $A, A' \in M_{p,q}(\mathbb{K})$ sont équivalentes, elles ont même rang. Pour déterminer le rang de $A$, il suffit donc de la rendre équivalente par une suite d'opérations élémentaires sur les lignes ou les colonnes à une matrice triangulaire, dont le rang s'obtient directement.</li>
<li>Calcul de l'inverse d'une matrice carrée (s'il existe)<br>
Soit $A \in M_{n}(\mathbb{K})$. Si $A$ est inversible, il existe $P_{1}, P_{2}, \ldots, P_{m} \in GL_{n}(\mathbb{K})$ (matrices élémentaires) telles que : $P_{m}P_{m-1} \ldots P_{1}A = I_{n}$.<br>
Dans ce cas, $A^{-1} = P_{m}P_{m-1} \ldots P_{1}I_{n}$, ce qui signifie que $A^{-1}$ se déduit de $I_{n}$ par la même suite d'opérations sur les lignes que celle utilisée pour déduire $I_{n}$ de $A$.<br>
√ On peut aussi utiliser des opérations sur les colonnes, ce qui revient à multiplier à droite par des matrices élémentaires, mais il ne faut pas mélanger les opérations sur les lignes et les colonnes!</li>
</ul>
</li>
</ul>
<h2>[S3.14] Déterminants</h2>
<ul>
<li><strong>Applications $n$-linéaires</strong><br>
Soit $n \in \mathbb{N}^{*}$. Soient $E$ et $F$ deux $\mathbb{K}$-espaces vectoriels. Une application $f: E^{n} \rightarrow F$ est dite $n$-linéaire si, pour tout $j \in 〚 1 ; n 〛$ et pour toute famille $(a_{i})_{i \in 〚 1 ; n 〛 \setminus \{j\}}$ composée de $(n-1)$ vecteurs de $E$, l'application partielle :
$$f_{j}: \begin{cases}
E \longrightarrow F \\
x \longmapsto f(a_{1}, \ldots, a_{j-1}, x, a_{j+1}, \ldots, a_{n})
\end{cases}$$
est une application linéaire sur $E$.<br>
Lorsque $F = \mathbb{K}$, on parle de forme $n$-linéaire.</li>
<li><strong>Applications $n$-linéaires symétriques, antisymétriques</strong><br>
$\mathfrak{S}_{n}$ désigne le groupe symétrique d'ordre $n$, c'est-à-dire l'ensemble des permutations de $〚 1 ; n 〛$ (cf. [S1.8]).<br>
Une application $n$-linéaire de $E^{n}$ dans $F$ est dite :
<ul>
<li>symétrique si :$$\forall \sigma \in \mathfrak{S}_{n}, \forall (x_{i})_{1 \leq i \leq n} \in E^{n}, f(x_{\sigma(1)}, \ldots, x_{\sigma(n)}) = f(x_{1}, \ldots, x_{n}).
$$</li>
<li>antisymétrique si :
$$\forall \sigma \in \mathfrak{S}_{n}, \forall (x_{i})_{1 \leq i \leq n} \in E^{n}, f(x_{\sigma(1)}, \ldots, x_{\sigma(n)}) = \varepsilon(\sigma) f(x_{1}, \ldots, x_{n})
$$
(où $\varepsilon(\sigma)$ désigne la signature de la permutation $\sigma$).</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010057 -->
<!-- Page 53 -->
<h3>Applications $n$-linéaires alternées</h3>
<p>Soit $f$ une application $n$-linéaire de $E^n$ dans $F$, $n \geqslant 2$.<br>
$f$ est dite alternée si, pour toute famille $(x_i)_{1 \leqslant i \leqslant n} \in E^n$, on a :
$$f(x_1, \ldots, x_n) = 0 \text{ dès qu'il existe deux indices } i \neq j \text{ tels que } x_i = x_j.$$</p>
<h3>Propriétés</h3>
<ul>
<li>$f$ antisymétrique $\Longleftrightarrow$ $f$ alternée.</li>
<li>Si $f$ est une application $n$-linéaire alternée, alors :
<ul>
<li>$(x_1, \ldots, x_n)$ liée $\Longrightarrow f(x_1, \ldots, x_n) = 0$;</li>
<li>on ne change pas la valeur de $f(x_1, \ldots, x_n)$ si l'on ajoute à un des vecteurs une combinaison linéaire des autres.</li>
</ul>
</li>
</ul>
<h3>Déterminant d'une famille de vecteurs</h3>
<p>Soient :</p>
<ul>
<li>$E$ un $\mathbb{K}$-espace vectoriel de dimension $n$ ($n \in \mathbb{N}^*$);</li>
<li>$\mathcal{B} = (e_1, \ldots, e_n)$ une base de $E$;</li>
<li>$(v_j)_{1 \leqslant j \leqslant n}$ une famille de vecteurs de $E$, et $A = (a_{ij})_{1 \leqslant i, j \leqslant n} \in \mathcal{M}_n(\mathbb{K})$ la matrice de cette famille dans la base $\mathcal{B}$, de sorte que :
$$\forall j \in 〚 1; n 〛, v_j = \sum_{i=1}^n a_{ij} e_i.$$</li>
</ul>
<p>Alors pour toute forme $n$-linéaire alternée $\varphi$ sur $E^n$ on a :
$$\varphi(v_1, \ldots, v_n) = \varphi(e_1, \ldots, e_n) \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod_{i=1}^n a_{\sigma(i)i}.$$
Le scalaire : $\operatorname{det}_{\mathcal{B}}(v_1, \ldots, v_n) = \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod_{i=1}^n a_{\sigma(i)i}$ s'appelle le déterminant dans la base $\mathcal{B}$ du système de vecteurs $(v_1, \ldots, v_n)$.</p>
<ul>
<li>Le déterminant d'une famille de vecteurs dépend de la base $\mathcal{B}$ choisie.</li>
<li>Le déterminant n'est défini que pour une famille de vecteurs dont le cardinal est égal à la dimension de $E$.</li>
</ul>
<h3>Propriétés</h3>
<ul>
<li>Avec les notations précédentes, on a aussi :
$$\operatorname{det}_{\mathcal{B}}(v_1, \ldots, v_n) = \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod_{i=1}^n a_{i \sigma(i)}.$$</li>
<li>L'application $\operatorname{det}_{\mathcal{B}} : E^n \rightarrow \mathbb{K}$ est une forme $n$-linéaire alternée.</li>
<li>$\operatorname{det}_{\mathcal{B}}(\mathcal{B}) = 1$.</li>
<li>Pour toute forme $n$-linéaire alternée $\varphi$ sur $E$, il existe un scalaire $\lambda$ tel que $\varphi = \lambda \operatorname{det}_{\mathcal{B}}$ (autrement dit, l'ensemble des formes $n$-linéaires alternées est la droite vectorielle engendrée par $\operatorname{det}_{\mathcal{B}}$).</li>
</ul>

<!-- Image omitted: image0010058 -->
<!-- Page 54 -->
<ul>
<li>$\det_{\mathscr{B}}$ est l'unique forme $n$-linéaire alternée $\varphi$ telle que $\varphi(\mathscr{B}) = 1$.</li>
<li>Pour toute permutation $\sigma \in \mathfrak{S}_n$, on a :
$$\det_{\mathscr{B}}(v_{\sigma(1)}, \ldots, v_{\sigma(n)}) = \varepsilon(\sigma) \det_{\mathscr{B}}(v_1, \ldots, v_n).
$$
En particulier, si l'on échange deux vecteurs dans la famille $(v_1, \ldots, v_n)$, le déterminant change de signe.</li>
<li>Pour tout $\lambda \in \mathbb{K}$, $\det_{\mathscr{B}}(\lambda v_1, \ldots, \lambda v_n) = \lambda^n \det_{\mathscr{B}}(v_1, \ldots, v_n)$.</li>
<li>On ne change pas le déterminant d'un système de vecteurs si l'on ajoute à l'un d'entre eux une combinaison linéaire des autres.</li>
</ul>
<h3>Changement de base</h3>
<p>Soient $\mathscr{B}$ et $\mathscr{B}'$ deux bases de $E$. Alors :</p>
<ol type="i">
<li>$\det_{\mathscr{B}'}(v_1, \ldots, v_n) = \det_{\mathscr{B}'}(\mathscr{B}) \times \det_{\mathscr{B}}(v_1, \ldots, v_n)$.</li>
<li>$\det_{\mathscr{B}'}(\mathscr{B}) \times \det_{\mathscr{B}}(\mathscr{B}') = 1$.</li>
</ol>
<h3>Caractérisation des bases</h3>
<p>Soit $(v_1, \ldots, v_n)$ une famille de vecteurs d'un espace vectoriel $E$ de dimension $n$, et $\mathscr{B}$ une base de $E$. Alors :
$$(v_1, \ldots, v_n) \text{ est une base de } E \Longleftrightarrow \det_{\mathscr{B}}(v_1, \ldots, v_n) \neq 0.
$$</p>
<h3>Déterminant d'un endomorphisme</h3>
<p>$E$ désigne toujours un $\mathbb{K}$-espace vectoriel de dimension $n \in \mathbb{N}^*$. Soit $u \in \mathscr{L}(E)$. Le scalaire $\det_{\mathscr{B}}(u(\mathscr{B}))$ est indépendant de la base $\mathscr{B}$ choisie. Ce scalaire s'appelle le déterminant de l'endomorphisme $u$, noté $\det u$.</p>
<h4>Propriétés</h4>
<ul>
<li>Pour tout endomorphisme $u$ de $E$, pour toute famille $(v_i)_{1 \leq i \leq n} \in E^n$ et pour toute base $\mathscr{B}$ de $E$ :
$$\det_{\mathscr{B}}(u(v_1), \ldots, u(v_n)) = \det u \times \det_{\mathscr{B}}(v_1, \ldots, v_n).
$$</li>
<li>$\det \text{Id}_E = 1$.</li>
<li>$\forall u \in \mathscr{L}(E), \forall \lambda \in \mathbb{K}, \det(\lambda u) = \lambda^n \det u$.</li>
<li>$\forall u \in \mathscr{L}(E), u \in \text{GL}(E) \Longleftrightarrow \det u \neq 0$.</li>
<li>$\forall u, v \in \mathscr{L}(E), \det(u \circ v) = \det u \times \det v$.</li>
<li>$\forall u \in \text{GL}(E), \det(u^{-1}) = \frac{1}{\det u}$.</li>
<li>L'ensemble des automorphismes de $E$ de déterminant égal à 1 est un sous-groupe du groupe $\text{GL}(E)$, appelé groupe spécial linéaire, et noté $\text{SL}(E)$.</li>
</ul>
<h3>Déterminant d'une matrice carrée</h3>
<p>Soit $A = (a_{ij})_{1 \leq i, j \leq n} \in \mathcal{M}_n(\mathbb{K})$. On appelle déterminant de $A$ le déterminant des vecteurs colonnes de $A$ dans la base canonique de $\mathcal{M}_{n,1}(\mathbb{K})$ :</p>
$$\det A = \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod_{i=1}^n a_{\sigma(i)i} = \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod_{i=1}^n a_{i \sigma(i)}.
$$
<p>Si $A$ est la matrice dans une base $\mathscr{B}$ d'un $\mathbb{K}$-espace vectoriel $E$ d'un endomorphisme $u$, alors $\det A = \det u$.</p>

<!-- Image omitted: image0010059 -->
<!-- Page 55 -->
<h4>Propriétés</h4>
<ul>
<li>$\det I_n = 1$.</li>
<li>$\forall A \in \mathcal{M}_n(\mathbb{K}), \forall \lambda \in \mathbb{K}, \det(\lambda A) = \lambda^n \det A$.</li>
<li>$\forall A \in \mathcal{M}_n(\mathbb{K}), A \in \text{GL}_n(\mathbb{K}) \iff \det A \neq 0$.</li>
<li>$\forall A, B \in \mathcal{M}_n(\mathbb{K}), \det(AB) = \det A \times \det B = \det(BA)$.</li>
<li>$\forall A \in \text{GL}_n(\mathbb{K}), \det(A^{-1}) = \frac{1}{\det A}$.</li>
<li>Deux matrices semblables ont même déterminant.</li>
<li>$\forall A \in \mathcal{M}_n(\mathbb{K}), \det(^t A) = \det A$.</li>
</ul>
<h3>Calcul du déterminant par opérations élémentaires</h3>
<ul>
<li>Le déterminant d'une matrice est multiplié par $-1$ lors de l'échange de deux de ses colonnes ou de deux de ses lignes.</li>
<li>Pour tout scalaire $\lambda$, le déterminant d'une matrice est multiplié par $\lambda$ lorsqu'une colonne ou une ligne est multipliée par $\lambda$.</li>
<li>Le déterminant d'une matrice reste inchangé en ajoutant à l'une de ses colonnes une combinaison linéaire des autres colonnes ou en ajoutant à l'une de ses lignes une combinaison linéaire des autres lignes.</li>
</ul>
<h3>Calcul d'un déterminant par blocs</h3>
<p>Soit $A \in \mathcal{M}_n(\mathbb{K})$, écrite par blocs sous la forme $A = \begin{bmatrix} A_1 & B_2 \\ 0 & A_2 \end{bmatrix}$, où $A_1$ et $A_2$ sont deux blocs carrés. Alors $\det A = \det A_1 \times \det A_2$.<br>
En particulier, le déterminant d'une matrice triangulaire est égal au produit de ses termes diagonaux.</p>
<h3>Développement suivant une rangée</h3>
<ul>
<li>Soit $A = (a_{ij})_{1 \leq i,j \leq n} \in \mathcal{M}_n(\mathbb{K})$. Pour tout $(i, j) \in 〚 1 ; n 〛^2$, on note $\Delta_{ij}$ le déterminant obtenu en supprimant la $i$-ème ligne et la $j$-ème colonne de $A$. On dit alors que $\Delta_{ij}$ est le mineur d'indice $(i, j)$ de la matrice $A$.</li>
<li>Développement de $\det A$ selon la $j$-ème colonne :
$\forall j \in 〚 1 ; n 〛, \det A = \sum_{i=1}^{n} (-1)^{i+j} a_{ij} \Delta_{ij}$.</li>
<li>Développement de $\det A$ selon la $i$-ème ligne :
$\forall i \in 〚 1 ; n 〛, \det A = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \Delta_{ij}$.</li>
<li>Le terme $(-1)^{i+j} \Delta_{ij}$ s'appelle le cofacteur d'indice $(i, j)$ de la matrice $A$.</li>
</ul>
<h3>Comatrice</h3>
<p>Soit $A \in \mathcal{M}_n(\mathbb{K})$. On appelle comatrice de $A$, notée $\text{com}(A)$, la matrice formée des cofacteurs de $A$, soit plus précisément :
$\forall (i, j) \in 〚 1 ; n 〛^2, (\text{com}(A))_{ij} = (-1)^{i+j} \Delta_{ij}$ avec $\Delta_{ij}$ mineur d'indice $(i, j)$.<br>
On appelle matrice complémentaire de $A$ la matrice $\tilde{A} = ^t \text{com}(A)$. On a alors :</p>

<!-- Image omitted: image0010060 -->
<!-- Page 56 -->
<ul>
<li>$\forall A \in \mathcal{M}_n(\mathbb{K}), A\tilde{A} = \tilde{A}A = \det A . I_n$.</li>
<li>Si $A \in \text{GL}_n(\mathbb{K})$, alors : $A^{-1} = \frac{1}{\det A} \tilde{A}$.</li>
</ul>
<h3>Déterminant de Vandermonde</h3>
<p>On appelle déterminant de Vandermonde un déterminant de la forme :
$$V(a_1, a_2, \ldots, a_n) = \begin{vmatrix}
1 & a_1 & a_1^2 & \cdots & a_1^{n-1} \\
1 & a_2 & a_2^2 & \cdots & a_2^{n-1} \\
\vdots & \vdots & \vdots & \cdots & \vdots \\
1 & a_n & a_n^2 & \cdots & a_n^{n-1}
\end{vmatrix}
$$
où $(a_1, \ldots, a_n) \in \mathbb{K}^n$.<br>
On sait calculer ce déterminant :
$$V(a_1, a_2, \ldots, a_n) = \prod_{1 \leq i < j \leq n} (a_j - a_i).
$$
Il en résulte que :
$$V(a_1, a_2, \ldots, a_n) \neq 0 \Longleftrightarrow \text{les } a_i \text{ sont deux à deux distincts}.
$$</p>
<h2>[S3.15] Systèmes linéaires</h2>
<ul>
<li>Un système linéaire de $p$ équations à $q$ inconnues est un système de la forme :
$$\begin{cases}
a_{11}x_1 + \ldots + a_{1q}x_q & = b_1 \\
a_{21}x_1 + \ldots + a_{2q}x_q & = b_2 \\
& \vdots \\
a_{p1}x_1 + \ldots + a_{pq}x_q & = b_p
\end{cases}
$$
où les coefficients $(a_{ij})_{1 \leq i \leq p, 1 \leq j \leq q}$ et $(b_i)_{1 \leq i \leq p}$ sont donnés dans $\mathbb{K}$. $x_1, \ldots, x_q$, éléments de $\mathbb{K}$, sont les inconnues.<br>
Le système est dit homogène si $b_1 = \ldots = b_p = 0$.</li>
</ul>
<h3>Interprétations</h3>
<p>Un système linéaire peut être interprété de différentes façons, chacune correspondant à un problème donné.</p>
<ul>
<li>La matrice $A = (a_{ij})_{1 \leq i \leq p, 1 \leq j \leq q}$ $\in \mathcal{M}_{p,q}(\mathbb{K})$ s'appelle la matrice du système; le rang de $A$ s'appelle le rang du système.<br>
Soit alors $B = ^t(b_1 \ldots b_p) \in \mathcal{M}_{p,1}(\mathbb{K})$ et $X = ^t(x_1 \ldots x_q) \in \mathcal{M}_{q,1}(\mathbb{K})$; le système $(S)$ s'écrit alors matriciellement sous la forme
$$AX = B.
$$</li>
<li>En notant $C_1, \ldots, C_q$ les vecteurs colonnes de $A$, on peut aussi écrire le système sous la forme : $\sum_{j=1}^q x_j C_j = B$.</li>
</ul>

<!-- Image omitted: image0010061 -->
<!-- Page 57 -->
<p>Résoudre le système équivaut donc à écrire $B$ comme combinaison linéaire des $C_j$.<br>
On notera que le rang du système est aussi égal au rang du système de vecteurs $(C_1, \ldots, C_q)$.</p>
<p>Soient $E$ et $F$ deux $\mathbb{K}$-espaces vectoriels tels que $\dim E = q$ et $\dim F = p$. Soient alors $\mathscr{B}_E$ et $\mathscr{B}_F$ deux bases respectivement de $E$ et de $F$, et $u \in \mathscr{L}(E, F)$ telle que $A = M_{\mathscr{B}_E}^{\mathscr{B}_F}(u)$.<br>
Le système $(S)$ peut alors s'écrire $u(x) = b$, avec $x \in E$ de coordonnées $(x_1, \ldots, x_q)$ dans $\mathscr{B}_E$ et $b \in F$ de coordonnées $(b_1, \ldots, b_p)$ dans $\mathscr{B}_F$.<br>
Résoudre le système revient ici à trouver les antécédents de $b$ par $u$. On notera que le rang du système est aussi égal au rang de l'application linéaire $u$.</p>
<p>Avec les notations précédentes, en notant $\varphi_1, \ldots, \varphi_p$ les formes linéaires sur $E$ de matrices respectives les vecteurs lignes de $A$ (c'est-à-dire $\varphi_i : x \mapsto \sum_{j=1}^{q} a_{ij} x_j$), le système $(L)$ s'écrit :
$$\begin{cases}
\varphi_1(x) = b_1 \\
\vdots \\
\varphi_p(x) = b_p
\end{cases}
$$
Résoudre le système revient ici à chercher l'intersection de $p$ hyperplans affines. On notera que le rang du système est aussi égal au rang de la famille $(\varphi_1, \ldots, \varphi_p)$ dans l'espace dual $E^*$ (car $\operatorname{rg} A = \operatorname{rg} ^t A$).<br>
Cas particulier : dans le cas d'un système homogène, qui s'écrit donc $\forall i \in 〚 1 ; p 〛, \varphi_i(x) = 0$, l'ensemble des solutions est alors égal à $\bigcap_{i=1}^{p} \operatorname{Ker} \varphi_i$; c'est l'intersection de $p$ hyperplans de $E$, et c'est un espace de dimension $q - r$, où $r$ est le rang du système (d'après le théorème du rang, puisque cet espace est aussi égal à $\operatorname{Ker} u$).</p>
<h3>Systèmes de Cramer</h3>
<p>Un système linéaire $(S)$ est dit de Cramer si sa matrice associée est inversible. Tout système de Cramer admet une solution unique.<br>
Soit un système de Cramer de taille $n$ écrit sous la forme : $\sum_{j=1}^{n} x_j C_j = B$.<br>
On a alors, pour tout $i \in 〚 1 ; n 〛$ :
$$\begin{aligned}
\det(C_1, \ldots, C_{i-1}, B, C_{i+1}, \ldots, C_n) &= \sum_{j=1}^{n} x_j \det(C_1, \ldots, C_{i-1}, C_j, C_{i+1}, \ldots, C_n) \\
&= x_i \det(C_1, \ldots, C_{i-1}, C_i, C_{i+1}, \ldots, C_n) \\
&= x_i \det A.
\end{aligned}
$$
On a donc les <em>formules de Cramer</em> :
$$\forall i \in 〚 1 ; n 〛, x_i = \frac{\det A_i(B)}{\det A},
$$
où $A_i(B)$ est la matrice obtenue en remplaçant la $i^e$ colonne de $A$ par $B$.</p>

<!-- Image omitted: image0010062 -->
<!-- Page 58 -->
<ul>
<li><strong>Structure de l'ensemble des solutions</strong><br>
On note $\mathscr{S}$ l'ensemble des solutions de $(S)$. On dit que $(S)$ est compatible si $\mathscr{S} \neq \emptyset$, et incompatible sinon.<br>
L'ensemble des solutions du système homogène associé à $(S)$ est $\operatorname{Ker} A = \{X \in \mathcal{M}_{q, 1}(\mathbb{K}) \mid AX = 0\}$. Si $(S)$ est compatible, alors il existe $X_0 \in \mathcal{M}_{q, 1}(\mathbb{K})$ tel que $AX_0 = B$; $X_0$ s'appelle une solution particulière du système. L'ensemble des solutions du système est alors le sous-espace affine passant par $X_0$ et de direction $\operatorname{Ker} A$ :
$$\mathscr{S} = X_0 + \operatorname{Ker} A = \{X_0 + X \mid X \in \operatorname{Ker} A\}
$$</li>
<li><strong>Résolution dans le cas général</strong>
<ul>
<li>Résolution avec les formules de Cramer<br>
Soit un système de $p$ équations à $q$ inconnues $AX = B$, et $r$ le rang de ce système $(r \leq p$ et $r \leq q)$.<br>
On appelle matrice principale du système toute matrice carrée inversible d'ordre $r$ extraite de $A$ (on sait qu'il en existe).<br>
Si $A_r$ est la matrice principale choisie, les inconnues dont les indices sont ceux des colonnes correspondantes de $A_r$ sont appelées inconnues principales, et les équations dont les indices sont ceux des lignes de $A_r$ sont appelées équations principales.<br>
Le système est compatible si et seulement si les $p - r$ équations secondaires sont combinaisons linéaires des $r$ équations principales; dans ce cas, le système équivaut à celui formé par ces $r$ équations principales; en résolvant un système de Cramer de matrice $A_r$, on peut alors exprimer les $r$ inconnues principales en fonction des $q - r$ inconnues secondaires.</li>
<li>Résolution par la méthode du pivot<br>
Pour résoudre le système $AX = B$, on peut utiliser des opérations élémentaires sur les lignes de la matrice complète du système $[A \quad B]$ pour obtenir un système équivalent dont la matrice associée $A'$ est échelonnée par lignes.<br>
Les inconnues principales sont alors celles correspondant aux pivots de $A'$. Le système sera compatible si et seulement si les $q - r$ dernières équations s'écrivent «$0 = 0$». Dans ce cas, le nouveau système obtenu permettra d'exprimer facilement les inconnues principales en fonction des inconnues secondaires.</li>
</ul>
</li>
<li>Le choix des inconnues principales et secondaires est souvent arbitraire. Cependant leur nombre est toujours le même : si $r$ est le rang du système, il y a $r$ inconnues principales et $q - r = \dim (\operatorname{Ker} A)$ inconnues secondaires.</li>
</ul>

<!-- Image omitted: image0010063 -->
<!-- Page 59 -->
<h1>Thème 4 - Réduction des endomorphismes</h1>
<p>Réduire un endomorphisme $u \in \mathscr{L}(E)$ consiste, dans le cas le plus général, à trouver une famille $(E_i)_{1 \leqslant i \leqslant p}$ de sous-espaces vectoriels de $E$, non réduits à $\{0\}$, stables par $u$, tels que $E = \bigoplus_{i=1}^p E_i$.<br>
En dimension finie, cela revient à trouver une base $\mathscr{B}$ de $E$Où la matrice de $u$ est diagonale par blocs :
$$M_{\mathscr{B}}(u) = \begin{pmatrix}
\boxed{A_1} & & & 0 \\
& \boxed{A_2} & & \\
& & \ddots & \\
0 & & & \boxed{A_p}
\end{pmatrix}
$$
chaque bloc $A_i$ étant la matrice dans une base de $E_i$ de l'endomorphisme $u_i$ induit par $u$ sur $E_i$ (cf. [S3.8]).<br>
Le cas le plus simple est celui où les $E_i$ sont de dimension 1, ce qui revient à chercher les droites vectorielles stables par $u$.</p>
<h2>[S4.1] Éléments propres d'un endomorphisme</h2>
<ul>
<li>Soit $u$ un endomorphisme d'un $\mathbb{K}$-espace vectoriel $E$.
<ul>
<li>On dit que $\lambda \in \mathbb{K}$ est une valeur propre de $u$ s'il existe un vecteur $x \in E$, $x \neq 0_E$, tel que $u(x) = \lambda x$.</li>
<li>On dit que $x \in E$ est un vecteur propre de $u$ si $x \neq 0_E$ et s'il existe $\lambda \in \mathbb{K}$ tel que $u(x) = \lambda x$.</li>
<li>On dit alors que $x$ est <em>un vecteur propre associé</em> à la valeur propre $\lambda$, ou que $\lambda$ est <em>la valeur propre associée</em> au vecteur propre $x$.</li>
<li>Cela équivaut à dire que la droite vectorielle $\mathbb{K}.x$ est stable par $u$.</li>
<li>On appelle spectre de $u$, noté $\text{Sp}_{\mathbb{K}}(u)$Ou plus simplement $\text{Sp}(u)$, l'ensemble des valeurs propres de $u$ (dans $\mathbb{K}$).</li>
</ul>
</li>
<li>Il est très important de bien connaître et comprendre les équivalences suivantes :
$$\lambda \in \text{Sp}(u) \Longleftrightarrow \exists x \neq 0 \text{ tel que } u(x) = \lambda x \\
\Longleftrightarrow \exists x \neq 0 \text{ tel que } (u - \lambda \text{Id}_E)(x) = 0 \\
\Longleftrightarrow \text{Ker}(u - \lambda \text{Id}_E) \neq \{0\} \\
\Longleftrightarrow (u - \lambda \text{Id}_E) \text{ non injective.}
$$</li>
<li>En particulier : $0$ est valeur propre de $u \Longleftrightarrow u$ non injectif.</li>
<li>Si $\lambda \in \text{Sp}(u)$, on appelle sous-espace propre de $u$ associé à la valeur propre $\lambda$ le sous-espace vectoriel :
$$E_\lambda(u) = \text{Ker}(u - \lambda \text{Id}_E).
$$</li>
</ul>

<!-- Image omitted: image0010064 -->
<!-- Page 60 -->
<p>Ainsi, $E_{\lambda}(u) = \{x \in E \mid u(x) = \lambda x\}$ est constitué des vecteurs propres de $u$ associés à la valeur propre $\lambda$ et du vecteur nul.<br>
$\checkmark$On rappelle que le vecteur nul n’est <em>pas</em> un vecteur propre !</p>
<ul>
<li>La propriété suivante, qui découle de la définition, est très souvent utilisée.<br>
Le sous-espace propre associé à la valeur propre 0 est le noyau de $u$.</li>
</ul>
<p>On a aussi une propriété importante concernant les valeurs propres non nulles.<br>
Les sous-espaces propres de $u$ associés à une valeur propre non nulle sont inclus dans $\text{Im } u$.<br>
(En effet, si $x$ est un vecteur propre associé à la valeur propre $\lambda \neq 0$On a $u(x) = \lambda x$ donc $x = u \left( \frac{x}{\lambda} \right) \in \text{Im } u$).</p>
<h2>[S4.2] Propriétés des sous-espaces propres</h2>
<p>$u$ désigne toujours ici un endomorphisme d’un espace vectoriel $E$.</p>
<ul>
<li>Si $\lambda$ est une valeur propre de $u$, $E_{\lambda}(u)$ est stable par $u$, et l’endomorphisme induit par $u$ sur $E_{\lambda}(u)$ est l’homothétie de rapport $\lambda$.</li>
<li>Si $v \in \mathscr{L}(E)$ commute avec $u$, $E_{\lambda}(u)$ est stable par $v$.</li>
<li>Soit $p \in \mathbb{N}^*$ et $(\lambda_i)_{1 \leqslant i \leqslant p}$ une famille de valeurs propres de $u$ <em>deux à deux distinctes</em>.<br>
Alors les sous-espaces propres $E_{\lambda_i}(u)$ pour $1 \leqslant i \leqslant p$ sont en somme directe.</li>
<li>Soit $(\lambda_i)_{1 \leqslant i \leqslant p}$ une famille de valeurs propres de $u$ <em>deux à deux distinctes</em> et, pour tout $i \in [1 ; p]$, $x_i$ un vecteur propre associé à la valeur propre $\lambda_i$.<br>
Alors la famille $(x_i)_{1 \leqslant i \leqslant p}$ est <em>libre</em>.</li>
<li>On en déduit immédiatement que, dans un espace vectoriel de dimension $n$, un endomorphisme possède au plus $n$ valeurs propres distinctes.</li>
</ul>
<h2>[S4.3] Éléments propres d’une matrice</h2>
<ul>
<li>Soit $A$ une matrice de $\mathcal{M}_n(\mathbb{K})$.
<ul>
<li>On dit que $\lambda \in \mathbb{K}$ est une valeur propre de $A$ s’il existe un vecteur colonne $V \in \mathcal{M}_{n,1}(\mathbb{K})$, $V \neq 0$, tel que $AV = \lambda V$.</li>
<li>On dit alors que le vecteur colonne $V \in \mathcal{M}_{n,1}(\mathbb{K})$ est un vecteur propre de $A$ associé à la valeur propre $\lambda$.</li>
<li>On appelle spectre de $A$, noté $\text{Sp}_{\mathbb{K}}(A)$Ou plus simplement $\text{Sp}(A)$, l’ensemble des valeurs propres de $A$ (dans $\mathbb{K}$).</li>
<li>Si $\lambda \in \text{Sp}(A)$, on appelle sous-espace propre de $A$ associé à $\lambda$ l’ensemble $E_{\lambda}(A) = \{V \in \mathcal{M}_{n,1}(\mathbb{K}) \mid AV = \lambda V\}$. C’est évidemment un sous-espace vectoriel de $\mathcal{M}_{n,1}(\mathbb{K})$.</li>
</ul>
$\checkmark$ Lorsque $A$ est à coefficients réels, il est important de distinguer ses valeurs propres réelles et ses valeurs propres complexes, c’est-à-dire les ensembles $\text{Sp}_{\mathbb{R}}(A)$ et $\text{Sp}_{\mathbb{C}}(A)$. Dans le premier cas, les vecteurs propres seront des</li>
</ul>

<!-- Image omitted: image0010065 -->
<!-- Page 61 -->
<p>vecteurs colonnes à chercher dans $\mathcal{M}_{n,1}(\mathbb{R})$ et dans le second cas dans $\mathcal{M}_{n,1}(\mathbb{C})$.<br>
De plus, si $\lambda$ est une valeur propre complexe de la matrice $A$ à coefficients réels, alors $\bar{\lambda}$ est encore une valeur propre de $A$ et
$$V \in E_{\lambda}(A) \Longleftrightarrow \bar{V} \in E_{\bar{\lambda}}(A).$$</p>
<h3>Lien entre éléments propres d'une matrice et d'un endomorphisme</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n \in \mathbb{N}^*$ et $\mathcal{B}$ une base de $E$. Soit $u$ un endomorphisme de $E$ et $A$ la matrice de $u$ dans la base $\mathcal{B}$.</p>
<ul>
<li>$\lambda \in \mathbb{K}$ est valeur propre de $A \Longleftrightarrow \lambda$ est valeur propre de $u$.</li>
<li>Soit $x \in E$ et $V$ la matrice colonne de ses coordonnées dans $\mathcal{B}$.
$$x \in E_{\lambda}(u) \Longleftrightarrow V \in E_{\lambda}(A).$$</li>
</ul>
<h2>[S4.4] Polynôme caractéristique</h2>
<p>$E$ désigne ici un $\mathbb{K}$-espace vectoriel de dimension $n \in \mathbb{N}^*$.</p>
<ul>
<li>Soit $u \in \mathcal{L}(E)$ et $\lambda \in \mathbb{K}$. Par définition d'une valeur propre on a :
$$\lambda \in \text{Sp}(u) \Longleftrightarrow \det(\lambda \text{Id}_E - u) = 0.$$
Soit $\mathcal{B}$ une base de $E$, $A = (a_{ij})$ la matrice de $u$ dans $\mathcal{B}$. Alors :
$$\det(\lambda \text{Id}_E - u) = \det(\lambda I_n - A) = \begin{vmatrix}
\lambda - a_{11} & \cdots & \cdots & -a_{1n} \\
-a_{21} & \lambda - a_{22} & \cdots & -a_{2n} \\
\vdots & \cdots & \ddots & \vdots \\
-a_{n1} & \cdots & \cdots & \lambda - a_{nn}
\end{vmatrix}.$$
D'après la formule donnant la valeur du déterminant d'une matrice, il apparaît clairement que $\det(\lambda \text{Id}_E - u)$ est une fonction polynôme en $\lambda$, de degré $n$. On appelle polynôme caractéristique de $u$ le polynôme, noté $X_u$, défini par :
$$\forall \lambda \in \mathbb{K}, X_u(\lambda) = \det(\lambda \text{Id}_E - u) \quad \text{ou} \quad X_u = \det(X \text{Id}_E - u).$$
Si $\mathcal{B}$ est une base quelconque de $E$, et si $A = M_{\mathcal{B}}(u)$, on a aussi :
$$\forall \lambda \in \mathbb{K}, X_u(\lambda) = \det(\lambda I_n - A) \quad \text{ou} \quad X_u = \det(X I_n - A).$$
Le polynôme $X_A = \det(X I_n - A)$ s'appelle naturellement le polynôme caractéristique de la matrice $A$.</li>
<li>Les valeurs propres d'un endomorphisme de $E$ (ou d'une matrice carrée) sont exactement les racines dans $\mathbb{K}$ de son polynôme caractéristique.<br>
$\checkmark$ Comme il a déjà été dit, lorsque $A$ est une matrice à coefficients réels, il convient de distinguer ses valeurs propres dans $\mathbb{R}$ ou dans $\mathbb{C}$, c'est-à-dire les racines de $X_A$ dans $\mathbb{R}$ ou dans $\mathbb{C}$.</li>
<li>Deux matrices semblables ont même polynôme caractéristique (donc mêmes valeurs propres).</li>
</ul>

<!-- Image omitted: image0010066 -->
<!-- Page 62 -->
<ul>
<li>Le polynôme caractéristique d'une matrice est égal à celui de sa transposée. Donc pour toute matrice $A \in \mathcal{M}_n(\mathbb{K})$, $\text{Sp } A = \text{Sp } ^tA$.</li>
<li>Le polynôme caractéristique de $u$ est un polynôme unitaire de degré $n$. Plus précisément, il s'écrit :
$$\chi_u = X^n - (\text{tr } u)X^{n-1} + \cdots + (-1)^n \det u .
$$</li>
<li>Si le polynôme caractéristique $\chi_u$ de $u$ est scindé (ce qui est toujours le cas si $\mathbb{K} = \mathbb{C}$), on peut écrire :
$$\chi_u = \prod_{i=1}^n (X - \lambda_i),
$$
où les $\lambda_i$ sont les valeurs propres, distinctes ou non, de $u$. À l'aide des relations coefficients-racines on obtient alors les relations (très importantes) :
$$\text{tr } u = \lambda_1 + \cdots + \lambda_n \quad ; \quad \det u = \lambda_1 \lambda_2 \cdots \lambda_n.
$$
✓ On prendra soin cependant de $n$ utiliser ces relations que lorsque le polynôme caractéristique de $u$ est scindé. Ainsi, dans le cas d'une matrice à coefficients réels, il faut considérer toutes les valeurs propres dans $\mathbb{C}$.</li>
<li><strong>Ordre de multiplicité d'une valeur propre</strong><br>
Soit $u \in \mathscr{L}(E)$ et soit $\lambda \in \text{Sp}_{\mathbb{K}}(u)$.<br>
On dit que $\lambda$ est valeur propre d'ordre de multiplicité $m_\lambda$ de $u$ si $\lambda$ est une racine d'ordre $m_\lambda$ du polynôme caractéristique $\chi_u$ de $u$.<br>
On a alors :
$$1 \leq \dim E_\lambda(u) \leq m_\lambda.
$$</li>
</ul>
<h2>[S4.5] Endomorphismes ou matrices diagonalisables</h2>
<ul>
<li>Un endomorphisme $u$ d'un $\mathbb{K}$-espace vectoriel $E$ de dimension finie est dit diagonalisable s'il existe une base de $E$ formée de vecteurs propres de $u$. Cela équivaut à dire qu'il existe une base $\mathscr{B}$ de $E$ dans laquelle la matrice de $u$ est diagonale; $\mathscr{B}$ est formée de vecteurs propres de $u$, et les valeurs propres de $u$ (éventuellement confondues) sont les éléments de la diagonale de $M_{\mathscr{B}}(u)$.</li>
<li><strong>Diagonalisabilité et sous-espaces propres</strong><br>
Un endomorphisme $u$ est diagonalisable si et seulement si il vérifie l'une des propriétés (équivalentes) suivantes :
<ol type="i">
<li>$E$ est somme (directe) des sous-espaces propres de $u$ : $E = \bigoplus_{\lambda \in \text{Sp}(u)} E_\lambda(u)$,</li>
<li>$\dim E = \sum_{\lambda \in \text{Sp}(u)} \dim (E_\lambda(u))$,</li>
<li>$\chi_u$ est scindé dans $\mathbb{K}[X]$ et, pour tout $\lambda \in \text{Sp}(u)$, on a : $m_\lambda = \dim E_\lambda(u)$ (en notant $m_\lambda$ l'ordre de multiplicité de la valeur propre $\lambda$).</li>
</ol>
</li>
</ul>

<!-- Image omitted: image0010067 -->
<!-- Page 63 -->
<p>Cas particulier : si $u$ est un endomorphisme d'un $\mathbb{K}$-espace vectoriel de dimension $n$ qui possède $n$ valeurs propres distinctes, alors $u$ est diagonalisable (et, dans ce cas, tous ses sous-espaces propres sont de dimension 1).<br>
$\checkmark$ Attention ! il ne s'agit là que d'une condition <em>suffisante</em>, et non nécessaire, de diagonalisabilité.<br>
Considérer par exemple $u = \text{Id}_E$ : $u$ est diagonalisable, mais $u$ admet une seule valeur propre, 1.</p>
<h2>[S4.6] Polynômes d'endomorphismes</h2>
<ul>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel, et $u \in \mathscr{L}(E)$.<br>
À tout polynôme $P = \sum_{k=0}^{d^\circ(P)} a_k X^k$ de $\mathbb{K}[X]$On peut associer l'endomorphisme de $E$ : $P(u) = \sum_{k=0}^{d^\circ(P)} a_k u^k$ (où $u^k$ désigne, comme d'habitude, l'endomorphisme défini par : $u^0 = \text{Id}_E$ et, pour tout $k \geq 1$ : $u^k = u \circ u^{k-1}$).<br>
$\checkmark$ Si $x$ est un vecteur de $E$, l'écriture $P(u)(x)$ a un sens (c'est l'image de $x$ par l'endomorphisme $P(u)$) mais l'écriture $P(u(x))$ ne veut rien dire !</li>
<li>$u$ étant un endomorphisme donné de $E$, l'application $\varphi_u : \begin{cases} \mathbb{K}[X] & \to \mathscr{L}(E) \\ P & \mapsto P(u) \end{cases}$ est un morphisme de $\mathbb{K}$-algèbres, c'est-à-dire que, pour $P, Q \in \mathbb{K}[X]$ et $\lambda \in \mathbb{K}$ :
<ul>
<li>si $P$ est le polynôme constant égal à 1, $P(u) = \text{Id}_E$;</li>
<li>$(P + Q)(u) = P(u) + Q(u)$;</li>
<li>$(\lambda P)(u) = \lambda P(u)$;</li>
<li>$(PQ)(u) = P(u) \circ Q(u)$.</li>
</ul>
On note $\mathbb{K}[u] = \text{Im} \varphi_u = \{P(u) \mid P \in \mathbb{K}[X]\}$ l'algèbre des polynômes de l'endomorphisme $u$; c'est une sous-algèbre commutative de $\mathscr{L}(E)$ : si $P, Q \in \mathbb{K}[X]$, les endomorphismes $P(u)$ et $Q(u)$ commutent.</li>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel, et $u, v \in \mathscr{L}(E)$, tels que $u \circ v = v \circ u$. Alors pour tous $P, Q \in \mathbb{K}[X]$, $\text{Im}(P(u))$ et $\text{Ker}(P(u))$ sont stables par $Q(v)$.</li>
<li>Soit $F$ un sous-espace vectoriel de $E$, stable par $u \in \mathscr{L}(E)$ et soit $P \in \mathbb{K}[X]$. Alors $F$ est stable par $P(u)$.</li>
<li>Si $\lambda$ est une valeur propre de $u$ et si $P \in \mathbb{K}[X]$, alors $P(\lambda)$ est une valeur propre de $P(u)$.</li>
</ul>
<h2>[S4.7] Polynômes annulateurs, polynôme minimal</h2>
<ul>
<li>Soit $u \in \mathscr{L}(E)$. Un polynôme $P \in \mathbb{K}[X]$ s'appelle un polynôme annulateur de $u$ si $P(u) = 0$ (endomorphisme nul).</li>
</ul>

<!-- Image omitted: image0010068 -->
<!-- Page 64 -->
<p>L'ensemble des polynômes annulateurs de $u$ est donc le noyau du morphisme d'algèbres $\varphi_u : \begin{cases} \mathbb{K}[X] \rightarrow \mathscr{L}(E) \\ P \mapsto P(u) \end{cases}$, c'est un idéal de $\mathbb{K}[X]$, appelé idéal annulateur de $u$ et noté $\text{Ann}(u)$.<br>
Puisque l'anneau $\mathbb{K}[X]$ est principal,</p>
<ul>
<li>soit $\text{Ann}(u) = \{0\}$ (et alors $\varphi_u$ est injective);</li>
<li>soit il existe un et un seul polynôme unitaire (non nul), noté $\pi_u$, tel que $\text{Ann}(u)$ soit exactement l'ensemble des polynômes multiples de $\pi_u$.</li>
</ul>
<p>$\pi_u$ s'appelle alors le polynôme minimal de $u$.<br>
Le polynôme minimal d'un endomorphisme $u$ (s'il existe) est donc caractérisé par :
$$\begin{cases}
\pi_u \neq 0 \text{ et } \pi_u \text{ unitaire;} \\
\pi_u(u) = 0_{\mathscr{L}(E)}; \\
\forall P \in \mathbb{K}[X], P(u) = 0_{\mathscr{L}(E)} \implies P \text{ multiple de } \pi_u.
\end{cases}
$$</p>
<ul>
<li>Si $E$ est un $\mathbb{K}$-espace vectoriel de dimension finie, tout endomorphisme $u \in \mathscr{L}(E)$ possède un polynôme minimal.<br>
$\checkmark$ Lorsque $E$ n'est pas de dimension finie, un endomorphisme peut ne pas posséder de polynôme annulateur autre que le polynôme nul (considérer par exemple l'endomorphisme $u$ de $\mathbb{K}[X]$ qui à tout polynôme $P$ associe son polynôme dérivé $P'$).</li>
<li><strong>Deux exemples importants d'utilisation d'un polynôme annulateur</strong><br>
Soit $u \in \mathscr{L}(E)$, possédant un polynôme annulateur (non nul)$P$.
<ul>
<li>Calcul de l'inverse<br>
Si $P$ s'écrit $P = \sum_{k=0}^{n} a_k X^k$ avec $a_0 \neq 0$, la relation $P(u) = 0$ s'écrit aussi :
$$u \circ \left( \sum_{k=1}^{n} a_k u^{k-1} \right) = -a_0 \text{Id}_E,
$$
ce qui prouve que $u$ est inversible et permet d'exprimer $u^{-1}$ à l'aide des puissances de $u$.</li>
<li>Calcul des puissances<br>
Pour tout $n \in \mathbb{N}$, la division euclidienne de $X^n$ par $P$ s'écrit : $X^n = PQ_n + R_n$ avec $\deg(R_n) < \deg(P)$.<br>
On a alors : $u^n = P(u) \circ Q_n(u) + R_n(u) = R_n(u)$, ce qui permet d'exprimer $u^n$ à l'aide des premières puissances de $u$.</li>
</ul>
</li>
</ul>
<h2>[S4.8] Polynômes de matrices</h2>
<ul>
<li>Soit $M \in \mathcal{M}_n(\mathbb{K})$. À tout polynôme $P = \sum_{k=0}^{d^o(P)} a_k X^k$ de $\mathbb{K}[X]$, on peut associer la matrice $P(M) = \sum_{k=0}^{d^o(P)} a_k M^k$ (avec $M^0 = I_n$).</li>
</ul>

<!-- Image omitted: image0010069 -->
<!-- Page 65 -->
<p>Il est clair que, si $M = M_{\mathscr{B}}(u)$, où $u$ est un endomorphisme d'un $\mathbb{K}$-espace vectoriel $E$ de dimension $n$ rapporté à une base $\mathscr{B}$, on aura $P(M) = M_{\mathscr{B}}(P(u))$. On dira qu'un polynôme $P$ est annulateur de $M$ si $P(M) = 0$. Cela équivaut à dire que $P$ est annulateur de $u$.<br>
Puisque $u$, endomorphisme d'un espace vectoriel de dimension finie, possède un polynôme annulateur non nul, on en déduit que $M$ possède un polynôme annulateur non nul. On peut donc définir le polynôme minimal $\pi_M$ de $M$, qui est le même que celui de $u$.</p>
<ul>
<li>Si $M$ est une matrice à coefficients réels, son polynôme minimal en tant que matrice de $\mathcal{M}_n(\mathbb{R})$ est le même que son polynôme minimal en tant que matrice de $\mathcal{M}_n(\mathbb{C})$.</li>
<li>Si deux matrices carrées $A$ et $A'$ sont semblables et si $P$ est un polynôme annulateur de $A$, $P$ est aussi un polynôme annulateur de $A'$. En particulier, $A$ et $A'$Ont même polynôme minimal.</li>
<li>Si $A \in \mathcal{M}_n(\mathbb{K})$ et si $P$ est un polynôme annulateur de $A$, $P$ est aussi un polynôme annulateur de $^tA$. En particulier, $A$ et $^tA$Ont même polynôme minimal.</li>
</ul>
<h2>[S4.9] Théorème de décomposition des noyaux</h2>
<p>La plupart d'entre vous ne connaissent ce théorème que sous sa forme finale (la plus utilisée il est vrai), mais pour l'obtenir, il existe certains théorèmes intermédiaires non dénués d'intérêt propre que nous allons citer.</p>
<ul>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel, $u$ un endomorphisme de $E$, et $P, Q$ deux polynômes de $\mathbb{K}[X]$.<br>
Si l'on note $D = \operatorname{pgcd}(P, Q)$ et $M = \operatorname{ppcm}(P, Q)$, on a :
<ol type="i">
<li>$\operatorname{Ker} M(u) = \operatorname{Ker} P(u) + \operatorname{Ker} Q(u)$,</li>
<li>$\operatorname{Ker} D(u) = \operatorname{Ker} P(u) \cap \operatorname{Ker} Q(u)$.</li>
</ol>
</li>
<li>Si $P$ et $Q$ sont deux polynômes de $\mathbb{K}[X]$ premiers entre eux, alors :
$$\operatorname{Ker}(PQ)(u) = \operatorname{Ker} P(u) \oplus \operatorname{Ker} Q(u).$$</li>
<li>Si $P_1, \ldots, P_n$ sont $n$ polynômes de $\mathbb{K}[X]$ premiers entre eux deux à deux, alors :
$$\operatorname{Ker} \left( \prod_{k=1}^n P_k \right)(u) = \bigoplus_{k=1}^n \operatorname{Ker} P_k(u).$$</li>
<li>Si $P$ est un polynôme annulateur de $u$, et si $P = \lambda \prod_{k=1}^n P_k^{m_k}$ est sa décomposition en facteurs irréductibles de $\mathbb{K}[X]$ ($P_k$ polynômes irréductibles unitaires et distincts deux à deux, $m_k$ entiers naturels non nuls), alors :
$$E = \bigoplus_{k=1}^n \operatorname{Ker} P_k^{m_k}(u).$$</li>
</ul>

<!-- Image omitted: image0010070 -->
<!-- Page 66 -->
<h2>[S4.10] Polynômes annulateurs et réduction</h2>
<ul>
<li>Soit $u \in \mathscr{L}(E)$ et $P$ un polynôme annulateur non nul de $u$. Alors toute valeur propre (dans $\mathbb{K}$) de $u$ est une racine de $P$ (dans $\mathbb{K}$). Autrement dit : $\operatorname{Sp}(u)$ est <em>inclus</em> dans l'ensemble des racines de $P$.<br>
$\checkmark$ Attention : si $P$ est un polynôme annulateur quelconque de $u$, toutes ses racines ne sont pas nécessairement valeurs propres de $u$ !</li>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie, et $u \in \mathscr{L}(E)$. Le spectre de $u$ dans $\mathbb{K}$ est exactement l'ensemble des racines dans $\mathbb{K}$ du polynôme minimal $\pi_u$.</li>
<li>Un endomorphisme $u$ d'un $\mathbb{K}$-espace vectoriel de dimension finie est diagonalisable si et seulement si il possède un polynôme annulateur scindé dans $\mathbb{K}[X]$ et à racines simples.</li>
<li>Un endomorphisme $u$ d'un $\mathbb{K}$-espace vectoriel de dimension finie est diagonalisable si et seulement si son polynôme minimal est scindé dans $\mathbb{K}[X]$ et à racines simples.</li>
<li><strong>Théorème de Cayley-Hamilton</strong><br>
Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie, et $u$ un endomorphisme de $E$. Alors : $\chi_u(u) = 0_{\mathscr{L}(E)}$.<br>
Autrement dit, le polynôme caractéristique de $u$ est aussi un polynôme annulateur de $u$, ou : le polynôme minimal de $u$ divise son polynôme caractéristique.</li>
</ul>
<h2>[S4.11] Sous-espaces stables</h2>
<ul>
<li>Soit $u$ un endomorphisme d'un $\mathbb{K}$-espace vectoriel de dimension finie $E$. Soit $F$ un sous-espace vectoriel de $E$ <em>stable</em> par $u$, et $v \in \mathscr{L}(F)$ l'endomorphisme de $F$ induit par $u$.
<ul>
<li>Le polynôme caractéristique de $v$ divise le polynôme caractéristique de $u$. En particulier on a :
$$
\operatorname{Sp}(v) \subset \operatorname{Sp}(u) \quad \text{et} \quad \forall \lambda \in \operatorname{Sp}(v), E_\lambda(v) = E_\lambda(u) \cap F.
$$</li>
<li>Le polynôme minimal de $v$ divise le polynôme minimal de $u$.</li>
</ul>
</li>
<li><strong>Diagonalisabilité et sous-espaces stables</strong><br>
Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie, et $u \in \mathscr{L}(E)$.
<ul>
<li>Si $F$ est un sous-espace vectoriel de $E$ <em>stable</em> par $u$, et si $v \in \mathscr{L}(F)$ est l'endomorphisme de $F$ induit par $u$, alors :
$$
u \text{ diagonalisable } \Longrightarrow v \text{ diagonalisable}.
$$</li>
<li>Lorsque $u$ est diagonalisable, un sous-espace vectoriel $F$ de $E$ est stable par $u$ si et seulement si $F$ admet une base formée de vecteurs propres de $u$.</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010071 -->
<!-- Page 67 -->
<p>$\checkmark$ Ce résultat tombe en défaut lorsque $u$ n'est pas diagonalisable : par exemple, si $u$ est une rotation d'un espace euclidien de dimension 3, différente de $\pm \text{Id}_E$, le plan orthogonal à l'axe de cette rotation est stable par $u$ mais ne contient aucun vecteur propre de $u$.</p>
<ul>
<li>Dans le cas d'un endomorphisme non diagonalisable mais dont le polynôme caractéristique est scindé, voir [S4.14].</li>
</ul>
<h2>[S4.12] Trigonalisation</h2>
<p>$E$ désigne ici un $\mathbb{K}$-espace vectoriel de dimension finie non nulle.</p>
<ul>
<li>Un endomorphisme $u \in \mathscr{L}(E)$ est dit trigonalisable s'il existe une base $\mathscr{B}$ de $E$ telle que $M_{\mathscr{B}}(u)$ soit triangulaire supérieure.<br>
Une matrice carrée $A \in \mathcal{M}(\mathbb{K})$ est dite trigonalisable si l'endomorphisme $a$ de $\mathbb{K}^n$ qui lui est canoniquement associé est trigonalisable.<br>
Cela revient à dire que $A$ est semblable à une matrice triangulaire supérieure, c'est-à-dire qu'il existe $P \in \text{GL}_n(\mathbb{K})$ et $T \in \mathcal{T}_n^+(\mathbb{K})$ telles que : $T = P^{-1}AP$ (ou $A = PTP^{-1}$).</li>
<li>Un endomorphisme $u \in \mathscr{L}(E)$ est trigonalisable si et seulement si il possède un polynôme annulateur scindé dans $\mathbb{K}[X]$ (ce qui équivaut à dire que $\chi_u$ est scindé dans $\mathbb{K}[X]$).<br>
En particulier, si $\mathbb{K} = \mathbb{C}$, tout endomorphisme de $E$ est trigonalisable, et toute matrice de $\mathcal{M}_n(\mathbb{C})$ est trigonalisable.<br>
$\checkmark$ Si $u \in \mathscr{L}(E)$ est trigonalisable et si $\mathscr{B}$ est une base de $E$ dans laquelle la matrice $T$ de $u$ est triangulaire supérieure, alors les éléments diagonaux de $T$ sont exactement les valeurs propres de $u$ (chacune étant comptée avec son ordre de multiplicité).</li>
</ul>
<h2>[S4.13] Endomorphismes nilpotents</h2>
<ul>
<li>Un endomorphisme $u$ d'un espace vectoriel $E$ est dit nilpotent s'il existe un entier naturel $n$ tel que $u^n = 0$.<br>
On appelle alors indice de nilpotence de $u$ le plus petit entier $p$ tel que $u^p = 0$.</li>
<li>Si $E$ est de dimension finie, et si $u$ est nilpotent d'indice $p$, alors $p \leq \dim E$.</li>
<li>$u$ est un endomorphisme nilpotent d'indice $p$ si et seulement si son polynôme minimal est $\pi_u = X^p$.</li>
<li>Si $u$ est un endomorphisme d'un espace vectoriel de dimension $n$, $u$ est nilpotent si et seulement si son polynôme caractéristique est égal à $X^n$.</li>
<li>Un endomorphisme $u$ d'un $\mathbb{C}$-espace vectoriel de dimension finie est nilpotent si et seulement si sa seule valeur propre est 0.<br>
$\checkmark$ Ce résultat tombe en défaut si $E$ est un $\mathbb{R}$-espace vectoriel ; par exemple, la matrice $\begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & -1 & 0 \end{pmatrix}$ admet pour seule valeur propre réelle 0 mais n'est pas nilpotente.</li>
</ul>

<!-- Image omitted: image0010072 -->
<!-- Page 68 -->
<ul>
<li>Si $u$ est un endomorphisme d'un espace vectoriel de dimension finie, $u$ est nilpotent si et seulement si il existe une base de $E$ dans laquelle la matrice de $u$ est triangulaire supérieure à éléments diagonaux nuls.</li>
</ul>
<h2>[S4.14] Endomorphismes à polynôme annulateur scindé</h2>
<ul>
<li>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie $n \in \mathbb{N}^*$ et $u \in \mathscr{L}(E)$. Les propositions suivantes sont équivalentes :
<ol type="i">
<li>il existe un polynôme annulateur de $u$ scindé dans $\mathbb{K}[X]$,</li>
<li>le polynôme minimal de $u$ est scindé dans $\mathbb{K}[X]$,</li>
<li>le polynôme caractéristique de $u$ est scindé dans $\mathbb{K}[X]$.</li>
</ol>
</li>
</ul>
<h3>Sous-espaces caractéristiques</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie $n \in \mathbb{N}^*$ et $u \in \mathscr{L}(E)$ annulant un polynôme scindé.<br>
Son polynôme caractéristique s'écrit alors $X_u = \prod_{i=1}^{p} (X - \lambda_i)^{m_i}$, où les $\lambda_i \in \mathbb{K}$ sont les valeurs propres distinctes de $u$ et $m_i$ leurs ordres de multiplicité.<br>
Pour $i \in 〚 1; p 〛$, soit $F_{\lambda_i} = \text{Ker}((u - \lambda_i \text{Id}_E)^{m_i})$ : ce sont les sous-espaces caractéristiques de $u$. Alors :</p>
<ul>
<li>$E = \bigoplus_{i=1}^{p} F_{\lambda_i}$ ;</li>
<li>$\forall i \in 〚 1 ; p 〛$, $\text{dim} F_{\lambda_i} = m_i$ ;</li>
<li>les $F_{\lambda_i}$ sont stables par $u$; si l'on note $u_i$ l'endomorphisme induit par $u$ sur $F_{\lambda_i}$, on a les propriétés suivantes :
<ul>
<li>le polynôme minimal $\pi_{u_i}$ de $u_i$ est égal à $(X - \lambda_i)^{\alpha_i}$, où $1 \leq \alpha_i \leq m_i$; ainsi, $F_{\lambda_i} = \text{Ker}((u - \lambda_i \text{Id}_E)^{\alpha_i})$ et $u_i$ est la somme de l'homothétie de rapport $\lambda_i$ et d'un endomorphisme de $F_{\lambda_i}$ nilpotent d'indice $\alpha_i$;</li>
<li>le polynôme minimal $\pi_u$ de $u$ est : $\pi_u = \prod_{i=1}^{p} \pi_{u_i}$ ;</li>
<li>un sous-espace vectoriel $F$ de $E$ est stable par $u$ si et seulement si son intersection avec chacun des $F_{\lambda_i}$ est stable par $u_i$ et dans ce cas :
$$F = \bigoplus_{i=1}^{p} (F \cap F_{\lambda_i}).$$</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010073 -->
<!-- Page 69 -->
<h1>Thème 5 - Espaces préhilbertiens réels</h1>
<h2>[S5.1] Produit scalaire</h2>
<ul>
<li>Une forme bilinéaire sur un $\mathbb{R}$-espace vectoriel $E$ est une application $\varphi$ de $E \times E$ dans $\mathbb{R}$ linéaire par rapport à chacune des deux variables, c'est-à-dire :
<ul>
<li>pour tout $y \in E$, l'application $x \mapsto \varphi(x, y)$ est une forme linéaire sur $E$;</li>
<li>pour tout $x \in E$, l'application $y \mapsto \varphi(x, y)$ est une forme linéaire sur $E$.</li>
</ul>
La forme bilinéaire $\varphi : E^2 \to \mathbb{R}$ est dite symétrique si :
$$\forall (x, y) \in E^2, \varphi(x, y) = \varphi(y, x).
$$
Si $\varphi$ est une application de $E^2$ dans $\mathbb{R}$ symétrique et linéaire par rapport à l'une des variables, alors $\varphi$ est bilinéaire.</li>
<li>On appelle produit scalaire sur un $\mathbb{R}$-espace vectoriel $E$ une forme bilinéaire symétrique $\varphi$ qui est, de plus :
<ul>
<li>positive, c'est-à-dire : $\forall x \in E, \varphi(x, x) \geq 0$;</li>
<li>et définie, c'est-à-dire : $\forall x \in E, \varphi(x, x) = 0 \implies x = 0_E$.</li>
</ul>
Dire que $\varphi$ est définie positive peut aussi s'écrire directement :
$$\forall x \in E \setminus \{0\}, \varphi(x, x) > 0.
$$
Si $\varphi$ est un produit scalaire sur un $\mathbb{R}$-espace vectoriel $E$, le couple $(E, \varphi)$ s'appelle un espace préhilbertien réel.<br>
Le produit scalaire de deux vecteurs $x$ et $y$ se note en général $\langle x | y \rangle$.</li>
<li><strong>Exemples</strong>
<ul>
<li>Dans $\mathbb{R}^n$, pour $x = (x_1, \ldots, x_n)$ et $y = (y_1, \ldots, y_n)$, on peut poser :
$$\langle x | y \rangle = x_1 y_1 + \cdots + x_n y_n.
$$
Il s'agit du produit scalaire canonique sur $\mathbb{R}^n$.<br>
En notant $X \in \mathcal{M}_{n, 1}(\mathbb{R})$ le vecteur colonne $^t(x_1 \ldots x_n)$ et $Y \in \mathcal{M}_{n, 1}(\mathbb{R})$ le vecteur colonne $^t(y_1 \ldots y_n)$, on a aussi : $\langle x | y \rangle = ^tXY$.<br>
$\checkmark$ Dans l'écriture ci-dessus, on assimile la matrice $^tXY$, de type (1,1), à un nombre réel.</li>
<li>Dans $\mathcal{M}_n(\mathbb{R})$, si $A = (a_{ij})_{1 \leq i, j \leq n}$ et $B = (b_{ij})_{1 \leq i, j \leq n}$, on peut poser :
$$\langle A | B \rangle = \text{tr}(^tAB) = \sum_{1 \leq i, j \leq n} a_{ij} b_{ij}.
$$
Il s'agit du produit scalaire canonique sur $\mathcal{M}_n(\mathbb{R})$.</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010074 -->
<!-- Page 70 -->
<ul>
<li>Dans l'espace vectoriel $\mathscr{C}([a ; b], \mathbb{R})$ des fonctions continues sur le segment $[a ; b]$, on peut poser :
$$\langle f \mid g \rangle = \int_{a}^{b} f(t) g(t) \, dt.
$$
On remarquera que ce n'est pas un produit scalaire sur l'espace vectoriel $\mathscr{C} \mathscr{M}([a ; b] \mathbb{R})$ des fonctions continues par morceaux sur $[a ; b]$.</li>
<li>Soit $I$ un intervalle quelconque de $\mathbb{R}$. On note $\mathscr{L}_{c}^{2}(I, \mathbb{R})$ l'ensemble des fonctions continues et de carré intégrable sur $I$, c'est-à-dire telles que l'intégrale $\int_{I} f^{2}$ converge.<br>
Alors $\mathscr{L}_{c}^{2}(I, \mathbb{R})$ est un sous-espace vectoriel de $\mathscr{C}(I, \mathbb{R})$, qui peut être muni d'un produit scalaire par : $\langle f \mid g \rangle = \int_{I} f g$.</li>
<li>On note $\ell^{2}(\mathbb{N}, \mathbb{R})$ l'ensemble des suites réelles $(u_{n})_{n \in \mathbb{N}}$ telles que la série $\sum_{n \in \mathbb{N}} u_{n}^{2}$ converge (suites de carré sommable). Alors $\ell^{2}(\mathbb{N}, \mathbb{R})$ est un sous-espace vectoriel de $\mathbb{R}^{\mathbb{N}}$, qui peut être muni du produit scalaire :
$$\langle u \mid v \rangle = \sum_{n=0}^{\infty} u_{n} v_{n}.
$$</li>
</ul>
<h2>[S5.2] Norme associée à un produit scalaire</h2>
<ul>
<li>Si $E$ est un espace préhilbertien réel, où le produit scalaire est noté $\langle \cdot \mid \cdot \rangle$, on appelle norme (euclidienne) associée l'application :
$$N : x \in E \mapsto \| x \| = \sqrt{\langle x \mid x \rangle}
$$
(cela a bien un sens car $\langle x \mid x \rangle \geqslant 0$).</li>
</ul>
<h3>Propriétés</h3>
<ul>
<li>$\forall x \in E, \| x \| = 0_{\mathbb{R}} \Longleftrightarrow x = 0_{E}$.</li>
<li>$\forall x \in E, \forall \lambda \in \mathbb{R}, \| \lambda x \| = | \lambda | \| x \|$.</li>
<li>Un vecteur $x$ est dit unitaire si $\| x \| = 1$.<br>
Si $x$ est un vecteur non nul, alors le vecteur $\frac{x}{\| x \|}$ est unitaire.</li>
<li>$\forall (x, y) \in E^{2}, \| x + y \|^{2} = \| x \|^{2} + 2 \langle x \mid y \rangle + \| y \|^{2}$.</li>
<li>$\forall (x, y) \in E^{2}, \langle x + y \mid x - y \rangle = \| x \|^{2} - \| y \|^{2}$.</li>
<li><em>Identité du parallélogramme</em>
$$\forall (x, y) \in E^{2}, \| x + y \|^{2} + \| x - y \|^{2} = 2 \left( \| x \|^{2} + \| y \|^{2} \right).
$$</li>
<!-- Image omitted: Parallelogram identity diagram -->
</ul>

<!-- Image omitted: image0010075 -->
<!-- Page 71 -->
<ul>
<li><strong>Identités de polarisation</strong>
$$\forall (x, y) \in E^2, \quad \begin{cases}
\langle x \mid y \rangle = \frac{1}{4} \left( \|x + y\|^2 - \|x - y\|^2 \right) \\
\langle x \mid y \rangle = \frac{1}{2} \left( \|x + y\|^2 - \|x\|^2 - \|y\|^2 \right)
\end{cases}
$$</li>
<li><strong>Inégalité de Cauchy-Schwarz</strong><br>
Soit $E$ un espace préhilbertien réel. Pour tous $x, y$ dans $E$On a :
$$|\langle x \mid y \rangle| \leq \|x\| \cdot \|y\|
$$
et il y a égalité si et seulement si le système $\{x, y\}$ est lié.<br>
Remarque : pour $f, g \in \mathscr{L}^2(I, \mathbb{R})$ muni du produit scalaire usuel, l'inégalité de Cauchy-Schwarz s'écrit :
$$\left( \int_I fg \right)^2 \leq \left( \int_I f^2 \right) \left( \int_I g^2 \right).
$$
$\checkmark$ L'inégalité de Cauchy-Schwarz implique que le produit scalaire est une application bilinéaire continue sur $E \times E$, lorsque $E$ est muni de la norme associée au produit scalaire (cf. chapitre sur les espaces vectoriels normés).</li>
<li><strong>Inégalité de Minkowski (ou inégalité triangulaire)</strong><br>
Soit $E$ un espace préhilbertien réel. Pour tous $x, y$ dans $E$On a :
$$\|x + y\| \leq \|x\| + \|y\|
$$
et il y a égalité si et seulement si la famille $\{x, y\}$ est positivement liée (c'est-à-dire qu'il existe un réel $\lambda$ positif tel que $x = \lambda y$Ou $y = \lambda x$).<br>
On en déduit :
$$\forall (x, y) \in E^2, \quad \left| \|x\| - \|y\| \right| \leq \|x - y\|.
$$</li>
<li>Soit $E$ un espace préhilbertien réel. La norme associée au produit scalaire sur $E$ munit $E$ d'une structure d'espace vectoriel normé.</li>
</ul>
<h2>[S5.3] Orthogonalité</h2>
<p>Dans toute la suite, $E$ désigne un espace préhilbertien réel.</p>
<ul>
<li><strong>Vecteurs orthogonaux</strong><br>
On dit que deux vecteurs $x$ et $y$ de $E$ sont orthogonaux, et on note $x \perp y$, lorsque $\langle x \mid y \rangle = 0$.<br>
La relation $\langle x \mid y \rangle = 0$ est équivalente à $\langle y \mid x \rangle = 0$. La relation d'orthogonalité est donc symétrique.</li>
<li><strong>Orthogonal d'une partie</strong><br>
Soit $A$ une partie non vide d'un espace préhilbertien $E$.<br>
Un vecteur de $E$ est dit orthogonal à $A$ s'il est orthogonal à tout vecteur de $A$. L'ensemble des vecteurs orthogonaux à $A$ s'appelle l'orthogonal de $A$, noté $A^\perp$. Ainsi :
$$A^\perp = \{ x \in E \mid \forall a \in A, \langle x \mid a \rangle = 0 \}.
$$</li>
<li>Si $x$ est un vecteur non nul de $E$, alors $\{x\}^\perp$ est un hyperplan de $E$. Un supplémentaire de $\{x\}^\perp$ est la droite vectorielle $\mathbb{R} \cdot x$.</li>
<li>Si $A$ est une partie non vide de $E$, alors $A^\perp$ est un sous-espace vectoriel de $E$.</li>
</ul>

<!-- Image omitted: image0010076 -->
<!-- Page 72 -->
<h3>Propriétés de l'orthogonal</h3>
<ul>
<li>$E^{\perp} = \{0\}$ et $\{0\}^{\perp} = E$.</li>
<li>$\forall (A, B) \in \mathcal{P}(E)^2, A \subset B \implies B^{\perp} \subset A^{\perp}$.</li>
<li>$\forall A \in \mathcal{P}(E), A^{\perp} = \text{Vect}(A)^{\perp}$.<br>
Il en résulte que, si $F$ est un sous-espace vectoriel de $E$ et si $(e_i)_{i \in I}$ est une base de $F$, un vecteur $x$ appartient à $F^{\perp}$ si et seulement si pour tout $i \in I$On a $\langle x | e_i \rangle = 0$.</li>
<li>$\forall A \in \mathcal{P}(E), A \subset (A^{\perp})^{\perp}$.<br>
Si $F$ est un sous-espace vectoriel de $E$, on n'a pas nécessairement $F = (F^{\perp})^{\perp}$. Un contre-exemple se trouve en [SF5.7].</li>
<li>Si $F$ et $G$ sont des sous-espaces vectoriels de $E : (F + G)^{\perp} = F^{\perp} \cap G^{\perp}$.</li>
<li>Deux sous-espaces vectoriels $F$ et $G$ d'un espace préhilbertien $E$ sont dits orthogonaux, et on note $F \perp G$, lorsque :
$\forall (x, y) \in F \times G, \langle x | y \rangle = 0$,<br>
autrement dit lorsque tout vecteur de $F$ est orthogonal à tout vecteur de $G$. Cela équivaut à : $F \subset G^{\perp}$Ou $G \subset F^{\perp}$.<br>
Remarque : dans $\mathbb{R}^3$, on pourra ainsi parler d'une droite et d'un plan orthogonaux, mais deux plans $P_1$ et $P_2$ ne peuvent pas être orthogonaux. Cependant, si les droites $P_1^{\perp}$ et $P_2^{\perp}$ sont orthogonales, les plans $P_1$ et $P_2$ sont dits perpendiculaires.</li>
<li>Si $F$ et $G$ sont deux sous-espaces vectoriels orthogonaux de $E$, alors $F \cap G = \{0\}$. Plus généralement, des sous-espaces vectoriels orthogonaux deux à deux sont en somme directe.</li>
</ul>
<h2>[S5.4] Familles orthogonales, orthonormales</h2>
<ul>
<li>Une famille $(x_i)_{i \in I}$ de vecteurs d'un espace préhilbertien réel $E$ est dite orthogonale si : $\forall (i, j) \in I^2, i \neq j \implies \langle x_i | x_j \rangle = 0$.<br>
Elle est dite orthonormale si, de plus, $\|x_i\| = 1$ pour tout $i \in I$, c'est-à-dire si :
$\forall (i, j) \in I^2, \langle x_i | x_j \rangle = \delta_{ij}$.<br>
Si $(x_i)_{i \in I}$ est une famille orthogonale de vecteurs non nuls, alors la famille des vecteurs $\left( \frac{x_i}{\|x_i\|} \right)_{i \in I}$ est orthonormale.</li>
<li><strong>Relation de Pythagore</strong><br>
Si $(x_1, \ldots, x_p)$ est une famille orthogonale d'un espace préhilbertien réel, alors :
$\left\| \sum_{i=1}^p x_i \right\|^2 = \sum_{i=1}^p \|x_i\|^2$.<br>
Cas particulier : le théorème de Pythagore<br>
Soit $E$ un espace préhilbertien réel, et $x, y$ deux vecteurs de $E$. Alors :
$x \perp y \iff \|x + y\|^2 = \|x\|^2 + \|y\|^2$.</li>
<li>Si $(x_i)_{i \in I}$ est une famille orthogonale de vecteurs non nuls, alors cette famille est libre. En particulier, toute famille orthonormale est libre.</li>
</ul>

<!-- Image omitted: image0010077 -->
<!-- Page 73 -->
<h2>[S5.5] Bases orthonormales</h2>
<ul>
<li>Un espace vectoriel euclidien est un espace préhilbertien réel de dimension finie. Si $E$ est un espace euclidien, on appelle base orthonormale de $E$ toute base de $E$ qui est aussi une famille orthonormale.</li>
<li>Pour qu'une famille $(x_1, \ldots, x_n)$ de vecteurs d'un espace euclidien forme une base orthonormale, il faut et il suffit que la famille soit orthonormale et que $n = \dim E$.</li>
<li>Tout espace euclidien (non réduit à $\{0\}$) possède une base orthonormale.</li>
</ul>
<h3>Exemples</h3>
<ul>
<li>Dans $\mathbb{R}^n$ muni du produit scalaire canonique, la base canonique est une base orthonormale.</li>
<li>Dans $\mathcal{M}_n(\mathbb{R})$ muni du produit scalaire canonique, la base canonique $(E_{ij})_{1 \leq i, j \leq n}$ est une base orthonormale.</li>
</ul>
<h3>Expression du produit scalaire dans une base orthonormale</h3>
<ul>
<li>Soit $E$ un espace euclidien et $\mathscr{B} = (e_1, \ldots, e_n)$ une base orthonormale de $E$. Les coordonnées $(x_1, \ldots, x_n)$ d'un vecteur $x$ dans cette base sont données par :
$$\forall i \in 〚 1; n 〛, x_i = \langle e_i | x \rangle $$
Ainsi : $\forall x \in E, x = \sum_{i=1}^{n} \langle e_i | x \rangle e_i$.</li>
<li>Soient $x$ et $y$ deux vecteurs de $E$ :
$$ x = \sum_{i=1}^{n} x_i e_i \quad \text{et} \quad y = \sum_{i=1}^{n} y_i e_i \quad \text{avec} \ (x_1, \ldots, x_n) \ \text{et} \ (y_1, \ldots, y_n) \in \mathbb{K}^n. $$
En notant $X = ^t(x_1 \ldots x_n)$ et $Y = ^t(y_1 \ldots y_n)$ les matrices colonnes formées des coordonnées dans $\mathscr{B}$ des vecteurs $x$ et $y$, on a alors (en assimilant comme d'habitude une matrice de type (1,1) à un nombre réel) :
$$\langle x | y \rangle = \sum_{i=1}^{n} x_i y_i = {}^t X Y = {}^t Y X \quad \text{et} \quad \|x\| = \sqrt{\sum_{i=1}^{n} x_i^2}, \quad \|x\|^2 = {}^t X X. $$
$\checkmark$ Il est important de noter que les formules données ci-dessus ne sont valables que si la base choisie est orthonormale.</li>
</ul>
<h2>[S5.6] Projection orthogonale sur un sous-espace de dimension finie</h2>
<ul>
<li>Soit $F$ un sous-espace vectoriel de dimension finie d'un espace préhilbertien $E$. Alors :
$$ E = F \oplus F^{\perp} \quad \text{et} \quad (F^{\perp})^{\perp} = F. $$
On en déduit que si $F$ et $G$ sont deux sous-espaces vectoriels d'un espace préhilbertien de dimension finie $E$, alors :
$$ (F \cap G)^{\perp} = F^{\perp} + G^{\perp}. $$</li>
</ul>

<!-- Image omitted: image0010078 -->
<!-- Page 74 -->
<ul>
<li>$F^{\perp}$ s'appelle le supplémentaire orthogonal de $F$.<br>
On peut alors considérer la projection $p_F$ sur $F$ parallèlement à $F^{\perp}$; on l'appelle projection orthogonale sur $F$.<br>
<!-- Image omitted: Diagramme de projection orthogonale --></li>
<li><strong>Expression analytique</strong><br>
Si $(e_1, \ldots, e_p)$ est une base orthonormale de $F$, on a la formule :
$$\forall x \in E, p_F(x) = \sum_{i=1}^{p} \langle e_i | x \rangle e_i.
$$
Si $(e_1, \ldots, e_p)$ est seulement une base orthogonale de $F$, on a la formule :
$$\forall x \in E, p_F(x) = \sum_{i=1}^{p} \frac{\langle e_i | x \rangle}{\|e_i\|^2} e_i.
$$
En particulier, le projeté orthogonal sur la droite $D$ de base $a \neq 0$ est :
$$p_D(x) = \frac{\langle a | x \rangle}{\|a\|^2} a.
$$</li>
<li><strong>Lien avec la distance</strong><br>
Pour tout $x$ appartenant à $E$, le vecteur $p_F(x)$ est l'unique vecteur de $F$ réalisant la distance de $x$ à $F$, c'est-à-dire :
$$\|x - p_F(x)\| = \inf_{y \in F} \|x - y\| = d(x, F).
$$
<!-- Image omitted: Diagramme de distance à un sous-espace -->
D'après le théorème de Pythagore, on a :
$$d(x, F)^2 = \|x\|^2 - \|p_F(x)\|^2.$$</li>
</ul>

<!-- Image omitted: image0010079 -->
<!-- Page 75 -->
<p>En particulier, si l'on connaît une base orthonormale $(e_1, \ldots, e_p)$ de $F$, on aura la formule, pour tout $x \in E$ :
$$d(x, F)^2 = \|x\|^2 - \sum_{i=1}^{p} \langle e_i | x \rangle^2.
$$
Si la base $(e_1, \ldots, e_p)$ est seulement orthogonale, on aura :
$$d(x, F)^2 = \|x\|^2 - \sum_{i=1}^{p} \frac{\langle e_i | x \rangle^2}{\|e_i\|^2}.
$$
Des formules précédentes on déduit immédiatement l'inégalité de Bessel. Si $(e_1, \ldots, e_n)$ est une famille orthonormale d'un espace préhilbertien $E$, pour tout $x \in E$On a :
$$\sum_{i=1}^{n} \langle e_i | x \rangle^2 \leq \|x\|^2.
$$</p>
<h2>[S5.7] Procédé d'orthonormalisation de Gram-Schmidt</h2>
<ul>
<li>Soit $E$ un espace préhilbertien réel.<br>
On se donne dans $E$ une famille libre de vecteurs $(a_1, a_2, \ldots, a_n, \ldots)$, finie ou non. Il existe alors une et une seule famille orthonormale $(e_1, e_2, \ldots, e_n, \ldots)$ telle que :
<ol type="i">
<li>pour tout $k \geq 1$, $\text{Vect}(e_1, \ldots, e_k) = \text{Vect}(a_1, \ldots, a_k)$;</li>
<li>pour tout $k \geq 1$, $\langle e_k | a_k \rangle > 0$.</li>
</ol>
Dans ce théorème, la condition (ii) ne sert qu'à assurer l'unicité des $e_k$.</li>
<li>Le procédé de construction est tout aussi important à retenir que le théorème. Les vecteurs $e_k$ se calculent par récurrence de la façon suivante :
<ul>
<li>on pose $e_1 = \frac{a_1}{\|a_1\|}$;</li>
<li>si l'on a calculé $e_1, \ldots, e_k$ alors on considère le vecteur :
$$b_{k+1} = a_{k+1} - \sum_{i=1}^{k} \langle e_i | a_{k+1} \rangle e_i,
$$
puis l'on pose $e_{k+1} = \frac{b_{k+1}}{\|b_{k+1}\|}$.</li>
</ul>
Pour retenir facilement ces formules, il suffit de remarquer que le vecteur $\sum_{i=1}^{k} \langle e_i | a_{k+1} \rangle e_i$ n'est autre que le projeté orthogonal $p(a_{k+1})$ de $a_{k+1}$ sur le sous-espace vectoriel $\text{Vect}(e_1, \ldots, e_k) = \text{Vect}(a_1, \ldots, a_k)$, d'après la formule qui a été rappelée précédemment. Le vecteur $b_{k+1} = a_{k+1} - p(a_{k+1})$ appartient donc bien au supplémentaire orthogonal de $\text{Vect}(a_1, \ldots, e_k)$ dans $\text{Vect}(a_1, \ldots, a_k, a_{k+1})$. Dans de nombreux cas, trouver une base orthogonale (et non orthonormale) suffit. On trouve alors une telle base $(e_1', \ldots, e_n')$ comme suit :</li>
</ul>

<!-- Image omitted: image0010080 -->
<!-- Page 76 -->
<ul>
<li>on pose $e'_1 = a_1$;</li>
<li>si l'on a calculé $e'_1, \ldots, e'_k$ alors on pose :
$$e'_{k+1} = a_{k+1} - \sum_{i=1}^{k} \frac{\langle e'_i, a_{k+1} \rangle}{\| e'_i \|^2} e'_i.
$$</li>
</ul>
<h3>Interprétation matricielle</h3>
<p>Supposons maintenant $E$ de dimension finie $n$, et soit $\mathcal{B} = (a_1, \ldots, a_n)$ une base de $E$. Le procédé ci-dessus a permis de construire une base orthonormale $\mathcal{B}' = (e_1, \ldots, e_n)$.<br>
On peut alors vérifier que la matrice de passage de la base $\mathcal{B}$ à son orthonormalisée par le procédé de Schmidt $\mathcal{B}'$ est une matrice triangulaire supérieure à éléments diagonaux strictement positifs.</p>
<h2>[S5.8] Suites totales</h2>
<ul>
<li>Soit $E$ un espace préhilbertien réel. Une suite $\mathcal{B} = (e_n)_{n \in \mathbb{N}}$ d'éléments de $E$ est dite totale si $\text{Vect}(\mathcal{B})$ est dense dans l'espace vectoriel normé $(E, \| \cdot \|)$.</li>
<li>Si $\mathcal{B}$ est une suite totale alors $(\text{Vect}(\mathcal{B}))^\perp = \{0\}$.</li>
<li>La réciproque de cette propriété est fausse : si $(\text{Vect}(\mathcal{B}))^\perp = \{0\}$, la famille $\mathcal{B}$ n'est pas forcément totale.</li>
<li>Cette propriété implique que, lorsque $E$ est de dimension finie, une famille totale $\mathcal{B}$ est nécessairement génératrice de $E$.</li>
<li>Soit $\mathcal{B} = (e_n)_{n \in \mathbb{N}}$ une suite d'éléments d'un espace préhilbertien réel $E$. Pour tout $n \in \mathbb{N}$, notons $p_n$ la projection orthogonale sur le sous-espace vectoriel $\text{Vect}(e_0, \ldots, e_n)$.<br>
La suite $\mathcal{B}$ est totale si et seulement si pour tout $x \in E$ la suite $(p_n(x))_{n \in \mathbb{N}}$ converge vers $x$ (dans l'espace vectoriel normé $(E, \| \cdot \|)$).</li>
<li>Soit $\mathcal{B} = (e_n)_{n \in \mathbb{N}}$ une suite orthonormale d'éléments d'un espace préhilbertien $E$.
<ul>
<li>Pour tout $x \in E$ la série $\sum_{n \in \mathbb{N}} \langle e_n, x \rangle^2$ converge et :
$$\sum_{n=1}^{+\infty} \langle e_n, x \rangle^2 \leq \| x \|^2 \quad \text{(inégalité de Bessel généralisée)}.
$$</li>
<li>La suite $\mathcal{B}$ est totale si et seulement si pour tout $x \in E$, $\| x \|^2 = \sum_{n=1}^{+\infty} \langle e_n, x \rangle^2$ (égalité de Parseval).</li>
</ul>
</li>
</ul>
<h3>Polynômes orthogonaux</h3>
<p>Soit $B = \mathcal{C}([a; b], \mathbb{R})$ l'espace vectoriel des fonctions continues sur $[a; b]$ ($a < b$), et soit $\omega$ une fonction continue positive sur $[a; b]$ ne s'annulant qu'en un nombre fini de points ($\omega$ s'appelle une fonction de poids).</p>

<!-- Image omitted: image0010081 -->
<!-- Page 77 -->
<ul>
<li>L'application $\varphi : (f, g) \mapsto \int_{a}^{b} f(t) g(t) \omega(t) \, dt$ est un produit scalaire sur $E$.</li>
<li>Il existe une et une seule suite $(P_n)_{n \in \mathbb{N}}$ de polynômes unitaires, orthogonaux deux à deux pour ce produit scalaire, et tels que, pour tout $n \in \mathbb{N}$, $\deg(P_n) = n$.</li>
<li>La suite $(P_n)_{n \in \mathbb{N}}$ est une suite totale de l'espace préhilbertien $(E, \varphi)$.</li>
</ul>
<h2>[S5.9] Caractérisation des projecteurs orthogonaux et des symétries orthogonales</h2>
<p>$E$ désigne toujours un espace préhilbertien réel.</p>
<h4>Caractérisation des projecteurs orthogonaux parmi les projecteurs</h4>
<p>Soit $p$ un projecteur de $E$ ($p \in \mathscr{L}(E)$ et $p \circ p = p$). Les trois propriétés suivantes sont équivalentes :</p>
<ol type="i">
<li>$p$ est un projecteur orthogonal (c'est-à-dire $\operatorname{Im} p \perp \operatorname{Ker} p$),</li>
<li>pour tout $x \in E$, $\|p(x)\| \leqslant \|x\|$,</li>
<li>pour tous $x, y \in E$, $\langle p(x) \mid y \rangle = \langle x \mid p(y) \rangle$.</li>
</ol>
<p>$\checkmark$ La propriété (ii) implique que $p$ est une application linéaire continue sur sur $E$, lorsque $E$ est muni de la norme associée au produit scalaire (cf. chapitre sur les espaces vectoriels normés).</p>
<h4>Caractérisation des symétries orthogonales parmi les symétries</h4>
<p>Soit $p$ une symétrie de $E$ ($s \in \mathscr{L}(E)$ et $s \circ s = \operatorname{Id}_E$). Les trois propriétés suivantes sont équivalentes :</p>
<ol type="i">
<li>$s$ est une symétrie orthogonale,</li>
<li>pour tout $x \in E$, $\|s(x)\| = \|x\|$,</li>
<li>pour tous $x, y \in E$, $\langle s(x) \mid y \rangle = \langle x \mid s(y) \rangle$.</li>
</ol>

<!-- Image omitted: image0010082 -->
<!-- Page 78 -->
<h1>Thème 6 - Endomorphismes des espaces euclidiens</h1>
<p>Dans tout ce chapitre, $E$ désigne un espace euclidien, c'est-à-dire un espace préhilbertien réel de dimension finie, où le produit scalaire est noté $\langle \cdot \mid \cdot \rangle$.</p>
<h2>[S6.1] Isométries vectorielles</h2>
<ul>
<li>Une isométrie est un endomorphisme $u$ de $E$ tel que :
$$
\forall (x, y) \in E^2, \langle u(x) \mid u(y) \rangle = \langle x \mid y \rangle.
$$
On dit que $u$ conserve le produit scalaire.</li>
<li>Une isométrie $u$ peut aussi être caractérisée par l'une des propriétés équivalentes suivantes :
<ol type="i">
<li>$\forall x \in E, \|u(x)\| = \|x\|$ (on dit que $u$ conserve la norme),</li>
<li>$u$ transforme une/toute base orthonormale en une base orthonormale.</li>
</ol>
La propriété (ii) implique que, si $u$ est une isométrie, $u$ est aussi un endomorphisme bijectif (pour cette raison, une isométrie est aussi appelée automorphisme orthogonal).</li>
<li>Exemple : une symétrie orthogonale par rapport à un sous-espace vectoriel $F$ de $E$ est une isométrie.</li>
<li><strong>Matrices orthogonales</strong>
<ul>
<li>Une matrice carrée $A \in \mathcal{M}_n(\mathbb{R})$ est dite orthogonale si c'est la matrice d'une isométrie dans une base orthonormale, ou encore si c'est la matrice de passage d'une base orthonormale à une base orthonormale.<br>
Cela équivaut à dire que $^tAA = A^tA = I_n$.<br>
Pour reconnaître rapidement si une matrice est orthogonale, il est important de se souvenir de la propriété suivante :<br>
une matrice $A \in \mathcal{M}_n(\mathbb{R})$ est orthogonale si et seulement si ses vecteurs colonnes (ou ses vecteurs lignes), considérés comme éléments de $\mathbb{R}^n$, forment une famille orthonormale de $\mathbb{R}^n$ pour le produit scalaire canonique.</li>
<li>L'ensemble $\mathcal{O}_n(\mathbb{R})$ des matrices carrées orthogonales d'ordre $n$ est un sous-groupe du groupe $(\text{GL}_n(\mathbb{R}), \times)$ des matrices inversibles.</li>
<li>Si $A$ est une matrice orthogonale, toute valeur propre $\lambda$ de $A$, réelle ou complexe, est telle que $|\lambda| = 1$.<br>
En particulier, les seules valeurs propres possibles d'une isométrie sont $\pm 1$.</li>
</ul>
</li>
<li><strong>Groupe orthogonal</strong>
<ul>
<li>L'ensemble $O(E)$ des isométries de $E$ est un groupe pour la loi $\circ$; c'est un sous-groupe de $(\text{GL}(E), \circ)$, appelé groupe orthogonal.</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010083 -->
<!-- Page 79 -->
<ul>
<li>Si $u \in O(E)$, alors $|\det u| = 1$.<br>
L'ensemble des isométries de déterminant égal à +1 est un sous-groupe de $O(E)$, appelé groupe spécial orthogonal et noté $O^+(E)$; ses éléments sont appelés des rotations ou isométries positives.<br>
Une symétrie orthogonale par rapport à un hyperplan de $H$ s'appelle une réflexion; c'est une isométrie de déterminant -1.<br>
Une isométrie de déterminant -1 s'appelle aussi une isométrie négative; leur ensemble est noté $O^-(E)$.</li>
</ul>
<h2>[S6.2] Produit vectoriel</h2>
<h3>Produit mixte</h3>
<p>Soit $E$ un espace vectoriel euclidien orienté de dimension $n$, et $(x_1, \ldots, x_n)$ une famille de vecteurs de $E$.<br>
Alors le déterminant de cette famille dans toute base orthonormale directe est le même; il s'appelle le produit mixte de $(x_1, \ldots, x_n)$ et est noté $[x_1, \ldots, x_n]$ :
$$[x_1, \ldots, x_n] = \det_{\mathscr{B}}(x_1, \ldots, x_n) \quad \text{où } \mathscr{B} \text{ est une base orthonormale directe.}
$$</p>
<h4>Interprétation géométrique</h4>
<p>En dimension 2, la valeur absolue du produit mixte de $x_1$ et $x_2$ représente l'aire du parallélogramme construit sur $x_1$ et $x_2$.<br>
En dimension 3, la valeur absolue du produit mixte de trois vecteurs $x_1$, $x_2$ et $x_3$ représente le volume du parallélépipède construit sur ces trois vecteurs.<br>
<!-- Image omitted: Geometric interpretation of mixed product -->
</p>
<h3>Produit vectoriel</h3>
<p>Soit $E_3$ un espace vectoriel euclidien orienté de dimension 3, et $u$, $v$ deux vecteurs de $E_3$. Alors il existe un et un seul vecteur $a \in E_3$ tel que :
$$\forall w \in E_3, [u, v, w] = (a | w).
$$
$a$ s'appelle le produit vectoriel de $u$ et $v$ et est noté $u \wedge v$. Il est donc caractérisé par :
$$\forall (u, v, w) \in E_3^3, [u, v, w] = (u \wedge v | w).
$$</p>
<h3>Propriétés</h3>
<ul>
<li>Soient $u$ et $v$ deux vecteurs de $E_3$.
<ul>
<li>$u \wedge v = 0_E \iff$ la famille $(u, v)$ est liée.</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010084 -->
<!-- Page 80 -->
<ul>
<li>Le vecteur $u \wedge v$ appartient à $\text{Vect}(u, v)^{\perp}$. De plus, si la famille $(u, v)$ est libre, la famille $(u, v, u \wedge v)$ est une base directe.</li>
<li>L'application $\left\{ \begin{array}{ccc} E_{3} \times E_{3} & \longrightarrow & E_{3} \\ (u, v) & \longmapsto & u \wedge v \end{array} \right.$ est bilinéaire antisymétrique.</li>
<li>Si $u$ et $v$Ont pour coordonnées respectives $(x, y, z)$ et $(x', y', z')$ dans une base orthonormale directe $\mathcal{B}$, les coordonnées dans cette même base du produit vectoriel $u \wedge v$ sont :
$$\begin{vmatrix}
y & y' \\
z & z'
\end{vmatrix}, \quad -\begin{vmatrix}
x & x' \\
z & z'
\end{vmatrix} \quad \text{et} \quad \begin{vmatrix}
x & x' \\
y & y'
\end{vmatrix}.
$$</li>
<li><em>Formule du double produit vectoriel</em>
$$\forall (u, v, w) \in E_{3}^{3}, \quad u \wedge (v \wedge w) = \langle u | w \rangle v - \langle u | v \rangle w.
$$</li>
</ul>
<h2>[S6.3] Classification des isométries en dimension 2</h2>
<p>On note ici $E$ un plan vectoriel euclidien, muni d'une base orthonormale $\mathcal{B}$, et on supposera $E$ orienté par le choix de cette base.<br>
Soit $u$ une isométrie de $E$. Il y a deux cas possibles.</p>
<ul>
<li>$\text{det} \, u = +1$<br>
$u$ est alors une rotation. Il existe un réel $\theta \in ]-\pi ; \pi]$, appelé angle de la rotation, tel que la matrice de $u$ dans toute base orthonormale directe soit de la forme :
$$M_{\mathcal{B}}(u) = \begin{pmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{pmatrix}.
$$
Sa matrice dans une base orthonormale indirecte s'obtient en changeant $\theta$ en $-\theta$.</li>
<li>$\text{det} \, u = -1$<br>
Il existe alors un réel $\theta$ tel que :
$$M_{\mathcal{B}}(u) = \begin{pmatrix}
\cos \theta & \sin \theta \\
\sin \theta & -\cos \theta
\end{pmatrix}
$$
mais ici, $\theta$ dépend de la base orthonormale $\mathcal{B}$ choisie.<br>
Il est alors facile de vérifier, en cherchant les vecteurs propres de $u$ pour les valeurs propres $\pm 1$, que $u$ est une symétrie orthogonale par rapport à une droite $D$ (réflexion). Cette droite est engendrée par le vecteur de coordonnées $\left( \cos \frac{\theta}{2}, \sin \frac{\theta}{2} \right)$ dans la base $\mathcal{B}$.</li>
</ul>
<h2>[S6.4] Classification des isométries en dimension 3</h2>
<p>On note ici $E$ un espace vectoriel euclidien orienté de dimension 3.<br>
Soit $u$ une isométrie de $E$. Il y a deux cas possibles.</p>

<!-- Image omitted: image0010085 -->
<!-- Page 81 -->
<ul>
<li>$\operatorname{det} u = +1$<br>
$u$ est une rotation. On cherche alors l'ensemble des vecteurs invariants par $u$, c'est-à-dire le sous-espace propre $E_1(u)$ associé à la valeur propre 1. On montre qu'il n'y a que deux cas possibles :
<ul>
<li>soit $\operatorname{dim} E_1(u) = 3$ : $u$ est alors l'application identique de $E$.</li>
<li>soit $\operatorname{dim} E_1(u) = 1$ : l'ensemble des vecteurs invariants de $u$ est ici une droite $D$, appelée axe de la rotation.<br>
Le plan $P = D^{\perp}$ est alors stable par $u$; en considérant un vecteur de base unitaire $e_1$ de l'axe $D$ et une base orthonormale $(e_2, e_3)$ de $P$ telle que la base $\mathscr{B} = (e_1, e_2, e_3)$ soit directe, la matrice de $u$ dans $\mathscr{B}$ est de la forme :
$$M_{\mathscr{B}}(u) = \begin{pmatrix}
1 & 0 & 0 \\
0 & \cos \theta & -\sin \theta \\
0 & \sin \theta & \cos \theta
\end{pmatrix}.
$$
$\theta$ est l'angle de la rotation; il vérifie la relation : $\operatorname{tr} u = 1 + 2 \cos \theta$ (on rappelle que la trace d'un endomorphisme ne dépend pas de la base dans laquelle on écrit sa matrice). Cette relation très simple permet de déterminer facilement $\pm \theta$. Pour déterminer la valeur exacte de $\theta$, dont le signe dépend de l'orientation, nous vous invitons à vous reporter à l'exemple traité en [SF6.10] page 364.</li>
</ul>
</li>
<li>$\operatorname{det} u = -1$<br>
On montre alors qu'il existe une base orthonormale $\mathscr{B} = (e_1, e_2, e_3)$ (que l'on peut choisir directe) telle que la matrice de $u$ dans cette base soit de la forme :
$$M_{\mathscr{B}}(u) = \begin{pmatrix}
-1 & 0 & 0 \\
0 & \cos \theta & -\sin \theta \\
0 & \sin \theta & \cos \theta
\end{pmatrix}.
$$
<ul>
<li>si $\theta \equiv 0 \ (\bmod\ 2\pi)$, $u$ est la symétrie orthogonale par rapport au plan $P$ de base $(e_2, e_3)$. Ce plan est le sous-espace propre $E_1(u)$.</li>
<li>si $\theta \equiv \pi \ (\bmod\ 2\pi)$, on a alors simplement $u = -\operatorname{Id}_E$.</li>
<li>sinon, comme on peut le vérifier facilement à l'aide de la représentation matricielle, $u$ est la composée commutative de la réflexion par rapport à $P$ et de la rotation d'axe $\mathbb{R} e_1$ et d'angle $\theta$; cet axe est alors l'ensemble des vecteurs $x$ tels que $u(x) = -x$, c'est-à-dire le sous-espace propre $E_{-1}(u)$, et l'ensemble des vecteurs invariants de $u$ est réduit à $\{0\}$. L'angle $\theta$ de la rotation vérifie ici la relation : $\operatorname{tr} u = -1 + 2 \cos \theta$. $u$ s'appelle l'antirotation d'axe $\mathbb{R} e_1$ et d'angle $\theta$.</li>
</ul>
$\checkmark$ Vous remarquerez que la simple détermination des sous-espaces propres de $u$ et le calcul de sa trace permettent de caractériser $u$ presque entièrement (au signe près de $\theta$ dans le cas d'une rotation ou d'une antirotation).<br>
Les isométries sont en fait caractérisées par la dimension du sous-espace des vecteurs invariants, comme le montrent les tableaux récapitulatifs suivants.</li>
</ul>

<!-- Image omitted: image0010086 -->
<!-- Page 82 -->
<ul>
<li>si $u$ est une isométrie en dimension 2
<table>
<thead>
<tr>
<th>$\dim E_{1}(u)$</th>
<th>Nature de $u$</th>
<th>Type d'isométrie</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>rotation</td>
<td>+</td>
</tr>
<tr>
<td>1</td>
<td>réflexion d'axe $E_{1}(u)$</td>
<td>-</td>
</tr>
<tr>
<td>2</td>
<td>$\text{Id}_{E}$</td>
<td>+</td>
</tr>
</tbody>
</table>
</li>
<li>si $u$ est une isométrie en dimension 3
<table>
<thead>
<tr>
<th>$\dim E_{1}(u)$</th>
<th>Nature de $u$</th>
<th>Type d'isométrie</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>antirotation d'axe $E_{-1}(u)$</td>
<td>-</td>
</tr>
<tr>
<td>1</td>
<td>rotation d'axe $E_{1}(u)$</td>
<td>+</td>
</tr>
<tr>
<td>2</td>
<td>réflexion par rapport au plan $E_{1}(u)$</td>
<td>-</td>
</tr>
<tr>
<td>3</td>
<td>$\text{Id}_{E}$</td>
<td>+</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h2>[S6.5] Réduction d'une isométrie dans le cas général</h2>
<p>On note ici $E$ un espace vectoriel euclidien de dimension $n \in \mathbb{N}^{*}$. On rappelle que si $u \in O(E)$, alors $\det u = \pm 1$ et $\text{Sp}(u) \subset \{-1,1\}$.</p>
<ul>
<li><strong>Sous-espaces stables</strong><br>
Soit $u$ une isométrie de $E$. Si $F$ est un sous-espace de $E$ stable par $u$, alors $F^{\perp}$ est aussi stable par $u$.</li>
<li><strong>Réduction d'une isométrie dans une base orthonormale</strong><br>
Soit $u$ une isométrie de $E$.<br>
Il existe une base orthonormale $\mathcal{B}$ de $E$ telle que la matrice $A$ de $u$ dans $\mathcal{B}$ soit diagonale par blocs, de la forme :
$$A = \begin{bmatrix}
I_{p} & 0 & \cdots & \cdots & 0 \\
0 & -I_{q} & & & \vdots \\
\vdots & & R_{1} & & \vdots \\
\vdots & & & \ddots & 0 \\
0 & \cdots & \cdots & 0 & R_{s}
\end{bmatrix}
$$
où $I_{p}$ et $I_{q}$ désignent les matrices identités d'ordres $p$ et $q$ et où pour tout $i \in 〚 1; s 〛$ $R_{i}$ est une matrice de rotation de la forme $R_{i} = \begin{pmatrix} \cos \theta_{i} & -\sin \theta_{i} \\ \sin \theta_{i} & \cos \theta_{i} \end{pmatrix}$ avec $\theta_{i} \in \mathbb{R} \setminus \pi \mathbb{Z}$.<br>
$\checkmark$ Dans l'écriture ci-dessus, certains blocs peuvent ne pas être présents. Par exemple, si les blocs $R_{i}$ ne figurent pas, $u$ est une symétrie orthogonale, et il s'agit du seul cas où une isométrie peut être diagonalisable.</li>
</ul>

<!-- Image omitted: image0010087 -->
<!-- Page 83 -->
<h2>[S6.6] Endomorphismes symétriques</h2>
<ul>
<li>Un endomorphisme $u$ d'un espace vectoriel euclidien $E$ est dit symétrique (relativement au produit scalaire considéré) si :
$$\forall (x, y) \in E^2, \ \langle u(x) | y \rangle = \langle x | u(y) \rangle.
$$
Cette notion dépend du produit scalaire considéré sur $E$.<br>
L'ensemble $\mathscr{S}(E)$ des endomorphismes symétriques de $E$ est un sous-espace vectoriel de $\mathscr{L}(E)$.</li>
<li>Un endomorphisme est symétrique si et seulement si sa matrice dans une/toute base orthonormale est symétrique.</li>
<li>Un projecteur est un endomorphisme symétrique si et seulement si c'est un projecteur orthogonal.</li>
<li>Une symétrie est un endomorphisme symétrique si et seulement si c'est une symétrie orthogonale.</li>
<li>Soit $u$ un endomorphisme symétrique de $E$.<br>
Si $F$ est un sous-espace vectoriel de $E$ stable par $u$, alors $F^{\perp}$ est aussi stable par $u$.</li>
<li><strong>Théorème spectral</strong><br>
Soit $E$ un espace vectoriel euclidien, et $u$ un endomorphisme symétrique de $E$.
<ol type="i">
<li>Le polynôme caractéristique de $u$ est scindé dans $\mathbb{R}[X]$.</li>
<li>Les sous-espaces propres de $u$ sont orthogonaux deux à deux.</li>
<li>Il existe une base orthonormale de $E$ formée de vecteurs propres de $u$ (ce que l'on traduit en disant brièvement : l'endomorphisme $u$ est diagonalisable dans une base orthonormale).</li>
</ol>
Puisque la matrice d'un endomorphisme symétrique dans une base orthonormale est une matrice symétrique, on en déduit le théorème suivant.<br>
Si $S$ est une matrice symétrique réelle, il existe une matrice diagonale $D$ et une matrice orthogonale $P$ telles que :
$$S = PDP^{-1} = PDP^t
$$
(on dit aussi que $S$ est orthodiagonalisable).</li>
</ul>

<!-- Image omitted: image0010088 -->
<!-- Page 84 -->
<h1>Thème 7 - Espaces vectoriels normés</h1>
<p>Dans tout ce chapitre, les espaces vectoriels considérés sont des espaces vectoriels sur $\mathbb{K}$, où $\mathbb{K}$ désigne $\mathbb{R}$Ou $\mathbb{C}$.<br>
Si $\lambda \in \mathbb{K}$, l'écriture $|\lambda|$ désigne la <em>valeur absolue</em> de $\lambda$ si $\mathbb{K} = \mathbb{R}$, et le <em>module</em> de $\lambda$ si $\mathbb{K} = \mathbb{C}$.</p>
<h2>[S7.1] Espaces vectoriels normés</h2>
<h3>Normes sur un espace vectoriel</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel. On appelle norme sur $E$ une application $N : E \longrightarrow \mathbb{R}$ telle que :</p>
<ol type="i">
<li>$\forall x \in E, N(x) \geqslant 0$ (<em>positivité</em>) ;</li>
<li>$\forall x \in E, N(x) = 0 \Longrightarrow x = 0$ (<em>séparation</em>) ;</li>
<li>$\forall x \in E, \forall \lambda \in \mathbb{K}, N(\lambda x) = |\lambda|N(x)$ (<em>homogénéité</em>) ;</li>
<li>$\forall (x, y) \in E^2, N(x + y) \leqslant N(x) + N(y)$ (<em>inégalité triangulaire</em>).</li>
</ol>
<p>On dit alors que le couple $(E, N)$ est un espace vectoriel normé. On écrit souvent $N(x) = \|x\|$.<br>
Un vecteur $x$ d'un espace vectoriel normé $(E, \|\cdot\|)$ est dit unitaire si $\|x\| = 1$. Pour tout vecteur $x \in E$ non nul, $\frac{x}{\|x\|}$ est unitaire.</p>
<h3>Inégalité triangulaire</h3>
<p>Soit $(E, \|\cdot\|)$ un espace vectoriel normé. Alors :</p>
<ul>
<li>$\forall (x, y) \in E^2, | \|x\| - \|y\| | \leqslant \|x + y\| \leqslant \|x\| + \|y\|$.</li>
<li>$\forall (x_1, \ldots, x_n) \in E^n, \forall (\lambda_1, \ldots, \lambda_n) \in \mathbb{K}^n, \left\| \sum_{i=1}^n \lambda_i x_i \right\| \leqslant \sum_{i=1}^n |\lambda_i| \|x_i\|$.</li>
</ul>
<h3>Exemples</h3>
<ul>
<li>La valeur absolue dans $\mathbb{R}$ et le module dans $\mathbb{C}$ sont des normes.</li>
<li>Soit $(E, \langle \cdot \mid \cdot \rangle)$ un espace vectoriel préhilbertien réel. Alors la norme euclidienne associée au produit scalaire $\langle \cdot \mid \cdot \rangle$, définie par $\|x\| = \sqrt{\langle x \mid x \rangle}$, est une norme sur $E$.</li>
<li>Soit $E$ un espace vectoriel de dimension finie $n$, muni d'une base $\mathcal{B}$. Pour tout vecteur $x$ de coordonnées $(x_1, \ldots, x_n) \in \mathbb{K}^n$ dans $\mathcal{B}$, on note :
$$\|x\|_1 = \sum_{i=1}^n |x_i|, \quad \|x\|_2 = \sqrt{\sum_{i=1}^n |x_i|^2} \quad \text{et} \quad \|x\|_{\infty} = \max_{1 \leqslant i \leqslant n} |x_i|.
$$
Alors $\|\cdot\|_1, \|\cdot\|_2$ et $\|\cdot\|_{\infty}$ sont trois normes sur $E$.</li>
</ul>

<!-- Image omitted: image0010089 -->
<!-- Page 85 -->
<p>Soient $a < b$ deux réels. Pour toute fonction $f \in \mathscr{C}([a;b],\mathbb{K})$, on note :
$$\Vert f \Vert_1 = \int_a^b |f(t)|\,dt, \quad \Vert f \Vert_2 = \sqrt{\int_a^b |f(t)|^2\,dt} \quad \text{et} \quad \Vert f \Vert_\infty = \sup_{t \in [a;b]} |f(t)|.
$$
Alors $\Vert \cdot \Vert_1$, $\Vert \cdot \Vert_2$ et $\Vert \cdot \Vert_\infty$ sont trois normes sur $\mathscr{C}([a;b],\mathbb{K})$, appelées respectivement : norme de la convergence en moyenne, norme de la convergence en moyenne quadratique et norme de la convergence uniforme.</p>
<h3>Produit d'espaces vectoriels normés</h3>
<p>Soient $(E_1, N_1), \ldots, (E_p, N_p)$ une famille finie de $\mathbb{K}$-espaces vectoriels normés. On peut définir sur l'espace vectoriel produit $E = E_1 \times \cdots \times E_p$ l'application :
$$N : (x_1, \ldots, x_p) \mapsto \max_{1 \leq i \leq p} N_i(x_i).
$$
Alors $N$ est une norme sur $E$; l'espace vectoriel normé $(E, N)$ s'appelle l'espace vectoriel normé produit des $(E_i, N_i)$.</p>
<h3>Algèbre normée</h3>
<p>Une algèbre normée est une $\mathbb{K}$-algèbre $(A, +, \cdot, \times)$ munie d'une norme $\Vert \cdot \Vert$ telle que :
$$\forall (x, y) \in A^2, \quad \Vert x \times y \Vert \leq \Vert x \Vert \Vert y \Vert.
$$
On dira qu'il s'agit d'une algèbre normée unitaire si, de plus : $\Vert 1_A \Vert = 1$.</p>
<h3>Distance associée à une norme</h3>
<p>Soit $(E, \Vert \cdot \Vert)$ un espace vectoriel normé. Pour tout $(x, y) \in E^2$, on appelle distance de $x$ à $y$ le réel $d(x, y) = \Vert x - y \Vert$.</p>
<h4>Propriétés</h4>
<ul>
<li>$\forall (x, y) \in E^2, d(x, y) \geq 0$.</li>
<li>$\forall (x, y) \in E^2, d(x, y) = d(y, x)$.</li>
<li>$\forall (x, y) \in E^2, d(x, y) = 0 \Longleftrightarrow x = y$.</li>
<li>$\forall (x, y, z) \in E^3, |d(x, y) - d(y, z)| \leq d(x, z) \leq d(x, y) + d(y, z)$.</li>
</ul>
<h3>Distance à une partie</h3>
<p>Soit $(E, \Vert \cdot \Vert)$ un espace vectoriel normé, $A$ une partie de $E$ non vide, et $x \in E$. L'ensemble $\{\Vert x - a \Vert, a \in A\}$ est une partie non vide de $\mathbb{R}$, minorée par 0; elle admet donc une borne inférieure, appelée distance de $x$ à $A$, et notée $d(x, A)$ :
$$d(x, A) = \inf_{a \in A} d(x, a).
$$</p>
<h2>[S7.2] Boules</h2>
<ul>
<li>Soit $(E, \Vert \cdot \Vert)$ un espace vectoriel normé, soit $a$ un vecteur de $E$ et soit $r$ un réel positif. On appelle boule ouverte de centre $a$ et de rayon $r$ l'ensemble :
$$B(a, r) = \{x \in E \mid d(a, x) < r\} = \{x \in E \mid \Vert a - x \Vert < r\}.
$$
On appelle boule fermée de centre $a$ et de rayon $r$ l'ensemble :
$$B_f(a, r) = \{x \in E \mid d(a, x) \leq r\} = \{x \in E \mid \Vert a - x \Vert \leq r\}.
$$</li>
</ul>

<!-- Image omitted: image0010090 -->
<!-- Page 86 -->
<p>On appelle sphère de centre $a$ et de rayon $r$ l'ensemble :
$$\mathcal{S}(a, r) = \{ x \in E \mid d(a, x) = r \} = \{ x \in E \mid \|a - x\| = r \}.
$$
Dans le cas où $a = 0_E$ et $r = 1$, on parle de boule unité et de sphère unité.</p>
<h3>Exemples</h3>
<ul>
<li>Dans $\mathbb{R}$ muni de la valeur absolue $|\cdot|$ :
$$\mathcal{B}(a, r) = ]a - r ; a + r[, \quad \mathcal{B}_f(a, r) = [a - r ; a + r] \quad \text{et} \quad \mathcal{S}(a, r) = \{a - r, a + r\}.
$$</li>
<li>La figure ci-après représente les boules unités fermées pour les trois normes usuelles de $\mathbb{R}^2$.
<ul>
<li>Pour la norme $\|\cdot\|_1$, $B_1 = \{(x, y) \in \mathbb{R}^2 \mid |x| + |y| \leq 1\}$.</li>
<li>Pour la norme $\|\cdot\|_2$, $B_2 = \{(x, y) \in \mathbb{R}^2 \mid x^2 + y^2 \leq 1\}$.</li>
<li>Pour la norme $\|\cdot\|_{\infty}$, $B_{\infty} = \{(x, y) \in \mathbb{R}^2 \mid |x| \leq 1 \quad \text{et} \quad |y| \leq 1\}$.</li>
</ul>
<!-- Image omitted: Figure des boules unités --></li>
<li>Soit $(E, \|\cdot\|)$ un espace vectoriel normé, soit $(a_1, a_2) \in E^2$, et soient $r_1$ et $r_2$ deux réels strictement positifs. On note $d = d(a_1, a_2) = \|a_1 - a_2\|$. Alors :
$$\begin{aligned}
&\mathcal{B}(a_1, r_1) \subset \mathcal{B}(a_2, r_2) \Longleftrightarrow d \leq r_2 - r_1, \\
&\mathcal{B}_f(a_1, r_1) \subset \mathcal{B}_f(a_2, r_2) \Longleftrightarrow d \leq r_2 - r_1, \\
&\mathcal{B}(a_1, r_1) \cap \mathcal{B}(a_2, r_2) = \emptyset \Longleftrightarrow d \geq r_1 + r_2, \\
&\mathcal{B}_f(a_1, r_1) \cap \mathcal{B}_f(a_2, r_2) = \emptyset \Longleftrightarrow d > r_1 + r_2.
\end{aligned}
$$</li>
</ul>
<h3>Parties convexes</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel. Pour tout $(x, y) \in E^2$, on appelle segment $[x ; y]$ l'ensemble :
$$[x ; y] = \{tx + (1 - t)y \mid t \in [0 ; 1]\}.
$$
Une partie $A$ d'un espace vectoriel est dite convexe si, pour tout $(x, y) \in A^2$, le segment $[x ; y]$ est inclus dans $A$.<br>
Dans un espace vectoriel normé $(E, \|\cdot\|)$, toute boule (ouverte ou fermée) est une partie convexe de $E$.</p>
<h3>Parties bornées</h3>
<p>Soit $(E, \|\cdot\|)$ un espace vectoriel normé. Une partie $A$ de $E$ est dite bornée si :
$$\exists M \in \mathbb{R}, \forall x \in A, \|x\| \leq M.
$$
Autrement dit, $A$ est bornée s'il existe un réel positif $M$ tel que $A \subset \mathcal{B}_f(0, M)$.</p>

<!-- Image omitted: image0010091 -->
<!-- Page 87 -->
<h2>[S7.3] Comparaison des normes</h2>
<ul>
<li>On dit que deux normes $N_1$ et $N_2$ sur $E$ sont équivalentes s'il existe deux réels strictement positifs $\alpha$ et $\beta$ tels que :
$$
\forall x \in E, \alpha N_1(x) \leq N_2(x) \leq \beta N_1(x),
$$
ce qui équivaut à dire que les rapports $\frac{N_1}{N_2}$ et $\frac{N_2}{N_1}$ sont bornés sur $E \setminus \{0\}$.</li>
<li><strong>Exemple</strong><br>
Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension $n$, muni d'une base $\mathcal{B}$. On note $\|\cdot\|_1$, $\|\cdot\|_2$ et $\|\cdot\|_\infty$ les normes usuelles associées. Alors :
$$
\forall x \in E, \|x\|_\infty \leq \|x\|_1 \leq n \cdot \|x\|_\infty \quad \text{et} \quad \|x\|_\infty \leq \|x\|_2 \leq \sqrt{n} \cdot \|x\|_\infty.
$$
Cela démontre que les trois normes $\|\cdot\|_1$, $\|\cdot\|_2$ et $\|\cdot\|_\infty$ sont équivalentes.</li>
<li><strong>Caractérisations</strong><br>
Soit $E$ un $\mathbb{K}$-espace vectoriel, et $N_1$, $N_2$ deux normes sur $E$.
<ul>
<li>$N_1$ et $N_2$ sont équivalentes si et seulement si toute boule ouverte pour l'une de ces deux normes contient une boule ouverte pour l'autre, de même centre.</li>
<li>Les deux normes $N_1$ et $N_2$ sont équivalentes si et seulement si toute partie bornée pour l'une de ces deux normes est aussi bornée pour l'autre.</li>
</ul>
</li>
<li><strong>Équivalence des normes en dimension finie</strong><br>
Si $E$ est un $\mathbb{K}$-espace vectoriel de dimension finie, toutes les normes sur $E$ sont équivalentes.<br>
Ce résultat ne subsiste pas en dimension infinie.<br>
Par exemple, dans $E = \mathcal{C}([a, b], \mathbb{R})$, les normes $\|\cdot\|_1$, $\|\cdot\|_2$ et $\|\cdot\|_\infty$ sont deux à deux non équivalentes.</li>
</ul>
<h2>[S7.4] Suites d'un espace vectoriel normé</h2>
<p>Soit $(E, \|\cdot\|)$ un espace vectoriel normé.</p>
<ul>
<li>On note $E^{\mathbb{N}}$ l'ensemble des suites définies sur $\mathbb{N}$ à valeurs dans $E$, c'est-à-dire l'ensemble des applications de la forme $u : \mathbb{N} \rightarrow E$
$$
n \mapsto u_n.
$$
Pour toutes suites $u, v \in E^{\mathbb{N}}$ et pour tout scalaire $\lambda \in \mathbb{K}$, on note :
$$
u + v : n \mapsto u_n + v_n \quad \text{et} \quad \lambda u : n \mapsto \lambda u_n.
$$
Muni de ces lois, l'ensemble $(E^{\mathbb{N}}, +, \cdot)$ est un $\mathbb{K}$-espace vectoriel.</li>
<li><strong>Suites bornées</strong><br>
Soit $u \in E^{\mathbb{N}}$ une suite. On dit que $u$ est bornée si :
$$
\exists M \in \mathbb{R}, \forall n \in \mathbb{N}, \|u_n\| \leq M.
$$
Ainsi, $u$ est bornée si l'ensemble $\{u_n \mid n \in \mathbb{N}\}$ est une partie bornée de $E$.<br>
L'ensemble noté $\ell^{\infty}(E)$ des suites bornées d'éléments de $E$ est un sous-espace vectoriel de $E^{\mathbb{N}}$ qui peut être muni d'une structure d'espace vectoriel normé pour la norme $\|\cdot\|_\infty$ définie par :
$$
\forall u \in \ell^{\infty}(E), \|u\|_\infty = \sup_{n \in \mathbb{N}} \|u_n\|.
$$</li>
</ul>

<!-- Image omitted: image0010092 -->
<!-- Page 88 -->
<h3>Suites convergentes</h3>
<p>Soit $u \in E^{\mathbb{N}}$ une suite. On dit que $u$ est convergente s'il existe $\ell \in E$ tel que :
$$\forall \varepsilon > 0, \exists n_0 \in \mathbb{N} \text{ tel que } \forall n \in \mathbb{N}, n \geq n_0 \implies \| u_n - \ell \| < \varepsilon.
$$
Cela signifie que, pour toute boule $B$ de centre $\ell$ et de rayon $\varepsilon$ strictement positif (arbitrairement petit), la suite $u$ est à valeurs dans $B$ à partir d'un certain rang. Si $u$ est convergente, alors le vecteur $\ell$ introduit dans la définition est unique. On dit alors que $u$ converge vers $\ell$, ou que $\ell$ est la limite de la suite $u$, et l'on note :
$$\ell = \lim_{n \to +\infty} u_n.
$$
Une suite qui n'est pas convergente est dite divergente.</p>
<h3>Propriétés des suites convergentes</h3>
<ul>
<li>Dire que $\ell = \lim_{n \to +\infty} u_n$ signifie aussi que la suite réelle $(\| u_n - \ell \|)$ converge vers 0 quand $n \to +\infty$.</li>
<li>Toute suite convergente est bornée.</li>
<li>Soient $(u_n)$ et $(v_n)$ deux suites d'éléments de $E$ convergeant respectivement vers $\ell$ et $\ell'$. Alors la suite $(u_n + v_n)$ converge vers $\ell + \ell'$.</li>
<li>Soit $(\lambda_n)$ une suite d'éléments de $\mathbb{K}$ convergeant vers $\lambda \in \mathbb{K}$, et $(u_n)$ une suite d'éléments de $E$ convergeant vers $\ell \in E$. Alors, la suite $(\lambda_n u_n)$ converge vers $\lambda \ell$.<br>
Cela entraîne que l'ensemble des suites convergentes à valeurs dans $E$ est un sous-espace vectoriel de $E^{\mathbb{N}}$. De plus, l'application qui à une suite convergente associe sa limite est linéaire.</li>
</ul>
<h3>Suites extraites</h3>
<ul>
<li>Soit $u \in E^{\mathbb{N}}$. On appelle suite extraite de $u$ toute suite de la forme $(u_{\varphi(n)})$, où $\varphi: \mathbb{N} \to \mathbb{N}$ est une application strictement croissante.</li>
<li>Un élément $a \in E$ est appelé valeur d'adhérence d'une suite $u \in E^{\mathbb{N}}$ s'il existe une suite extraite de $u$ qui converge vers $a$.</li>
<li>Si $u \in E^{\mathbb{N}}$ est une suite convergente de limite $\ell \in E$, toute suite extraite de $u$ converge vers $\ell$.<br>
Il en résulte qu'une suite qui a au moins deux valeurs d'adhérence distinctes est divergente.</li>
</ul>
<h3>Comparaison de normes</h3>
<p>Soit $E$ un $\mathbb{K}$-espace vectoriel, et soient $N_1$ et $N_2$ deux normes sur $E$.</p>
<ul>
<li>Les deux normes $N_1$ et $N_2$ sont équivalentes si et seulement si toute suite $u \in E^{\mathbb{N}}$ bornée pour l'une est aussi bornée pour l'autre.</li>
<li>Les deux normes $N_1$ et $N_2$ sont équivalentes si et seulement si toute suite $u \in E^{\mathbb{N}}$ qui converge vers $\ell \in E$ au sens de l'une converge aussi vers $\ell$ au sens de l'autre.<br>
$\checkmark$ Ces résultats peuvent tomber en défaut si les normes ne sont pas équivalentes.</li>
</ul>

<!-- Image omitted: image0010093 -->
<!-- Page 89 -->
<h3>Exemple :</h3>
<p>dans $E = \mathscr{C}([0;1], \mathbb{R})$ muni des normes usuelles $\|\cdot\|_{1}$ et $\|\cdot\|_{\infty}$,</p>
<ul>
<li>la suite $f_{n}: t \mapsto n t^{n}$ est bornée pour $\|\cdot\|_{1}$, mais pas pour $\|\cdot\|_{\infty}$;</li>
<li>la suite $g_{n}: t \mapsto t^{n}$ converge vers 0 pour $\|\cdot\|_{1}$, mais diverge
  ```html
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MATHS MP/MP* - Savoir & Faire en Prépas (Suite 2)</title>
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css">
    <!-- KaTeX JS -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"></script>
    <!-- KaTeX auto-render extension -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 2em;
            max-width: 800px; /* Limit width for readability */
            margin-left: auto;
            margin-right: auto;
        }
        h1, h2, h3, h4, h5, h6 {
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }
        h1 { border-bottom: 2px solid #ccc; padding-bottom: 0.3em; }
        h2 { border-bottom: 1px solid #eee; padding-bottom: 0.2em; }
        p { margin-bottom: 1em; }
        ul, ol { margin-left: 2em; margin-bottom: 1em; }
        li { margin-bottom: 0.5em; }
        blockquote {
            margin-left: 2em;
            padding-left: 1em;
            border-left: 3px solid #eee;
            font-style: italic;
            color: #555;
        }
        code {
            background-color: #f4f4f4;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
        }
        pre code {
            display: block;
            padding: 1em;
            overflow-x: auto;
        }
       .katex-display { /* Style for KaTeX display math */
            display: block;
            margin-top: 1em;
            margin-bottom: 1em;
            overflow-x: auto; /* Handle long formulas */
            overflow-y: hidden;
        }
        .center { text-align: center; }
        .small { font-size: 0.8em; }
        .isbn { font-family: monospace; }
        table {
            border-collapse: collapse;
            margin-bottom: 1em;
            width: 100%;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 0.5em;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>

<!-- Continuation from page 89 -->
<p>(plus précisément, cette suite converge simplement mais pas uniformément, comme nous le verrons dans le chapitre consacré aux suites de fonctions).</p>
<h3>Cas d'un espace vectoriel normé de dimension finie</h3>
<p>Soit $(E, \|\cdot\|)$ un espace vectoriel normé de dimension finie $p$, muni d'une base $\mathscr{B} = (e_{1}, \ldots, e_{p})$. Soit $u \in E^{\mathbb{N}}$, et soit $\ell \in E$. On note $(\ell_{1}, \ldots, \ell_{p}) \in \mathbb{K}^{p}$ les coordonnées de $\ell$ dans la base $\mathscr{B}$ et, pour tout $n \in \mathbb{N}$, on note $(u_{n}^{(1)}, \ldots, u_{n}^{(p)}) \in \mathbb{K}^{p}$ les coordonnées de $u_{n}$ dans la base $\mathscr{B}$, c'est-à-dire :
$$\ell = \sum_{i=1}^{p} \ell_{i} e_{i} \quad \text{et} \quad \forall n \in \mathbb{N}, u_{n} = \sum_{i=1}^{p} u_{n}^{(i)} e_{i}.
$$
Les suites scalaires $(u_{n}^{(i)})$ ainsi définies sont appelées les suites coordonnées de $u$ dans la base $\mathscr{B}$. Alors :
$$\lim_{n \to +\infty} u_{n} = \ell \iff \forall i \in 〚 1 ; p 〛, \lim_{n \to +\infty} u_{n}^{(i)} = \ell_{i}.
$$
Par conséquent, l'étude de la convergence d'une suite sur un espace vectoriel normé de dimension finie peut se ramener à l'étude de la convergence de ses suites coordonnées dans une base.</p>
<h3>Théorème de Bolzano-Weierstrass</h3>
<p>Toute suite bornée d'un espace vectoriel normé de dimension finie possède une valeur d'adhérence.<br>
$\checkmark$ Ce résultat tombe en défaut si $E$ n'est pas de dimension finie : considérer le $\mathbb{C}$-espace vectoriel $E = \mathscr{C}([0 ; 2 \pi], \mathbb{C})$ muni de la norme $\|\cdot\|_{\infty}$ et la suite de fonctions $f_{n}: t \mapsto e^{i n t}$ (exemple détaillé page 380).</p>
<h2>[S7.5] Topologie d'un espace vectoriel normé</h2>
<p>Soit $(E, \|\cdot\|)$ un espace vectoriel normé.</p>
<h4>Ouverts</h4>
<p>Une partie $\Omega$ de $E$ s'appelle un ouvert si :</p>
<ul>
<li>soit : $\Omega = \emptyset$;</li>
<li>soit : pour tout $a \in \Omega$, il existe $r > 0$ tel que $B(a, r) \subset \Omega$.</li>
</ul>
<h4>Propriétés</h4>
<ul>
<li>$\emptyset$ et $E$ sont des ouverts.</li>
<li>Une boule ouverte est un ouvert.</li>
<li>La réunion d'une famille quelconque d'ouverts est un ouvert.</li>
<li>L'intersection d'une famille finie d'ouverts est un ouvert.</li>
</ul>

<!-- Image omitted: image0010094 -->
<!-- Page 90 -->
<h3>Voisinages</h3>
<p>Soit $a \in E$. On dit qu'une partie $V$ de $E$ est un voisinage de $a$ s'il existe $r > 0$ tel que $B(a,r) \subset V$.<br>
On note $\mathcal{V}(a)$ l'ensemble de voisinages de $a$.<br>
Une partie $\Omega$ de $E$ est un ouvert si et seulement si $\Omega$ est voisinage de tous ses points.<br>
Dans le cas particulier $E = \mathbb{R}$, on définit les voisinages de $+\infty$ (respectivement $-\infty$) comme les parties de $\mathbb{R}$ contenant un intervalle de la forme $]a ; +\infty[$ (respectivement $]-\infty ; a[$).</p>
<h3>Fermés</h3>
<p>Soit $A \subset E$. On dit que $A$ est un fermé de $E$ (ou une partie fermée de $E$) si le complémentaire $C_E A$ de $A$ dans $E$ est un ouvert.</p>
<h4>Propriétés</h4>
<ul>
<li>$\emptyset$ et $E$ sont des fermés (on peut démontrer que ce sont les seules parties à la fois ouvertes et fermées de $E$).</li>
<li>Une boule fermée, une sphère, sont des fermés.</li>
<li>L'intersection d'une famille quelconque de fermés est un fermé.</li>
<li>La réunion d'une famille finie de fermés est un fermé.</li>
<li>Un sous-espace vectoriel de dimension finie d'un espace vectoriel normé est un fermé.</li>
</ul>
<h3>Points intérieurs à une partie, intérieur</h3>
<p>Soit $A$ une partie non vide de $E$. On dit que $a \in A$ est intérieur à $A$ s'il existe une boule ouverte non vide de centre $a$ qui est incluse dans $A$.<br>
L'ensemble des points intérieurs à $A$ s'appelle l'intérieur de $A$, et se note $\overset{\circ}{A}$ :
$$\overset{\circ}{A} = \{ a \in A \mid \exists r > 0, B(a,r) \subset A \}.
$$</p>
<h4>Propriétés</h4>
<ul>
<li>$a \in \overset{\circ}{A}$ si et seulement si $A$ est un voisinage de $a$.</li>
<li>$\overset{\circ}{A} \subset A$ et $\overset{\circ}{A} = A \Longleftrightarrow A$ est un ouvert.</li>
<li>$\overset{\circ}{A}$ est le plus grand ouvert inclus dans $A$.</li>
<li>L'intérieur d'une boule fermée est la boule ouverte de même centre et de même rayon.</li>
</ul>
<h3>Points adhérents à une partie, adhérence</h3>
<p>Soit $A$ une partie non vide de $E$. On dit que $x \in E$ est un point adhérent à $A$ si toute boule ouverte non vide de centre $x$ contient au moins un élément de $A$.<br>
L'ensemble des points adhérents à $A$ s'appelle l'adhérence de $A$ et se note $\bar{A}$ :
$$x \in \bar{A} \Longleftrightarrow \forall r > 0, B(x,r) \cap A \neq \emptyset \Longleftrightarrow \forall r > 0, \exists a \in A, d(x,a) < r.
$$</p>
<h4>Propriétés</h4>
<ul>
<li>$x \in \bar{A} \Longleftrightarrow d(x,A) = 0$.</li>
</ul>

<!-- Image omitted: image0010095 -->
<!-- Page 91 -->
<ul>
<li>$A \subset \bar{A}$ et $A = \bar{A} \Longleftrightarrow A$ est un fermé.</li>
<li>$\bar{A}$ est le plus petit fermé contenant $A$.</li>
<li>$\complement_E \bar{A} = \mathring{\complement_E A}$ et $\complement_E \mathring{A} = \overline{\complement_E A}$. (Note : $\complement_E$ désigne le complémentaire dans $E$)</li>
<li>L'adhérence d'une boule ouverte est la boule fermée de même centre et de même rayon.</li>
</ul>
<h3>Caractérisation séquentielle des points adhérents à une partie</h3>
<p>Soit $A \subset E$, et soit $x \in E$. Alors $x$ est un point adhérent à $A$ si et seulement si il existe une suite $(a_n)_{n \in \mathbb{N}}$ d'éléments de $A$ qui converge vers $x$.<br>
L'adhérence de $A$ est donc l'ensemble des limites des suites convergentes d'éléments de $A$.</p>
<h3>Caractérisation séquentielle d'un fermé</h3>
<p>Une partie $A$ de $E$ est un fermé si et seulement si toute suite d'éléments de $A$ qui converge dans $E$ converge dans $A$.</p>
<h3>Partie dense</h3>
<p>Soit $A$ une partie d'un espace vectoriel normé $E$. On dit que $A$ est dense dans $E$ si $\bar{A} = E$.<br>
Cela équivaut à dire que tout élément de $E$ est limite d'une suite d'éléments de $A$, ou encore que toute boule de $E$ rencontre $A$.</p>
<h4>Exemples</h4>
<ul>
<li>$\mathbb{Q}$ et $\complement_{\mathbb{R}} \mathbb{Q}$ sont denses dans $\mathbb{R}$.<br>
Cela signifie que tout réel est limite d'une suite de nombres rationnels ou d'une suite de nombres irrationnels; ce résultat a été démontré en $1^{re}$ année.</li>
<li>L'ensemble $\text{GL}_n(\mathbb{K})$ des matrices carrées inversibles d'ordre $n$ est dense dans $\mathcal{M}_n(\mathbb{K})$. Ce résultat important est démontré en [SF7.34] page 394.</li>
</ul>
<p>> Pour ces exemples, il n'est pas utile de préciser la norme utilisée, puisque toutes les normes sur un espace vectoriel de dimension finie sont équivalentes. Cependant, pour les démonstrations, certaines normes peuvent être plus pratiques que d'autres.</p>
<h3>Frontière</h3>
<p>Soit $A \subset E$. On appelle frontière de $A$ l'ensemble $\partial A = \bar{A} \setminus \mathring{A}$. La frontière de $A$ est donc l'ensemble des points de $E$ adhérents à $A$ et non intérieurs à $A$. On peut aussi définir la frontière de $A$ par : $\partial A = \bar{A} \cap \overline{\complement_E A}$.<br>
La frontière de $A$ est donc l'ensemble des points adhérents à la fois à $A$ et au complémentaire de $A$.</p>
<h3>Normes équivalentes</h3>
<p>Toutes les notions topologiques définies ci-dessus dépendent a priori du choix initial de la norme $\|\cdot\|$ sur $E$.<br>
Cependant, si deux normes $N_1$ et $N_2$ sont équivalentes, les parties ouvertes pour l'une le sont aussi pour l'autre. Il en résulte que toutes les autres notions (partie fermée, intérieur, adhérence etc...) sont aussi les mêmes pour les deux normes.<br>
En particulier, si $E$ est de dimension finie, l'équivalence des normes assure que la topologie est indépendante de la norme choisie.</p>

<!-- Image omitted: image0010096 -->
<!-- Page 92 -->
<h3>Topologie induite</h3>
<p>Soit $A$ une partie non vide d'un espace vectoriel normé $E$.</p>
<ul>
<li>Soit $a \in A$. On appelle voisinage de $a$ relatif à $A$ l'intersection avec $A$ d'un voisinage de $a$ dans $E$.</li>
<li>On appelle ouvert relatif (respectivement fermé relatif) de $A$ l'intersection avec $A$ d'un ouvert de $E$ (respectivement d'un fermé de $E$).</li>
<li><strong>Caractérisation séquentielle d'un fermé relatif</strong><br>
Une partie $F$ de $A$ est un fermé relatif à $A$ si et seulement si toute suite d'éléments de $F$ qui converge dans $A$ converge dans $F$.</li>
</ul>
<h2>[S7.6] Compacité</h2>
<ul>
<li>Une partie $K$ d'un espace vectoriel normé $(E, \|\cdot\|)$ est dite compacte si de toute suite d'éléments de $K$On peut extraire une suite convergente vers un élément de $K$, autrement dit si toute suite d'éléments de $K$ admet une valeur d'adhérence dans $K$.</li>
<li><strong>Propriétés</strong>
<ul>
<li>Si $K$ est une partie compacte, alors $K$ est une partie fermée et bornée.</li>
<li>Si $K$ est un compact, et si $F$ est un fermé inclus dans $K$, alors $F$ est compact.</li>
<li>Toute suite d'éléments d'une partie compacte $K$ qui admet une seule valeur d'adhérence $\ell$ est convergente dans $K$, de limite $\ell$ (ce résultat est démontré en [SF7.5] page 372).</li>
<li>Un produit d'une famille finie de compacts est un compact (dans l'espace vectoriel normé produit).</li>
<li>Si $E$ est un espace vectoriel normé de <em>dimension finie</em>, une partie de $E$ est compacte si et seulement si elle est fermée et bornée.</li>
</ul>
</li>
<li>Un exemple de partie fermée bornée non compacte est donné en [SF7.15] page 380.</li>
</ul>
<h2>[S7.7] Limites</h2>
<p>$(E, \|\cdot\|_E)$ et $(F, \|\cdot\|_F)$ sont ici deux espaces vectoriels normés, et $A$ désigne une partie non vide de $E$.</p>
<ul>
<li>Soit $f$ une application de $A$ dans $F$, et soit $a \in \bar{A}$ un point adhérent à $A$. On dit que $f$ admet une limite en $a$ s'il existe $\ell \in F$ tel que :
$$
\forall \epsilon > 0, \exists \alpha > 0, \forall x \in A, \|x - a\|_E \leq \alpha \implies \|f(x) - \ell\|_F \leq \epsilon
$$
ou bien, en termes de voisinages :
$$
\forall V \in \mathcal{V}(\ell), \exists U \in \mathcal{V}(a) \text{ tel que } f(U \cap A) \subset V.
$$
L'intérêt de la définition faisant intervenir des voisinages est qu'elle s'applique aussi au cas où $a = \pm \infty$ lorsque $E = \mathbb{R}$ et au cas $\ell = \pm \infty$ lorsque $F = \mathbb{R}$. Si $f$ admet une limite en $a$, alors le vecteur $\ell$ introduit dans la définition est unique. On dit alors que $\ell$ est la limite de $f$ en $a$, et on note :
$$
\ell = \lim_{x \to a} f(x).
$$</li>
</ul>

<!-- Image omitted: image0010097 -->
<!-- Page 93 -->
<h3>Caractérisation séquentielle de la limite</h3>
<p>Soit $f$ une application de $A$ dans $F$, soit $a \in \bar{A}$ et soit $\ell \in F$.<br>
Les propriétés suivantes sont équivalentes :</p>
<ol type="i">
<li>$\lim_{x \to a} f(x) = \ell$,</li>
<li>pour toute suite $(x_n)_{n \in \mathbb{N}}$ d'éléments de $A$ qui converge vers $a$, la suite $(f(x_n))_{n \in \mathbb{N}}$ converge vers $\ell$.</li>
</ol>
<h3>Opérations algébriques sur les limites</h3>
<p>Soit $f$ et $g$ deux applications de $A$ dans $F$ et $\varphi$ une application de $A$ dans $\mathbb{K}$. Soit $a \in \bar{A}$.</p>
<ul>
<li>Si $\lim_{x \to a} f(x) = \ell$ et $\lim_{x \to a} g(x) = \ell'$ existent, alors $\lim_{x \to a} (f + g)(x)$ existe et est égale à $\ell + \ell'$.</li>
<li>Si $\lim_{x \to a} f(x) = \ell$ et $\lim_{x \to a} \varphi(x) = \lambda (\in \mathbb{K})$ existent, alors $\lim_{x \to a} (\varphi f)(x)$ existe et est égale à $\lambda \ell$.</li>
</ul>
<h3>Limite d'une composée</h3>
<p>Soient $E$, $F$ et $G$ trois espaces vectoriels normés, $f: A \subset E \to F$ et $g: B \subset F \to G$ deux applications.<br>
On suppose $f(A) \subset B$, et $\lim_{x \to a} f(x) = b$.<br>
Alors $b \in \overline{B}$, et si $\lim_{y \to b} g(y) = \ell \in G$, alors $\lim_{x \to a} (g \circ f)(x) = \ell$.</p>
<h3>Cas de la dimension finie</h3>
<p>On suppose ici que $f$ est une application définie sur une partie $A$ d'un espace vectoriel normé $E$, à valeurs dans un espace vectoriel normé $F$ de dimension finie. Soit $\mathscr{B} = (e_1, \ldots, e_p)$ une base de $F$.<br>
On note $(f_1, \ldots, f_p)$ les applications coordonnées de $f$ dans la base $\mathscr{B}$, c'est-à-dire :
$$\forall x \in A, f(x) = \sum_{i=1}^p f_i(x) e_i \quad \text{avec} \quad f_i : A \to \mathbb{K}.$$
Soit $a \in \bar{A}$. Pour que $f$ admette une limite $\ell = \sum_{i=1}^p \ell_i e_i \in F$ quand $x$ tend vers $a$, il faut et il suffit que, pour tout $i \in 〚 1 ; p 〛$, $f_i$ admette une limite dans $\mathbb{K}$ quand $x$ tend vers $a$, et on a alors : $\lim_{x \to a} f_i(x) = \ell_i$.</p>
<h2>[S7.8] Continuité</h2>
<p>Soient $(E, \| \cdot \|_E)$ et $(F, \| \cdot \|_F)$ deux espaces vectoriels normés, et $D$ une partie non vide de $E$.</p>
<ul>
<li>Soit $f$ une application de $D$ dans $F$, et soit $a$ appartenant à $D$. On dit que $f$ est continue en $a$ si $f$ admet une limite en $a$ (qui vaut alors nécessairement $f(a)$), c'est-à-dire :
$$\forall \varepsilon > 0, \exists \alpha > 0, \forall x \in D, \|x - a\|_E \leq \alpha \implies \|f(x) - f(a)\|_F \leq \varepsilon.$$
On dit que $f$ est continue sur $D$ si $f$ est continue en tout point de $D$. On note $\mathscr{C}(D, F)$ l'ensemble des applications continues sur $D$ à valeurs dans $F$.</li>
</ul>

<!-- Image omitted: image0010098 -->
<!-- Page 94 -->
<ul>
<li>Les notions de limite et de continuité sont inchangées si l'on remplace les normes par des normes équivalentes. Lorsque $E$ et $F$ sont de dimensions finies, ces notions sont donc indépendantes des normes choisies.<br>
En revanche, si deux normes ne sont pas équivalentes, une application peut être continue au sens d'une norme et pas de l'autre.</li>
<li><strong>Caractérisation séquentielle de la continuité</strong><br>
Soit $f$ une application de $D$ dans $F$, et $a \in D$.<br>
$f$ est continue en $a$ si et seulement si pour toute suite $(x_n)$ d'éléments de $D$ qui converge vers $a$, la suite $(f(x_n))$ converge vers $f(a)$.</li>
<li><strong>Opérations algébriques</strong><br>
Soient $f$ et $g$ deux applications de $D$ dans $F$ et $\varphi$ une application de $D$ dans $\mathbb{K}$. Soit $a \in D$.
<ul>
<li>Si $f$ et $g$ sont continues en $a$, alors $f + g$ est continue en $a$.</li>
<li>Si $f$ et $\varphi$ sont continues en $a$, l'application $\varphi \cdot f$ est continue en $a$.</li>
</ul>
En particulier, $\mathscr{C}(D, F)$ est un sous-espace vectoriel de $\mathcal{A}(D, F)$ et $\mathscr{C}(D, \mathbb{K})$ est une sous-algèbre de $\mathcal{A}(D, \mathbb{K})$.</li>
<li><strong>Composée d'applications continues</strong><br>
Soient $E$, $F$ et $G$ trois espaces vectoriels normés, $D$ une partie de $E$ et $\Delta$ une partie de $F$.<br>
Soit $f$ une application de $D$ dans $F$ telle que $f(D) \subset \Delta$, et $g$ une application de $\Delta$ dans $G$.
<ul>
<li>Si $f$ est continue en $a \in D$ et si $g$ est continue en $f(a)$, $g \circ f$ est continue en $a$.</li>
<li>Si $f \in \mathscr{C}(D, F)$ et $g \in \mathscr{C}(\Delta, G)$, alors $g \circ f \in \mathscr{C}(D, G)$. (Note: Correction $\mathscr{C}(D, \Delta)$ à $\mathscr{C}(D, F)$ pour correspondre au type de fonction)</li>
</ul>
</li>
<li><strong>Unicité du prolongement par continuité</strong><br>
Soient $E$ et $F$ deux espaces vectoriels normés et $D$ une partie de $E$. Soient $f$ et $g$ deux applications continues de $D$ dans $F$. Soit $A$ une partie dense dans $D$ (c'est-à-dire $\bar{A} = D$).<br>
Si $f$ et $g$ coïncident sur $A$, alors $f = g$.</li>
<li><strong>Image réciproque d'un ouvert ou d'un fermé</strong><br>
Soient $E$ et $F$ deux espaces vectoriels normés, et $f$ une application d'une partie $D$ de $E$ dans $F$. Les propositions suivantes sont équivalentes :
<ol type="i">
<li>$f$ est continue sur $D$,</li>
<li>l'image réciproque par $f$ de tout ouvert de $F$ est un ouvert relatif de $D$,</li>
<li>l'image réciproque par $f$ de tout fermé de $F$ est un fermé relatif de $D$.</li>
</ol>
<strong>Exemples</strong>
<ul>
<li>Soient $f, g : \mathbb{R} \to \mathbb{R}$, continues. Alors :
$$
\{x \in \mathbb{R} \mid f(x) < g(x)\} \text{ est un ouvert de } \mathbb{R}.
$$
$$
\{x \in \mathbb{R} \mid f(x) \leq g(x)\} \text{ est un fermé de } \mathbb{R}.
$$
(En effet, le premier ensemble est l'image réciproque par l'application continue $f - g$ de l'ouvert $]-\infty ; 0[$ et le second est l'image réciproque du fermé $]-\infty ; 0]$).</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010099 -->
<!-- Page 95 -->
<p>Soit $f \in \mathscr{C}(\mathbb{R}, \mathbb{R})$ et soient
$$\Gamma = \left\{ (x, y) \in \mathbb{R}^2 \mid y = f(x) \right\} \text{ le graphe de } f,
$$
$$\mathscr{E} = \left\{ (x, y) \in \mathbb{R}^2 \mid y > f(x) \right\} \text{ l'épigraphe strict de } f.
$$
Alors $\Gamma$ est une partie fermée de $\mathbb{R}^2$ et $\mathscr{E}$ est une partie ouverte de $\mathbb{R}^2$.<br>
(En effet, il suffit ici de considérer l'application continue $(x, y) \mapsto y - f(x)$ de $\mathbb{R}^2$ dans $\mathbb{R}$. $\Gamma$ est l'image réciproque du fermé $\{0\}$, et $\mathscr{E}$ celle de l'ouvert $]0 ; +\infty[$.)</p>
<p>L'ensemble $\text{GL}_n(\mathbb{K})$ des matrices carrées inversibles d'ordre $n$ est un ouvert de $\mathcal{M}_n(\mathbb{K})$.<br>
(En effet, c'est l'image réciproque de l'ouvert $\mathbb{K} \setminus \{0\}$ par l'application « déterminant », qui est continue de $\mathcal{M}_n(\mathbb{K})$ dans $\mathbb{K}$).</p>
<h3>Cas de la dimension finie</h3>
<p>On suppose ici que $F$ est de dimension finie. On munit $F$ d'une base $\mathscr{B} = (e_1, \ldots, e_p)$. Soit $f \in \mathcal{A}(D, F)$, et $(f_1, \ldots, f_p)$ les applications coordonnées de $f$ dans la base $\mathscr{B}$. Alors :<br>
$f$ est continue en $a \in D$ si et seulement si pour tout $i \in 〚 1 ; p 〛$ $f_i$ est continue en $a$. (Note: Correction de n à p)</p>
<h3>Continuité uniforme</h3>
<p>Soient $E$ et $F$ deux espaces vectoriels normés, $D$ une partie non vide de $E$, $f$ une application de $D$ dans $F$.<br>
On dira que $f$ est uniformément continue sur $D$ si et seulement si :
$$\forall \epsilon > 0, \exists \alpha > 0, \forall (x, y) \in D^2, \|x - y\|_E < \alpha \implies \|f(x) - f(y)\|_F < \epsilon.
$$
La différence entre cette notion (qui est globale) et la notion de continuité en un point (notion locale) est que, dans la propriété ci-dessus, $\alpha$ ne dépend que de $\epsilon$, et non pas de $x$Ou $y$.<br>
Si $f$ est uniformément continue sur $D$, alors $f$ est continue sur $D$.<br>
La réciproque de cette propriété est fausse : il existe des fonctions continues qui ne sont pas uniformément continues, comme le montre l'exemple de $x \mapsto x^2$ sur $\mathbb{R}$.</p>
<h3>Théorème de Heine</h3>
<p>Soient $E$ et $F$ deux espaces vectoriels normés, $K$ un compact de $E$, et $f : K \to F$ une application continue sur $K$.<br>
Alors $f$ est uniformément continue sur $K$.</p>
<h3>Fonctions lipschitziennes</h3>
<p>Soit $f$ une application de $D$ dans $F$. On dit que $f$ est lipschitzienne sur $D$ s'il existe un réel $k$ (positif) tel que :
$$\forall (x, y) \in E^2, \|f(x) - f(y)\|_F \leq k \|x - y\|_E.
$$
On dit alors que $f$ est lipschitzienne de rapport $k$.</p>

<!-- Image omitted: image0010100 -->
<!-- Page 96 -->
<ul>
<li>Si $f$ est lipschitzienne sur $D$, alors $f$ est uniformément continue sur $D$.<br>
$\checkmark$ La réciproque de cette propriété est fausse : par exemple, la fonction $x \mapsto \sqrt{x}$ est uniformément continue sur $\mathbb{R}_+$ (cela est démontré en [SF7.30] page 391), sans être lipschitzienne sur $\mathbb{R}_+$.</li>
<li><strong>Continuité et compacité</strong>
<ul>
<li>Soit $K$ une partie compacte d'un espace vectoriel normé, et $f$ une application continue de $K$ dans un espace vectoriel normé $F$. Alors $f(K)$ est une partie compacte de $F$.</li>
<li>Cela entraîne que si $f$ est continue sur $K$, alors $f$ est bornée et $\|f\|$ admet sur $K$ un maximum et un minimum.</li>
<li>Soit $f$ une application continue d'un compact $A$ de $E$ à valeurs dans $F$, et soit $B = f(A)$. Si $f$ est bijective de $A$ sur $B$, alors $f^{-1}$ est continue sur $B$.</li>
</ul>
</li>
<li><strong>Caractérisation des applications linéaires continues</strong><br>
Soient $E$ et $F$ deux espaces vectoriels normés, et $u \in \mathcal{L}(E, F)$. Les propriétés suivantes sont équivalentes :
<ol type="i">
<li>$u$ est continue,</li>
<li>$u$ est continue en $0_E$,</li>
<li>$u$ est bornée sur la boule unité fermée $B_f(0,1) = \{x \in E \mid \|x\|_E \leq 1\}$,</li>
<li>il existe un réel $k \geq 0$ tel que pour tout $x \in E$, $\|u(x)\|_F \leq k \|x\|_E$,</li>
<li>$u$ est lipschitzienne,</li>
<li>$u$ est uniformément continue.</li>
</ol>
</li>
<li><strong>Caractérisation des applications multilinéaires continues</strong><br>
La définition d'une application $p$-linéaire a été donnée en [S3.14].<br>
Soient $(E_i)_{1 \leq i \leq p}$ $p$ espaces vectoriels normés, muni chacun d'une norme notée $\|\cdot\|_i$.<br>
On munit l'espace vectoriel produit $E = \prod_{i=1}^{p} E_i$ de la norme $\|\cdot\|_{\infty}$ définie par :
$$\forall x = (x_1, \ldots, x_p) \in E, \quad \|x\|_{\infty} = \max_{1 \leq i \leq p} \|x_i\|_i.
$$
Soit $F$ un espace vectoriel normé, et $u : E \to F$ une application $p$-linéaire. Alors, les propriétés suivantes sont équivalentes :
<ol type="i">
<li>$u$ est continue,</li>
<li>$u$ est continue en $0_E$,</li>
<li>$u$ est bornée sur le produit $B_1 \times \ldots \times B_p$, où $B_i$ est la boule unité fermée de $E_i$ : $B_i = \{x_i \in E_i \mid \|x_i\|_i \leq 1\}$,</li>
<li>il existe un réel $k \geq 0$ tel que, pour tout $x = (x_1, \ldots, x_p) \in E$, on ait
$$\|u(x_1, \ldots, x_p)\|_F \leq k \|x_1\|_1 \cdot \|x_2\|_2 \cdots \|x_p\|_p.
$$</li>
</ol>
</li>
</ul>

<!-- Image omitted: image0010101 -->
<!-- Page 97 -->
<h3>Exemples fondamentaux d'applications continues</h3>
<ul>
<li>Si $E$ est de dimension finie, alors toute application linéaire $u: E \to F$ est continue sur $E$.</li>
<li>Si $E_1, \ldots, E_p$ sont des espaces vectoriels normés de dimensions finies, toute application $p$-linéaire sur $E_1 \times \cdots \times E_p$ est continue.</li>
<li>Toute application polynomiale $\varphi: \mathbb{K}^n \to \mathbb{K}$ est continue.</li>
<li>L'application $\det: \mathcal{M}_n(\mathbb{K}) \to \mathbb{K}$ est continue.</li>
</ul>
<h2>[S7.9] Parties connexes par arcs</h2>
<ul>
<li>Soit $E$ un espace vectoriel normé, et $a, b \in E$. Un chemin continu reliant $a$ à $b$ est une application $\gamma: [0; 1] \to E$, continue et telle que $\gamma(0) = a$ et $\gamma(1) = b$.<br>
On appelle support de $\gamma$ l'ensemble image $\gamma([0; 1]) = \{\gamma(t), t \in [0; 1]\}$.</li>
<li>Soit $A$ une partie non vide de $E$. La relation binaire $\mathcal{R}$ définie sur $A^2$ par :
$$
a \mathcal{R} b \iff \text{il existe un chemin continu reliant } a \text{ à } b \text{ et dont le support est inclus dans } A
$$
est une relation d'équivalence.<br>
Ses classes d'équivalence s'appellent les composantes connexes par arcs de $A$.</li>
<li>$A$ est dite connexe par arcs si elle possède une seule composante connexe, c'est-à-dire si pour tous $a, b \in A$ il existe un chemin continu reliant $a$ à $b$ et dont le support est inclus dans $A$.</li>
</ul>
<h3>Exemples</h3>
<ul>
<li>Toute partie convexe de $E$ est connexe par arcs.</li>
<li>Une partie $A$ est dite étoilée par rapport à un de ses points $\omega$ si pour tout $a \in A$ le segment $[\omega; a]$ est inclus dans $A$ (une partie convexe est donc une partie qui est étoilée par rapport à tous ses points).<br>
Si $A$ est étoilée par rapport à l'un de ses points, elle est connexe par arcs.</li>
<li>Les parties connexes par arcs de $\mathbb{R}$ sont les intervalles.</li>
</ul>
<h3>Image par une application continue</h3>
<p>Soient $E$ et $F$ deux espaces vectoriels normés, $A$ une partie non vide de $E$ et $f: A \to F$ une application continue.<br>
Si $A$ est une partie connexe par arcs de $E$, son image $f(A)$ est une partie connexe par arcs de $F$.</p>
<h3>Théorème des valeurs intermédiaires</h3>
<p>Soit $E$ un espace vectoriel normé, $A$ une partie connexe par arcs de $E$ et $f$ une application continue de $A$ dans $\mathbb{R}$.<br>
Alors $f(A)$ est un intervalle de $\mathbb{R}$; autrement dit, si $f$ prend deux valeurs réelles $x \leq y$ sur $A$, pour tout $t \in [x; y]$ il existe $a \in A$ tel que $f(a) = t$.</p>

<!-- Image omitted: image0010102 -->
<!-- Page 98 -->
<h1>Thème 8 - Suites numériques</h1>
<p>Les suites numériques ont été étudiées en première année, et nous avons vu dans le chapitre précédent les principales définitions et résultats concernant plus généralement les suites à valeurs vectorielles.<br>
Cependant il nous a semblé important d'introduire un bref chapitre de révisions à ce sujet compte tenu de l'importance qu'ont les suites numériques dans tout le reste du programme. De plus, il existe des résultats spécifiques aux suites à valeurs réelles, liés à la relation d'ordre sur $\mathbb{R}$.</p>
<h2>[S8.1] Limite d'une suite réelle</h2>
<p>Soit $(u_n)_{n \in \mathbb{N}}$ une suite à valeurs réelles.</p>
<ul>
<li>On dit que la suite $u$ admet le réel $\ell$ pour limite si :
$$\forall \varepsilon > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, |u_n - \ell| < \varepsilon.
$$</li>
<li>On dit que la suite $u$ tend vers $+\infty$ (respectivement $-\infty$) si :
$$\forall A > 0, \exists n_0 \in \mathbb{N}, \forall n \geq n_0, u_n > A \text{ (respectivement } u_n < -A).
$$</li>
</ul>
<p>√ Dire qu'une suite $(u_n)_{n \in \mathbb{N}}$ est convergente signifie que $\lim_{n \to +\infty} u_n = \ell$Où $\ell$ est un réel fini. Une suite qui tend vers $\pm \infty$ est divergente.</p>
<h2>[S8.2] Composition des limites</h2>
<p>Soit $f$ une fonction numérique et $(u_n)_{n \in \mathbb{N}}$ une suite réelle telle que $u_n$ appartienne au domaine de définition de $f$ au moins à partir d'un certain rang.</p>
<ul>
<li>Si $\lim_{n \to +\infty} u_n = a$ et si $\lim_{x \to a} f(x) = \ell$, alors $\lim_{n \to +\infty} f(u_n) = \ell$.</li>
<li>Si $\lim_{n \to +\infty} u_n = a$ et si $f$ est continue en $a$, alors $\lim_{n \to +\infty} f(u_n) = f(a)$.</li>
</ul>
<h2>[S8.3] Propriétés des suites réelles liées à l'ordre</h2>
<ul>
<li><strong>Prolongement des inégalités (cas de la convergence)</strong><br>
Soit $(u_n)_{n \in \mathbb{N}}$ et $(v_n)_{n \in \mathbb{N}}$ deux suites réelles convergeant respectivement vers $\ell$ et $\ell'$.
<ul>
<li>Si $\ell < \ell'$, il existe un entier $n_0$ tel que $\forall n \in \mathbb{N}, n \geq n_0 \implies u_n < v_n$.</li>
<li>S'il existe un entier $n_0$ tel que $\forall n \in \mathbb{N}, n \geq n_0 \implies u_n \leq v_n$, alors $\ell \leq \ell'$.</li>
</ul>
√ Pour appliquer cette propriété, il faut préalablement démontrer l'existence des limites.<br>
√ Si pour tout $n \geq n_0$On a $u_n < v_n$, on peut quand même avoir $\ell = \ell'$. Le passage à la limite affaiblit la relation d'ordre.</li>
</ul>

<!-- Image omitted: image0010003 -->
<!-- Page 99 -->
<h3>Prolongement des inégalités (cas de la divergence)</h3>
<p>Soit $(u_n)_{n \in \mathbb{N}}$ et $(v_n)_{n \in \mathbb{N}}$ deux suites réelles telles que :
$$\exists n_0 \in \mathbb{N} \text{ tel que } \forall n \in \mathbb{N}, n \geq n_0 \implies u_n \leq v_n.
$$
Alors on a :
$$\lim_{n \to +\infty} u_n = +\infty \implies \lim_{n \to +\infty} v_n = +\infty
$$
et
$$\lim_{n \to +\infty} v_n = -\infty \implies \lim_{n \to +\infty} u_n = -\infty.
$$</p>
<h3>Théorème d'encadrement</h3>
<p>Soit $(u_n)_{n \in \mathbb{N}}$, $(v_n)_{n \in \mathbb{N}}$ et $(w_n)_{n \in \mathbb{N}}$ trois suites réelles, telles que :
$$\exists n_0 \in \mathbb{N} \text{ tel que } \forall n \in \mathbb{N}, n \geq n_0 \implies u_n \leq v_n \leq w_n
$$
et
$$\lim_{n \to +\infty} u_n = \lim_{n \to +\infty} w_n = \ell.
$$
Alors la suite $(v_n)_{n \in \mathbb{N}}$ est convergente, et $\lim_{n \to +\infty} v_n = \ell$.</p>
<h3>Théorème de la limite monotone</h3>
<p>Soit $(u_n)$ une suite de réels croissante (respectivement décroissante), au moins à partir d'un certain rang.<br>
Alors $(u_n)$ est convergente si et seulement si elle est majorée (respectivement minorée).<br>
(On peut préciser : si $(u_n)$ est croissante non majorée, alors $\lim_{n \to +\infty} u_n = +\infty$).</p>
<h2>[S8.4] Suites adjacentes</h2>
<h3>Définition</h3>
<p>Soit $(u_n)_{n \in \mathbb{N}}$ et $(v_n)_{n \in \mathbb{N}}$ deux suites réelles.<br>
On dit que ces deux suites sont adjacentes si :
$$\begin{cases}
(u_n) \text{ est croissante} \\
(v_n) \text{ est décroissante} \\
\lim_{n \to +\infty} (v_n - u_n) = 0
\end{cases}
$$</p>
<h3>Théorème</h3>
<p>Deux suites adjacentes sont convergentes, vers la même limite.</p>
<ul>
<li>On a de plus : $\forall (p, q) \in \mathbb{N}^2, u_p \leq \ell \leq v_q$ (avec inégalités strictes si les deux suites sont strictement monotones).<br>
Cette remarque est importante car elle permet d'obtenir facilement un encadrement de $\ell$.</li>
</ul>

<!-- Image omitted: image0010104 -->
<!-- Page 100 -->
<h2>[S8.5] Suites arithmético-géométriques</h2>
<p>On considère ici l'ensemble des suites $(u_n)_{n \in \mathbb{N}}$ à valeurs dans $\mathbb{K} = \mathbb{R}$Ou $\mathbb{C}$ vérifiant une relation de récurrence de la forme :
$$\forall n \in \mathbb{N}, u_{n+1} = a u_n + b
$$
avec $a, b \in \mathbb{K}$.</p>
<ul>
<li>Si $a = 1$, la suite est tout simplement arithmétique de raison $b$ et l'on a alors $u_n = u_0 + n b$ pour tout entier $n$.</li>
<li>Si $a \neq 1$, la suite constante égale à $\ell = \frac{b}{1-a}$ vérifie bien la relation de récurrence (il suffit de résoudre l'équation $\ell = a \ell + b$). Si l'on pose alors $v_n = u_n - \ell$ pour tout $n$, on a :
$$v_{n+1} = u_{n+1} - \ell = (a u_n + b) - (a \ell + b) = a (u_n - \ell) = a v_n,
$$
de sorte que la suite $(v_n)_{n \in \mathbb{N}}$ est géométrique de raison $a$. On aura donc $v_n = a^n v_0$ pour tout $n$, d'où l'on tire finalement :
$$\forall n \in \mathbb{N}, u_n = v_n + \ell = a^n \left( u_0 - \frac{b}{1-a} \right) + \frac{b}{1-a}.
$$</li>
</ul>
<p>Il ne faut bien sûr pas retenir cette dernière formule par cœur mais seulement la méthode; les calculs sont alors faciles à refaire.<br>
Il faut savoir adapter les résultats dans le cas où la suite n'est définie qu'à partir d'un rang $n_0$.<br>
Pour cela, il suffit de se rappeler que les formules deviennent :</p>
<ul>
<li>$u_n = u_{n_0} + (n - n_0) b$ pour une suite arithmétique de raison $b$;</li>
<li>$v_n = a^{n - n_0} v_{n_0}$ pour une suite géométrique de raison $a$.</li>
</ul>
<h2>[S8.6] Suites récurrentes linéaires d'ordre 2</h2>
<p>On considère ici l'ensemble $\mathcal{S}$ des suites $(u_n)_{n \in \mathbb{N}}$ à valeurs dans $\mathbb{K} = \mathbb{R}$Ou $\mathbb{C}$ qui vérifient une relation de la forme :
$$\forall n \in \mathbb{N}, a u_{n+2} + b u_{n+1} + c u_n = 0 \quad (L)
$$
avec $a, b, c \in \mathbb{K}$, $a \neq 0$.<br>
Il est facile de vérifier que $\mathcal{S}$ est un sous-espace vectoriel de l'espace vectoriel $\mathbb{K}^\mathbb{N}$ des suites à valeurs dans $\mathbb{K}$.<br>
On montre également que l'application $\varphi : \mathcal{S} \rightarrow \mathbb{K}^2$ qui à toute suite $u \in \mathcal{S}$ associe le couple formé par ses deux premiers termes, est linéaire et bijective. $\varphi$ est donc un isomorphisme d'espaces vectoriels et par conséquent, $\dim \mathcal{S} = 2$.<br>
On peut alors chercher des éléments particuliers de $\mathcal{S}$ : une suite géométrique $n \mapsto r^n$ avec $r \in \mathbb{K}^*$ appartenant à $\mathcal{S}$ si et seulement si
$$\forall n \in \mathbb{N}, a r^{n+2} + b r^{n+1} + c r^n = 0 \quad \text{soit} \quad a r^2 + b r + c = 0.$$</p>

<!-- Image omitted: image0010105 -->
<!-- Page 101 -->
<p>L'équation $(E_c) : ar^2 + br + c = 0$ s'appelle l'équation caractéristique de la récurrence.<br>
On étudie alors ses solutions. Il faut distinguer deux cas, selon que le corps de base est $\mathbb{R}$Ou $\mathbb{C}$. En notant $\Delta$ le discriminant de l'équation caractéristique, on a les résultats suivants.</p>
<ul>
<li><strong>Si $\mathbb{K} = \mathbb{C}$</strong>
<ul>
<li>Si $\Delta \neq 0$, $(E_c)$ possède deux racines distinctes $r_1$ et $r_2$; dans ce cas, les suites $n \mapsto r_1^n$ et $n \mapsto r_2^n$ sont deux solutions linéairement indépendantes de $(L)$. L'ensemble des solutions de $(L)$ est donc l'ensemble des suites de la forme :
$$u_n = \lambda_1 r_1^n + \lambda_2 r_2^n, (\lambda_1, \lambda_2) \in \mathbb{C}^2.$$</li>
<li>Si $\Delta = 0$, $(E_c)$ possède une racine double $r$; dans ce cas, les suites $n \mapsto r^n$ et $n \mapsto n r^n$ sont deux solutions linéairement indépendantes de $(L)$. L'ensemble des solutions de $(L)$ est alors l'ensemble des suites de la forme :
$$u_n = (\lambda n + \mu) r^n, (\lambda, \mu) \in \mathbb{C}^2.$$</li>
</ul>
</li>
<li><strong>Si $\mathbb{K} = \mathbb{R}$</strong>
<ul>
<li>Si $\Delta \geq 0$, on a les mêmes résultats que ci-dessus (mais avec ici $\lambda_1, \lambda_2, \lambda$ et $\mu$ réels).</li>
<li>Si $\Delta < 0$, l'équation $(E_c)$ admet deux racines complexes non réelles conjuguées $r e^{\pm i \theta}$, et alors les suites $n \mapsto r^n \cos n \theta$ et $n \mapsto r^n \sin n \theta$ sont deux solutions linéairement indépendantes de $(L)$. L'ensemble des solutions de $(L)$ est ici l'ensemble des suites de la forme :
$$u_n = (\lambda \cos n \theta + \mu \sin n \theta) r^n, (\lambda, \mu) \in \mathbb{R}^2.$$</li>
</ul>
</li>
</ul>
<p>$\checkmark$ Les suites arithmético-géométriques ou les suites récurrentes linéaires peuvent apparaître dans plusieurs types d'exercices (calculs de déterminants, calculs de probabilités...). Il est donc important d'apprendre les méthodes rappelées ci-dessus.</p>

<!-- Image omitted: image0010106 -->
<!-- Page 102 -->
<h1>Thème 9 - Fonctions réelles d'une variable réelle</h1>
<p>Une fonction numérique d'une variable réelle est une application définie sur un intervalle $I$ de $\mathbb{R}$, à valeurs dans $\mathbb{R}$. Avant d'aborder les fonctions à valeurs vectorielles dans le thème suivant, il nous a semblé important de rappeler certains résultats de $1^{re}$ année; ceux-ci sont complétés par la notion de fonction convexe.</p>
<h2>[S9.1] Rappels du cours de $1^{re}$ année</h2>
<h3>Théorème de la limite monotone</h3>
<ul>
<li>Soient $]a ; b[$ un intervalle ouvert (borné ou non) et $f$ une application croissante de $]a ; b[$ dans $\mathbb{R}$. Alors :
<ul>
<li>$f$ admet en tout point $x_0$ de $]a ; b[$ une limite à droite et une limite à gauche, notées respectivement $f(x_0^+)$ et $f(x_0^-)$, telles que $f(x_0^-) \leq f(x_0) \leq f(x_0^+)$.</li>
<li>$f$ admet en $b^-$ une limite, finie ou non; cette limite est finie si et seulement si $f$ est majorée, et dans le cas contraire, elle vaut $+\infty$.</li>
<li>$f$ admet en $a^+$ une limite, finie ou non; cette limite est finie si et seulement si $f$ est minorée, et dans le cas contraire, elle vaut $-\infty$.</li>
</ul>
</li>
<li>Dans le cas où $f$ est décroissante, on a :
<ul>
<li>$f$ admet en tout point $x_0$ de $]a ; b[$ une limite à droite et une limite à gauche, telles que $f(x_0^-) \geq f(x_0) \geq f(x_0^+)$.</li>
<li>$f$ admet en $b^-$ une limite, finie ou non; cette limite est finie si et seulement si $f$ est minorée, et dans le cas contraire, elle vaut $-\infty$.</li>
<li>$f$ admet en $a^+$ une limite, finie ou non; cette limite est finie si et seulement si $f$ est majorée, et dans le cas contraire, elle vaut $+\infty$.</li>
</ul>
</li>
</ul>
<h3>Théorème de la bijection</h3>
<p>Soit $f: I \longrightarrow \mathbb{R}$ une application continue et strictement monotone sur un intervalle $I$ de $\mathbb{R}$.<br>
Alors $J = f(I)$ est un intervalle de $\mathbb{R}$, $f$ est bijective de $I$ sur $J$, et $f^{-1}: J \longrightarrow I$ est continue, strictement monotone, de même sens de variation que $f$.</p>
<h3>Dérivée de la fonction réciproque</h3>
<p>Soit $f: I \longrightarrow J$ une bijection continue de $I$ sur $J$.<br>
Si $f$ est dérivable en $a \in I$ et si $f'(a) \neq 0$, alors $f^{-1}$ est dérivable en $b = f(a)$ et on a :
$$(f^{-1})'(b) = \frac{1}{f'(a)} = \frac{1}{f' \circ f^{-1}(b)}.
$$</p>
<h3>Caractérisation des $\mathcal{C}^k$-difféomorphismes</h3>
<p>Soit $k \in \mathbb{N}^* \cup \{\infty\}$. Soit $f$ une application de classe $\mathcal{C}^k$ d'un intervalle $I$ sur l'intervalle $J = f(I)$, telle que $f'$ ne s'annule pas sur $I$.<br>
Alors $f$ est bijective de $I$ sur $J$, et $f^{-1}$ est de classe $\mathcal{C}^k$ sur $J$.<br>
$\checkmark$ Une telle application s'appelle un $\mathcal{C}^k$-difféomorphisme de $I$ sur $J$.</p>

<!-- Image omitted: image0010107 -->
<!-- Page 103 -->
<ul>
<li>Soit $f$ une fonction numérique définie sur un intervalle $I$, et admettant en un point $t_0$ <em>intérieur</em> à $I$ un extrémum relatif.<br>
Si $f$ est dérivable en $t_0$, alors $f'(t_0) = 0$.</li>
<li>Le fait que $t_0$ est un point intérieur est indispensable! Par exemple, la fonction $f : \begin{cases} [0;1] \longrightarrow [0;1] \\ t \longmapsto t \end{cases}$ admet un minimum en $0$ et un maximum en $1$, mais sa dérivée ne s'annule jamais!</li>
<li><strong>Théorème de Rolle</strong><br>
Soit $f$ une fonction numérique continue sur l'intervalle fermé $[a;b]$ ($a < b$), dérivable sur l'intervalle ouvert $]a;b[$, et telle que $f(a) = f(b)$.<br>
Alors, il existe $c \in ]a;b[$ tel que $f'(c) = 0$.</li>
<li><strong>Théorème des accroissements finis</strong><br>
Soit $f$ une fonction numérique continue sur l'intervalle fermé $[a;b]$ ($a < b$) et dérivable sur l'intervalle ouvert $]a;b[$.<br>
Alors, il existe $c$ appartenant à $]a;b[$ tel que : $f(b) - f(a) = (b - a)f'(c)$.<br>
<em>Interprétation géométrique</em><br>
Si $f$ est une fonction numérique continue sur $[a;b]$ et dérivable sur $]a;b[$, il existe (au moins) un point du graphe de $f$Où la tangente est parallèle à la corde qui joint les points $A$ de coordonnées $(a, f(a))$ et $B$ de coordonnées $(b, f(b))$.</li>
<li><strong>Théorème de prolongement de la dérivée</strong><br>
Soit $f : [a;b] \rightarrow \mathbb{R}$. On suppose que :
<ul>
<li>$f$ est continue sur $[a;b]$;</li>
<li>$f$ est dérivable sur $]a;b[$;</li>
<li>$\lim_{x \rightarrow a^+} f'(x) = \ell \in \overline{\mathbb{R}}$.</li>
</ul>
Alors $\lim_{x \rightarrow a^+} \frac{f(x) - f(a)}{x - a} = \ell$.<br>
Lorsque $\ell$ est finie, on en déduit que $f$ est dérivable en $a^+$ et que $f'_d(a) = \ell$.<br>
Lorsque $\ell = \pm \infty$, la courbe de $f$ admet au point de coordonnées $(a, f(a))$ une tangente « verticale », et $f$ n'est pas dérivable en $a^+$.</li>
</ul>
<h2>[S9.2] Convexité dans un $\mathbb{R}$-espace vectoriel</h2>
<p>Dans ce paragraphe, $E$ désigne un $\mathbb{R}$-espace vectoriel, dont les éléments seront indifféremment appelés points ou vecteurs. Si deux éléments $a$ et $b$ de $E$ sont notés comme des points $A$ et $B$, on définit le vecteur $\overrightarrow{AB}$ par $\overrightarrow{AB} = b - a$.</p>
<ul>
<li><strong>Barycentres</strong><br>
Soit $n \in \mathbb{N}^*$, soient $A_1, \ldots, A_n$ des points de $E$ et $\lambda_1, \ldots, \lambda_n$ des réels tels que $\sum_{i=1}^n \lambda_i \neq 0$. On appelle barycentre du système de points pondérés $(A_i, \lambda_i)_{1 \leq i \leq n}$ l'unique point $G$ de $E$ qui vérifie :
$$\forall O \in E, \overrightarrow{OG} = \frac{1}{\sum_{i=1}^n \lambda_i} \sum_{i=1}^n \lambda_i \overrightarrow{OA_i}.
$$</li>
</ul>

<!-- Image omitted: image0010108 -->
<!-- Page 104 -->
<p>$G$ est aussi l'unique point de $E$ qui vérifie : $\sum_{i=1}^{n} \lambda_i \overrightarrow{GA_i} = \overrightarrow{0}$.</p>
<h3>Propriétés</h3>
<ul>
<li>Le barycentre d'un système de points pondérés $(A_i, \lambda_i)_{1 \leq i \leq n}$ est inchangé si l'on multiplie tous les poids $\lambda_i$ par un réel non nul. Dans la définition de $G$, on peut donc toujours faire en sorte d'avoir $\sum_{i=1}^{n} \lambda_i = 1$.</li>
<li>Le barycentre d'un système de points pondérés $(A_i, \lambda_i)_{1 \leq i \leq n}$ est inchangé si l'on remplace un sous-système de poids total non nul par son barycentre partiel affecté de ce poids total (c'est l'associativité du barycentre).</li>
</ul>
<h3>Caractérisation des parties convexes</h3>
<ul>
<li>Soient $A$, $B$ deux points de $E$. Rappelons que le segment $[A; B]$ est défini par :
$$[A; B] = \{ \lambda A + (1 - \lambda) B, \lambda \in [0; 1] \}$$
Il s'agit donc de l'ensemble des barycentres de $A$ et $B$ à coefficients positifs.</li>
<li>Rappelons également qu'une partie $C$ de $E$ est dite convexe si pour tous points $A$ et $B$ appartenant à $C$, le segment $[A; B]$ est inclus dans $C$.<br>
Puisque l'intersection de parties convexes de $E$, si elle est non vide, est encore une partie convexe de $E$, on peut définir l'enveloppe convexe d'une partie non vide $X \subset E$ comme étant l'intersection de toutes les parties convexes de $E$ contenant $X$; c'est donc le plus petit convexe de $E$ (au sens de l'inclusion) qui contient $X$.</li>
</ul>
<h3>Théorème</h3>
<p>L'enveloppe convexe d'une famille de points $(A_i)_{1 \leq i \leq n}$ est exactement l'ensemble des barycentres des $A_i$ affectés de poids positifs.</p>
<ul>
<li>Il en résulte qu'une partie $C$ de $E$ est convexe si et seulement si le barycentre de toute famille de points pondérés $(A_i, \lambda_i)_{1 \leq i \leq n}$ avec $A_i \in C$ et $\lambda_i \geq 0$ pour tout $i$, appartient encore à $C$.</li>
</ul>
<h2>[S9.3] Fonctions numériques convexes</h2>
<ul>
<li>Une fonction $f: I \rightarrow \mathbb{R}$, où $I$ est un intervalle de $\mathbb{R}$, est dite convexe si :
$$\forall (x, y) \in I^2, \forall \lambda \in [0; 1], f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y).$$
$f$ est dite strictement convexe si $f(\lambda x + (1 - \lambda) y) < \lambda f(x) + (1 - \lambda) f(y)$ dès que $x < y$ et $\lambda \in ]0; 1[$.<br>
$f$ est dite concave si $-f$ est convexe (soit $f(\lambda x + (1 - \lambda) y) \geq \lambda f(x) + (1 - \lambda) f(y)$ pour tous $x, y \in I$ et tout $\lambda \in [0; 1]$).</li>
</ul>
<h3>Inégalité de convexité (Jensen)</h3>
<p>Soit $f: I \rightarrow \mathbb{R}$ une fonction convexe. Alors pour tout $n \in \mathbb{N}^*$, si $\lambda_1, \ldots, \lambda_n$ sont des réels positifs tels que $\sum_{i=1}^{n} \lambda_i = 1$ et si $(x_1, \ldots, x_n)$ sont des éléments de $I$, on a :
$$f \left( \sum_{i=1}^{n} \lambda_i x_i \right) \leq \sum_{i=1}^{n} \lambda_i f(x_i)$$</p>

<!-- Image omitted: image0010109 -->
<!-- Page 105 -->
<p>(en abrégé : l'image du barycentre est inférieur au barycentre des images).<br>
De plus, lorsque $f$ est strictement convexe, il y a égalité si et seulement si tous les $x_i$ sont égaux (lorsque $\lambda_i \in ]0;1[$).</p>
<ul>
<li><strong>Interprétation géométrique</strong>
<ul>
<li>Dire que $f : I \rightarrow \mathbb{R}$ est convexe signifie que, pour tous points $A$ et $B$ de la courbe $C_f$ de $f$, l'arc $\overparen{AB}$ de la courbe est situé au-dessus du segment $[A;B]$ (sur la figure ci-après, le point $M$ est au-dessous du point $C$).</li>
<li>On appelle épigraphe de $f$ l'ensemble :
$$\mathcal{E}_f = \{(x,y) \in \mathbb{R}^2 \mid x \in I \text{ et } y \ge f(x)\}
$$
(cela correspond donc à l'ensemble des points du plan situés au-dessus de la courbe de $f$, en gris sur la figure).<br>
La fonction $f$ est convexe si et seulement si $\mathcal{E}_f$ est une partie convexe de $\mathbb{R}^2$.</li>
<!-- Image omitted: Graphe d'une fonction convexe -->
</ul>
</li>
<li><strong>Inégalité des trois pentes</strong><br>
Soit $f : I \rightarrow \mathbb{R}$. $f$ est convexe sur $I$ si et seulement si :
$$\forall (a,b,c) \in I^3, a < c < b \implies \frac{f(c) - f(a)}{c - a} \le \frac{f(b) - f(a)}{b - a} \le \frac{f(b) - f(c)}{b - c}.
$$Sur la figure ci-dessus, on a :
$$\text{pente}(AM) \le \text{pente}(AB) \le \text{pente}(MB).
$$</li>
</ul>

<!-- Image omitted: image0010110 -->
<!-- Page 106 -->
<h3>Croissance des pentes</h3>
<p>Soit $f: I \rightarrow \mathbb{R}$. $f$ est convexe sur $I$ si et seulement si pour tout $a \in I$ la fonction
$$x \mapsto \frac{f(x) - f(a)}{x - a}$$
est croissante sur $I \setminus \{a\}$.</p>
<ul>
<li>On en déduit que si $f$ est convexe sur $I$, elle est dérivable à droite et à gauche (donc continue) en tout point de $\overset{\circ}{I}$.</li>
</ul>
<h3>Fonctions convexes dérivables</h3>
<ul>
<li>Si $f: I \rightarrow \mathbb{R}$ est dérivable sur $I$, elle est (strictement) convexe si et seulement si sa dérivée $f'$ est (strictement) croissante sur $I$.</li>
<li>Si $f: I \rightarrow \mathbb{R}$ est dérivable et convexe, sa courbe représentative est au-dessus de ses tangentes, c'est-à-dire :
$$\forall (a, x) \in I^2, f(x) \geq f(a) + (x - a)f'(a).$$</li>
<li>Si $f: I \rightarrow \mathbb{R}$ est deux fois dérivable sur $I$, elle est convexe si et seulement si sa dérivée seconde $f''$ est à valeurs positives.</li>
</ul>

<!-- Image omitted: image0010111 -->
<!-- Page 107 -->
<h1>Thème 10 - Dérivation et intégration des fonctions vectorielles</h1>
<p>On considère dans ce chapitre des applications définies sur un intervalle $I$ de $\mathbb{R}$ (non réduit à un point), à valeurs dans un espace vectoriel normé $E$.<br>
Les résultats énoncés généralisent ceux obtenus en première année pour les fonctions de $\mathbb{R}$ dans $\mathbb{R}$.<br>
Tous les espaces vectoriels normés intervenant dans ce chapitre seront supposés de dimension finie.</p>
<h2>[S10.1] Dérivation d'une fonction vectorielle</h2>
<ul>
<li>Soit $f$ une application de $I$ dans $E$, et $t_0 \in I$.<br>
On dit que $f$ est dérivable en $t_0$ si
$$\lim_{\substack{t \to t_0 \\ t \neq t_0}} \frac{1}{t - t_0} (f(t) - f(t_0))
$$
existe dans $E$.<br>
Dans ce cas, cette limite se note $f'(t_0)$Ou $\frac{df}{dt}(t_0)$ et s'appelle le vecteur dérivé de $f$ en $t_0$.<br>
Comme pour les fonctions numériques, on définit de façon analogue les notions de vecteur dérivé à droite et de vecteur dérivé à gauche.</li>
<li><strong>Développement limité</strong><br>
Dire que $f$ est dérivable en $t_0$ peut aussi s'écrire :
$$\exists \ell \in E \text{ tel que } f(t) = f(t_0) + (t - t_0)\ell + o(t - t_0)
$$
où $o(t - t_0)$ désigne une fonction vectorielle de la forme $(t - t_0)\varepsilon(t)$ avec $\varepsilon : I \to E$ telle que $\lim_{\substack{t \to t_0 \\ t \neq t_0}} \varepsilon(t) = 0$. Cette dernière expression s'appelle un développement limité de $f$ au voisinage de $t_0$.<br>
Dans le cas où $f$ est dérivable en $t_0$, ce développement limité peut aussi s'écrire sous la forme :
$$f(t_0 + h) = f(t_0) + hf'(t_0) + o(h).
$$</li>
<li>Si $f$ est dérivable (respectivement dérivable à gauche, respectivement dérivable à droite) en $t_0$, alors $f$ est continue (respectivement continue à gauche, respectivement continue à droite) en $t_0$.<br>
$\checkmark$ La réciproque de cette propriété est fausse. Il suffit par exemple de considérer l'application $f : x \mapsto |x|$ de $\mathbb{R}$ dans $\mathbb{R}$ : $f$ est évidemment continue sur $\mathbb{R}$ (elle est lipschitzienne de rapport 1), mais elle n'est pas dérivable en 0 (car $f'_d(0) = 1$ et $f'_g(0) = -1$). (Note : Il y avait une erreur dans le markdown original, $f'_d(0)=1$)</li>
<li><strong>Dérivation par coordonnées</strong><br>
On suppose $E$ de dimension $n$, rapporté à une base $(e_1, \ldots, e_n)$.</li>
</ul>

<!-- Image omitted: image0010112 -->
<!-- Page 108 -->
<p>Soit $f$ une application de $I$ dans $E$. Pour tout $t \in I$, on peut écrire :
$$f(t) = \sum_{i=1}^{n} f_i(t) e_i
$$
où les $f_i : I \rightarrow \mathbb{K}$ sont les applications coordonnées de $f$.<br>
Alors, $f$ est dérivable en $t_0$ si et seulement si, pour tout $i \in 〚 1 ; n 〛$, $f_i$ est dérivable en $t_0$, et on a alors :
$$f'(t_0) = \sum_{i=1}^{n} f_i'(t_0) e_i.
$$</p>
<h3>Exemples</h3>
<ul>
<li>Une fonction $f : I \rightarrow \mathbb{C}$ est dérivable en $t_0 \in I$ si et seulement si les fonctions $Re(f)$ et $Im(f)$ le sont, et on a alors $f'(t_0) = (Re(f))'(t_0) + i(Im(f))'(t_0)$.</li>
<li>Une application $A : I \rightarrow \mathcal{M}_{p,q}(\mathbb{K})$ (Note: Formule pour A omise comme dans le md) est dérivable en $t_0$ si et seulement si pour tout $(i, j) \in 〚 1 ; p 〛 \times 〚 1 ; q 〛$ les fonctions coefficients $t \mapsto a_{i,j}(t)$ le sont, et dans ce cas la matrice $A'(t_0)$ est la matrice dont les coefficients sont les $a_{i,j}'(t_0)$.</li>
</ul>
<h3>Dérivabilité sur un intervalle</h3>
<ul>
<li>Une application $f : I \rightarrow E$ est dite dérivable sur $I$ si pour tout $t_0 \in I$, $f$ est dérivable en $t_0$.<br>
Si tel est le cas, on peut définir l'application dérivée de $f$, notée $f'$, qui à tout $t \in I$ associe le vecteur $f'(t) \in E$.<br>
L'ensemble des applications dérivables sur $I$ à valeurs dans $E$ sera noté $\mathcal{D}(I, E)$.</li>
<li>Une application $f : I \rightarrow E$ est dite de classe $\mathcal{C}^1$ sur $I$ si $f$ est dérivable sur $I$ et si $f'$ est en plus continue sur $I$.<br>
On notera $\mathcal{C}^1(I, E)$ l'ensemble des applications de classe $\mathcal{C}^1$ sur $I$ à valeurs dans $E$.<br>
$\checkmark$ Il existe des fonctions dérivables qui ne sont pas de classe $\mathcal{C}^1$ ! Autrement dit, l'inclusion $\mathcal{C}^1(I, E) \subset \mathcal{D}(I, E)$ est stricte.</li>
</ul>
<h2>[S10.2] Opérations sur les dérivées</h2>
<ul>
<li>Soient $f$ et $g$ deux applications de $I$ dans $E$, dérivables en $t_0$.<br>
Alors l'application $f + g$ est dérivable en $t_0$, et $(f + g)'(t_0) = f'(t_0) + g'(t_0)$.</li>
<li>Soit $f$ une application de $I$ dans $E$, et $\lambda$ une application de $I$ dans $\mathbb{K}$, dérivables en $t_0$.<br>
Alors l'application $\lambda f$ est dérivable en $t_0$ et $(\lambda f)'(t_0) = \lambda'(t_0) f(t_0) + \lambda(t_0) f'(t_0)$.<br>
En particulier, si $\alpha \in \mathbb{K}$ est constant, $\alpha f$ est dérivable en $t_0$ et $(\alpha f)'(t_0) = \alpha f'(t_0)$.<br>
Il en résulte que l'ensemble $\mathcal{D}(I, E)$ est un sous-espace vectoriel de $\mathcal{C}^0(I, E)$, et l'application $f \mapsto f'$ est linéaire de $\mathcal{D}(I, E)$ dans $\mathcal{A}(I, E)$.</li>
</ul>

<!-- Image omitted: image0010113 -->
<!-- Page 109 -->
<ul>
<li>Soient $E, F$ deux espaces vectoriels normés, $f$ une application d'un intervalle $I$ de $\mathbb{R}$ à valeurs dans $E$, et $u$ une application linéaire de $E$ dans $F$. Si $f$ est dérivable en $t_0$, $u \circ f$ l'est aussi et : $(u \circ f)'(t_0) = u[f'(t_0)]$.</li>
<li>Soient $E, F, G$ trois espaces vectoriels normés, et $B: E \times F \longrightarrow G$ une application bilinéaire. Soient $f: I \longrightarrow E$ et $g: I \longrightarrow F$ deux applications dérivables en $t_0 \in I$. Alors l'application $\varphi: \begin{cases} I \longrightarrow G \\ t \longmapsto B(f(t), g(t)) \end{cases}$ est dérivable en $t_0$ et $\varphi'(t_0) = B(f'(t_0), g(t_0)) + B(f(t_0), g'(t_0))$.<br>
Exemple : soit $E$ un espace préhilbertien réel, muni d'un produit scalaire noté $\langle \cdot \mid \cdot \rangle$. Si $f$ et $g$ sont deux applications définies sur $I$, à valeurs dans $E$ et dérivables, alors l'application $\varphi: \begin{cases} I \longrightarrow \mathbb{R} \\ t \longmapsto \langle f(t) \mid g(t) \rangle \end{cases}$ est dérivable et : $\langle f \mid g \rangle' = \langle f' \mid g \rangle + \langle f \mid g' \rangle$.<br>
On en déduit ensuite que, si $f: I \longrightarrow E$ est dérivable en $t_0$ et si $f(t_0) \neq 0$, l'application $t \mapsto \|f(t)\|$ est dérivable en $t_0$, et : $\|f\|'(t_0) = \frac{\langle f'(t_0) \mid f(t_0) \rangle}{\|f(t_0)\|}$. (Note: Correction de la formule: le deuxième terme du produit scalaire devrait être f(t_0) et non f'(t_0) comme dans le markdown original)</li>
<li>Le théorème précédent peut se généraliser. Soient $E_1, \ldots, E_n$ et $F$ des espaces vectoriels normés, et $g: E_1 \times E_2 \times \cdots \times E_n \longrightarrow F$ une application $n$-linéaire. Pour tout $i \in 〚 1 ; n 〛$, soit $f_i: I \longrightarrow E_i$ une application dérivable sur $I$. Alors l'application $\varphi: \begin{cases} I \longrightarrow F \\ t \longmapsto g(f_1(t), f_2(t), \ldots, f_n(t)) \end{cases}$ est dérivable sur $I$ et, pour tout $t \in I$ : $\varphi'(t) = \sum_{i=1}^n g(f_1(t), \ldots, f_{i-1}(t), f_i'(t), f_{i+1}(t), \ldots, f_n(t))$.<br>
<strong>Exemples</strong>
<ul>
<li>Si $f_1, \ldots, f_n$ sont $n$ applications dérivables sur $I$ et à valeurs dans une algèbre normée, leur produit $g: t \mapsto f_1(t) f_2(t) \cdots f_n(t)$ est dérivable sur $I$ et pour tout $t \in I$ :
$$g'(t) = f_1'(t) f_2(t) \cdots f_n(t) + f_1(t) f_2'(t) f_3(t) \cdots f_n(t) + \cdots + f_1(t) f_2(t) \cdots f_{n-1}(t) f_n'(t).
$$</li>
<li>On sait que l'application $\det: \begin{cases} \mathcal{M}_n(\mathbb{K}) \longrightarrow K \\ M \longmapsto \det M \end{cases}$ est une application $n$-linéaire des colonnes de la matrice $M$.</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010114 -->
<!-- Page 110 -->
<p>Soit alors $M: \begin{cases} I \rightarrow \mathcal{M}_n(\mathbb{K}) \\ t \mapsto M(t) \end{cases}$ une application dérivable sur $I$.<br>
Si l'on note $C_1(t), \ldots, C_n(t)$ les colonnes de la matrice $M(t)$, alors les applications $t \mapsto C_i(t)$ sont dérivables sur $I$ (car leurs fonctions coordonnées le sont). Par suite, l'application $\varphi : t \mapsto \det(M(t))$ est dérivable sur $I$ et :
$$\varphi'(t) = \sum_{j=1}^{n} \det(C_1(t), \ldots, C_{j-1}(t), C_j'(t), C_{j+1}(t), \ldots, C_n(t)).
$$</p>
<h3>Fonction composée</h3>
<p>Soit $\varphi : I \rightarrow J$ et $f : J \rightarrow E$, où $I$ et $J$ sont des intervalles de $\mathbb{R}$ et $E$ un espace vectoriel normé.<br>
Soit $t_0 \in I$. Si $\varphi$ est dérivable en $t_0$ et si $f$ est dérivable en $\varphi(t_0)$, alors $f \circ \varphi$ est dérivable en $t_0$ et :
$$(f \circ \varphi)'(t_0) = \varphi'(t_0) f'[\varphi(t_0)].
$$</p>
<h2>[S10.3] Dérivées successives</h2>
<ul>
<li>Soit $f$ une application de $I$ dans $E$. On peut définir par récurrence, si elles existent, les dérivées successives de $f$ de la façon suivante :
<ul>
<li>on pose $f^{(0)} = f$ (dérivée d'ordre zéro);</li>
<li>pour $k \in \mathbb{N}^*$, on dira que $f$ est $k$ fois dérivable sur $I$ si $f^{(k-1)}$ est dérivable sur $I$ et on pose alors $f^{(k)} = (f^{(k-1)})'$ (dérivée d'ordre $k$).</li>
</ul>
On notera $\mathscr{D}^k(I, E)$ l'ensemble des applications $k$ fois dérivables sur $I$ à valeurs dans $E$.<br>
Pour $k \in \mathbb{N}^*$, on dira que $f$ est de classe $\mathscr{C}^k$ sur $I$ si $f$ est $k$ fois dérivable sur $I$ et si $f^{(k)}$ est continue sur $I$; on note $\mathscr{C}^k(I, E)$ l'ensemble des applications de classe $\mathscr{C}^k$ sur $I$ à valeurs dans $E$.<br>
Enfin on dira que $f$ est de classe $\mathscr{C}^\infty$ sur $I$ si elle est de classe $\mathscr{C}^k$ pour tout $k \in \mathbb{N}$; et on notera $\mathscr{C}^\infty(I, E)$ l'ensemble des applications de classe $\mathscr{C}^\infty$ sur $I$ à valeurs dans $E$.</li>
<li>$\mathscr{D}^k(I, E)$, $\mathscr{C}^k(I, E)$ et $\mathscr{C}^\infty(I, E)$ sont des sous-espaces vectoriels de $\mathcal{A}(I, E)$. Si $f, g \in \mathscr{D}^k(I, E)$ et si $\lambda, \mu \in \mathbb{K}$, on a : $(\lambda f + \mu g)^{(k)} = \lambda f^{(k)} + \mu g^{(k)}$.</li>
<li>On suppose $E$ de dimension $n$, rapporté à une base $(e_1, \ldots, e_n)$. Soit $f$ une application de $I$ dans $E$. Pour tout $t \in I$, on peut écrire $f(t) = \sum_{i=1}^{n} f_i(t) e_i$ (les $f_i : I \rightarrow \mathbb{K}$ sont les applications coordonnées).<br>
Alors, $f$ est $k$-fois dérivable (respectivement de classe $\mathscr{C}^k$) sur $I$ si et seulement si, pour tout $i \in [1; n]$, $f_i$ est $k$ fois dérivable (respectivement de classe $\mathscr{C}^k$) sur $I$, et on a alors :
$$\forall t \in I, f^{(k)}(t) = \sum_{i=1}^{n} f_i^{(k)}(t) e_i.
$$</li>
</ul>

<!-- Image omitted: image0010115 -->
<!-- Page 111 -->
<h3>Formule de Leibniz</h3>
<p>Soient $E$, $F$, $G$ trois espaces vectoriels normés, et $B: E \times F \rightarrow G$ une application bilinéaire.<br>
Soient $f: I \rightarrow E$ et $g: I \rightarrow F$ deux applications $n$ fois dérivables (respectivement de classe $\mathscr{C}^n$) sur $I$.<br>
Alors l'application $\varphi : \begin{cases} I \rightarrow G \\ t \mapsto B(f(t), g(t)) \end{cases}$ est $n$ fois dérivable (respectivement de classe $\mathscr{C}^n$) sur $I$, et l'on a :
$$\forall t \in I, \varphi^{(n)}(t) = \sum_{k=0}^{n} \binom{n}{k} B(f^{(k)}(t), g^{(n-k)}(t)).
$$</p>
<h3>Conséquences</h3>
<ul>
<li>$\mathscr{D}^n(I, \mathbb{K})$ et $\mathscr{C}^n(I, \mathbb{K})$ sont des sous-algèbres de $\mathcal{A}(I, \mathbb{K})$.</li>
<li>Soit $\varphi : I \rightarrow J$ et $f: J \rightarrow E$, où $I$ et $J$ sont des intervalles de $\mathbb{R}$ et $E$ un espace vectoriel normé.<br>
Si $\varphi \in \mathscr{C}^n(I, J)$ et $f \in \mathscr{C}^n(J, E)$, alors $f \circ \varphi \in \mathscr{C}^n(I, E)$.</li>
</ul>
<h2>[S10.4] Applications de classe $\mathscr{C}^k$ par morceaux</h2>
<h3>Définitions</h3>
<ul>
<li>Soit $[a; b]$ ($a < b$) un segment de $\mathbb{R}$, $k \in \mathbb{N} \cup \{\infty\}$ et $f$ une application de $[a; b]$ dans un espace vectoriel normé $E$.<br>
On dira que $f$ est de classe $\mathscr{C}^k$ par morceaux sur $[a; b]$ s'il existe une subdivision $a = x_0 < x_1 < \ldots < x_{n-1} < x_n = b$ telle que la restriction de $f$ à chaque intervalle $]x_{i-1}; x_i[$ pour $1 \leq i \leq n$ se prolonge en une application de classe $\mathscr{C}^k$ sur $[x_{i-1}; x_i]$. Une telle subdivision est dite adaptée à $f$.</li>
<li>Soit $I$ un intervalle quelconque de $\mathbb{R}$, et $f$ une application de $I$ dans un espace vectoriel normé $E$.<br>
On dira que $f$ est de classe $\mathscr{C}^k$ par morceaux sur $I$ si sa restriction à tout segment $[a; b]$ inclus dans $I$ est de classe $\mathscr{C}^k$ par morceaux.</li>
<li>On notera $\mathscr{C}^k_M(I, E)$ l'ensemble des applications de classe $\mathscr{C}^k$ par morceaux de $I$ dans $E$; dans le cas $k = 0$, on parle simplement d'application continue par morceaux et on notera $\mathscr{C}_M(I, E)$ l'ensemble des applications continues par morceaux de $I$ dans $E$.</li>
</ul>
<h3>Exemples</h3>
<ul>
<li>La fonction "partie entière" est continue par morceaux sur $\mathbb{R}$.</li>
<li>La fonction $\tan$ n'est pas continue par morceaux sur $\mathbb{R}$.</li>
<li>La fonction $x \mapsto \sqrt{x}$ est $\mathscr{C}^1$ par morceaux sur $\mathbb{R}^*_+$ mais pas sur $\mathbb{R}_+$.</li>
<li>L'ensemble $\mathscr{C}^k_M(I, E)$ des applications de classe $\mathscr{C}^k$ par morceaux de $I$ dans $E$ est un sous-espace vectoriel de $\mathcal{A}(I, E)$.</li>
</ul>

<!-- Image omitted: image0010116 -->
<!-- Page 112 -->
<h2>[S10.5] Arcs paramétrés</h2>
<p>Nous nous contentons ici de rappeler les définitions de base. Les méthodes pratiques d'étude d'un arc paramétré seront détaillées dans le chapitre Savoir-Faire page 430.<br>
Pour toute la suite, on se donne un entier $n \geq 2$; l'ensemble $\mathbb{R}^n$ peut être considéré comme un espace vectoriel (un $n$-uplet $(x_1, \ldots, x_n)$ étant alors un vecteur $\vec{V}$) ou comme un espace affine (une origine $O$ étant choisie, le $n$-uplet $(x_1, \ldots, x_n)$ représente les coordonnées d'un point $M$ tel que $\overrightarrow{OM} = \vec{V}$).</p>
<ul>
<li>On appelle arc paramétré (ou courbe paramétrée) de $\mathbb{R}^n$ un couple $(I, \gamma)$Où $I$ est un intervalle de $\mathbb{R}$ non réduit à un point et $\gamma$ une application de $I$ dans $\mathbb{R}^n$. L'arc est dit de classe $\mathcal{C}^k$ si $\gamma$ est de classe $\mathcal{C}^k$.<br>
Pour tout $t \in I$On notera $M(t)$ le point de $\mathbb{R}^n$ tel que $\overrightarrow{OM(t)} = \gamma(t)$. L'ensemble :
$$\Gamma = \{ M(t) \in \mathbb{R}^n \mid t \in I \}
$$
des points de $\mathbb{R}^n$ atteints est appelé support de l'arc.<br>
En interprétant le paramètre $t$ comme le temps, un arc paramétré représente le mouvement d'un mobile $M(t)$; le support de l'arc est la trajectoire du mobile.<br>
Si $\gamma$ est de classe $\mathcal{C}^k$ avec $k \geq 2$,
$$\gamma'(t) = \frac{d\overrightarrow{OM}}{dt} \text{ est le vecteur vitesse et } \gamma''(t) = \frac{d^2\overrightarrow{OM}}{dt^2} \text{ le vecteur accélération.}
$$</li>
<li><strong>Tangente en un point d'un arc paramétré</strong><br>
Soit $(I, \gamma)$ un arc paramétré de classe $\mathcal{C}^k$, et $t_0 \in I$. On suppose qu'il existe un entier $i \in [1; k]$ tel que $\gamma^{(i)}(t_0) \neq 0$ et on note $p$ le plus petit de ces entiers.<br>
On a alors le développement limite :
$$\gamma(t_0 + h) = \gamma(t_0) + h^p \frac{\gamma^{(p)}(t_0)}{p!} + o(h^p)
$$
de sorte que :
$$\lim_{h \to 0} \frac{\overrightarrow{M(t_0)M(t_0 + h)}}{h^p} = \frac{\gamma^{(p)}(t_0)}{p!}.
$$
On appelle donc tangente à l'arc au point $M(t_0)$ de paramètre $t_0$ la droite passant par $M(t_0)$ et dirigée par le vecteur $\gamma^{(p)}(t_0)$ c'est-à-dire l'ensemble :
$$\left\{ M(t_0) + \lambda \gamma^{(p)}(t_0) \mid \lambda \in \mathbb{R} \right\}.
$$
Le point $M(t_0)$ de paramètre $t_0$ est dit régulier si $p = 1$, c'est-à-dire si $\gamma'(t_0) \neq 0$. Il est dit stationnaire sinon, c'est-à-dire si $\gamma'(t_0) = 0$.<br>
Un arc est dit régulier si tous ses points sont réguliers.</li>
<li><strong>Longueur d'un arc paramétré</strong><br>
On suppose ici $\mathbb{R}^n$ muni de sa structure euclidienne canonique.<br>
Soit $(I, \gamma)$ un arc paramétré de classe $\mathcal{C}^1$ à valeurs dans $\mathbb{R}^n$, et $t_0 < t_1$ dans $I$. La longueur de l'arc entre les points $M(t_0)$ et $M(t_1)$ de paramètres $t_0$ et $t_1$ est le réel positif :
$$\ell(M(t_0)M(t_1)) = \int_{t_0}^{t_1} \|\gamma'(t)\| dt.$$ (Note: notation $\overrightarrow{M(t_0)M(t_1)}$ remplacée par $\ell$ pour la longueur)</li>
</ul>

<!-- Image omitted: image0010117 -->
<!-- Page 113 -->
<p>En particulier, si l'arc est à valeurs dans $\mathbb{R}^2$On obtient la formule :
$$\ell(M(t_0)M(t_1)) = \int_{t_0}^{t_1} \sqrt{x'^2(t) + y'^2(t)} \, dt.
$$</p>
<h2>[S10.6] Intégrale d'une fonction continue par morceaux sur un segment</h2>
<p>Les applications considérées par la suite sont définies sur un segment $[a ; b]$ de $\mathbb{R}$ ($a < b$) et à valeurs dans un espace vectoriel normé de dimension finie $E$.</p>
<h3>Fonctions en escalier</h3>
<ul>
<li>Une application $f : [a ; b] \rightarrow E$Où $E$ est un $\mathbb{K}$-espace vectoriel normé est dite en escalier sur $[a ; b]$ s'il existe une subdivision $\sigma = x_0 < x_1 < \ldots < x_{n-1} < x_n = b$ de l'intervalle $[a ; b]$ telle que la restriction de $f$ à chaque intervalle $]x_{i-1} ; x_i[$ pour $1 \leq i \leq n$ soit constante. Une telle subdivision est dite adaptée à $f$.</li>
<li>Si $I$ est un intervalle quelconque de $\mathbb{R}$, $f$ est dite en escalier sur $I$ si elle l'est sur tout segment inclus dans $I$.</li>
<li>L'ensemble $\mathscr{E}sc(I, E)$ des fonctions en escalier de $I$ dans $E$ est un sous-espace vectoriel de $\mathcal{A}(I, E)$.</li>
</ul>
<h3>Intégrale d'une fonction en escalier sur un segment</h3>
<p>Soit $f \in \mathscr{E}sc([a ; b], E)$ et $\sigma = (x_0, \ldots, x_n)$ une subdivision adaptée à $f$, c'est-à-dire :
$$\forall i \in [1 ; n], \quad \forall t \in ]x_{i-1} ; x_i[, \quad f(t) = c_i
$$
où les $c_i$ sont des éléments de $E$. On pose :
$$I_\sigma(f) = \sum_{i=1}^n (x_i - x_{i-1})c_i.
$$
Alors le vecteur $I_\sigma(f)$ est indépendant de la subdivision $\sigma$ adaptée à $f$ choisie.<br>
On l'appelle intégrale de $f$ sur $[a ; b]$. Il est noté $\int_{[a ; b]} f$Ou $\int_a^b f$Ou $\int_a^b f(t) \, dt$.</p>
<h3>Intégrale d'une fonction continue par morceaux</h3>
<p>On sait qu'une fonction continue par morceaux sur un segment y est limite uniforme d'une suite de fonctions en escalier (voir [S12.6]).<br>
Soit $f \in \mathscr{C.M.}([a ; b], E)$. Pour toute suite $(\varphi_n)_{n \in \mathbb{N}}$ de fonctions en escalier qui converge uniformément vers $f$ sur $[a ; b]$, la suite $\left( \int_a^b \varphi_n \right)_{n \in \mathbb{N}}$ converge dans $E$.<br>
De plus, sa limite ne dépend que de $f$, et non de la suite $(\varphi_n)_{n \in \mathbb{N}}$ choisie. Cette limite s'appelle l'intégrale de $f$ sur $[a ; b]$, et est notée $\int_a^b f$Ou $\int_{[a ; b]} f$.</p>

<!-- Image omitted: image0010118 -->
<!-- Page 114 -->
<h2>[S10.7] Propriétés de l'intégrale</h2>
<ul>
<li><strong>Linéarité</strong><br>
L'application $I: \begin{cases} \mathscr{C.M.}([a ; b], E) & \longrightarrow E \\ f & \longmapsto \int_{[a ; b]} f \end{cases}$ est linéaire.</li>
<li>Soient $E, F$ deux espaces vectoriels normés.<br>
Soit $f \in \mathscr{C.M.}([a ; b], E)$ et $u$ une application linéaire de $E$ dans $F$.<br>
Alors, $u \circ f$ est continue par morceaux de $[a ; b]$ dans $F$ et l'on a :
$$
\int_{[a ; b]} u \circ f = u \left( \int_{[a ; b]} f \right).
$$</li>
<li><strong>Invariance</strong><br>
Soient $f, g \in \mathscr{C.M.}([a ; b], E)$. Si $f$ et $g$ ne diffèrent qu'en un nombre fini de points, alors $\int_{a}^{b} f = \int_{a}^{b} g$.</li>
<li>Soit $f \in \mathscr{C.M.}([a ; b], E)$. Alors la fonction $\| f \| : t \mapsto \| f(t) \|$ est continue par morceaux sur $[a ; b]$ et :
$$
\left\| \int_{a}^{b} f \right\| \leqslant \int_{a}^{b} \| f \| \leqslant (b - a) \| f \|_{\infty},
$$
où $\| f \|_{\infty}$ désigne comme d'habitude le réel $\sup \{ \| f(t) \|_{E}, t \in [a ; b] \}$.</li>
<li><strong>Relation de Chasles</strong><br>
Soit $f \in \mathscr{C.M.}([a ; b], E)$ et $c \in ]a ; b[$.<br>
Alors $\int_{[a ; c]} f$ et $\int_{[c ; b]} f$ existent et l'on a : (Note: corrigé pour indiquer que les intégrales existent sur les sous-intervalles)
$$
\int_{a}^{b} f = \int_{a}^{c} f + \int_{c}^{b} f.
$$</li>
<li><strong>Utilisation des fonctions coordonnées dans une base</strong><br>
Soit $\mathscr{B} = (e_{1}, \ldots, e_{n})$ une base de $E$. Si $f \in \mathscr{C.M.}([a ; b], E)$On a, pour tout $t \in [a ; b]$, $f(t) = \sum_{i=1}^{n} f_{i}(t) e_{i}$, où les $f_{i}$ sont des applications de $[a ; b]$ dans $\mathbb{K}$.<br>
Alors les $f_{i}$ sont continues par morceaux sur $[a ; b]$ et l'on a :
$$
\int_{a}^{b} f(t) \, dt = \sum_{i=1}^{n} \left( \int_{a}^{b} f_{i}(t) \, dt \right) e_{i}.
$$</li>
<li><strong>Cas des fonctions à valeurs réelles</strong>
<ul>
<li>Positivité
<ul>
<li>Si $f$ est une application continue par morceaux sur $[a ; b]$, à valeurs réelles positives, alors $\int_{a}^{b} f \geqslant 0$.</li>
<li>Si $f, g \in \mathscr{C.M.}([a ; b], \mathbb{R})$ sont telles que $f \leqslant g$ sur $[a ; b]$, alors $\int_{a}^{b} f \leqslant \int_{a}^{b} g$.</li>
</ul>
</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010119 -->
<!-- Page 115 -->
<ul>
<li>Si $f$ est une fonction <em>continue</em> sur $[a ; b]$, avec $a < b$, à valeurs <em>réelles positives</em>, telles que $\int_{a}^{b} f = 0$, alors $f = 0$ sur $[a ; b]$.</li>
<li><strong>Inégalité de Cauchy-Schwarz</strong><br>
Pour $f, g \in \mathscr{C.M.}([a ; b] ; \mathbb{R})$On a :
$$\left| \int_{a}^{b} f(t) g(t) \, \mathrm{d}t \right| \leqslant \sqrt{ \int_{a}^{b} f(t)^2 \, \mathrm{d}t } \sqrt{ \int_{a}^{b} g(t)^2 \, \mathrm{d}t }.
$$
De plus, lorsque $f$ et $g$ sont continues, il y a égalité si et seulement si la famille $(f, g)$ est liée.</li>
</ul>
<h2>[S10.8] Sommes de Riemann</h2>
<ul>
<li><strong>Définition</strong><br>
Soit $\sigma = (x_0, \ldots, x_n)$ une subdivision de $[a ; b]$.<br>
On appelle pas de cette subdivision le réel :
$$\max \{ x_{i+1} - x_i, \, i \in 〚 0 ; n-1 〛 \}.
$$
On appelle famille subordonnée à $\sigma$ toute famille $c = (c_i)_{1 \leqslant i \leqslant n}$ telle que $c_i \in [x_{i-1} ; x_i]$ pour tout $i \in 〚 1 ; n 〛$.<br>
Si $f$ est une fonction continue par morceaux sur $[a ; b]$, à valeurs dans $E$, le vecteur $S(f, \sigma, c) = \sum_{i=1}^{n} (x_i - x_{i-1}) f(c_i)$ s'appelle une somme de Riemann relative à $f, \sigma, c$.</li>
<li><strong>Théorème</strong><br>
Soit $f$ une fonction continue par morceaux sur $[a ; b]$ à valeurs dans $E$.<br>
Alors, pour tout $\epsilon > 0$, il existe $\alpha > 0$ tel que, pour toute subdivision $\sigma$ de $[a ; b]$ de pas inférieur à $\alpha$ et toute suite subordonnée $c$, on ait :
$$\left\| S(f, \sigma, c) - \int_{a}^{b} f \right\| < \epsilon
$$
(autrement dit, les sommes de Riemann associées à $f$ tendent vers $\int_{a}^{b} f$ lorsque le pas de la subdivision tend vers 0).</li>
<li><strong>Cas particulier</strong><br>
On utilise le plus souvent le théorème précédent dans le cas où $\sigma$ est une subdivision régulière de pas $\frac{b-a}{n}$, c'est-à-dire $x_i = a + i \frac{b-a}{n}$. On obtient dans ce cas :
$$S(f, \sigma, c) = \frac{b-a}{n} \sum_{i=1}^{n} f(c_i) \quad (c_i \in [x_{i-1} ; x_i]).
$$
En choisissant $c_i = x_i$ ( $c_i = x_{i-1}$Ou $c_i = \frac{x_i + x_{i-1}}{2}$ sont aussi des choix possibles), on obtient :
$$\lim_{n \to +\infty} \frac{b-a}{n} \sum_{i=1}^{n} f \left( a + i \frac{b-a}{n} \right) = \int_{a}^{b} f.
$$</li>
</ul>

<!-- Image omitted: image0010120 -->
<!-- Page 116 -->
<h2>[S10.9] Primitives</h2>
<ul>
<li><strong>Définitions</strong>
<ul>
<li>Soit $f$ une application <em>continue</em> sur un intervalle $I \subset \mathbb{R}$, à valeurs dans $E$. On appelle primitive de $f$ toute application $F: I \rightarrow E$, de classe $\mathscr{C}^1$, telle que $F' = f$.</li>
<li>Soit $f$ une application <em>continue par morceaux</em> sur un intervalle $I \subset \mathbb{R}$, à valeurs dans $E$.<br>
On appelle alors primitive de $f$ une application $F: I \rightarrow E$ telle que :
<ul>
<li>$F$ est continue et de classe $\mathscr{C}^1$ par morceaux sur $I$;</li>
<li>et $F'(t) = f(t)$ en tout point $t$ de $I$Où $f$ est continue.</li>
</ul>
</li>
</ul>
</li>
<li>Si $f \in \mathscr{CM}(I, E)$, deux primitives quelconques de $f$ diffèrent d'une constante.</li>
<li><strong>Théorème fondamental</strong><br>
Si $f \in \mathscr{CM}(I, E)$, et si $a \in I$, l'application $F_a: \begin{cases} I \rightarrow E \\ x \mapsto \int_a^x f(t) \, dt \end{cases}$ est une primitive de $f$.<br>
C'est l'unique primitive de $f$ qui s'annule en $a$.</li>
<li><strong>Conséquences</strong>
<ul>
<li>Si $f$ est continue par morceaux sur $[a; b]$ et si $F$ est une primitive de $f$, on a :
$$
\int_a^b f = F(b) - F(a) \quad \text{ce que l'on écrit :} \quad \int_a^b f = [F(t)]_a^b.
$$</li>
<li>Si $f$ est continue et de classe $\mathscr{C}^1$ par morceaux sur $[a; b]$, on a :
$$
\int_a^b f' = f(b) - f(a).
$$
(avec, ici, un abus de notation, $f'$ n'étant définie que sur $[a; b]$ privé d'un nombre fini de points.)</li>
</ul>
</li>
<li><strong>Inégalité des accroissements finis</strong><br>
Si $f$ est continue sur $[a; b]$ et de classe $\mathscr{C}^1$ sur $]a; b[$ et s'il existe un réel $M$ tel que : $\forall t \in ]a; b[, \|f'(t)\| \leq M$, on a :
$$
\|f(b) - f(a)\| \leq M(b - a).
$$</li>
<li><strong>Généralisation : intégrale fonction de ses bornes</strong><br>
Soit $f$ une application continue de $I$ dans $E$, et $u, v$ deux applications de classe $\mathscr{C}^1$ définies sur un intervalle $J$, à valeurs dans $I$.<br>
Alors, la fonction $F: \begin{cases} J \rightarrow E \\ x \mapsto \int_{u(x)}^{v(x)} f(t) \, dt \end{cases}$ est de classe $\mathscr{C}^1$ sur $J$ et :
$$
\forall x \in J, F'(x) = v'(x) \cdot f[v(x)] - u'(x) \cdot f[u(x)].
$$</li>
</ul>

<!-- Image omitted: image0010121 -->
<!-- Page 117 -->
<h2>[S10.10] Intégration par parties</h2>
<p>Soient $E$, $F$, $G$ trois $\mathbb{K}$-espaces vectoriels normés, et $B$ une application bilinéaire de $E \times F$ dans $G$.<br>
Soient $f: I \to E$ et $g: I \to F$ des applications continues et de classe $\mathscr{C}^1$ par morceaux sur un intervalle $I$ de $\mathbb{R}$. Alors :
$$\forall (a, b) \in I^2, \int_a^b B(f'(t), g(t)) \, dt = \left[ B(f(t), g(t)) \right]_a^b - \int_a^b B(f(t), g'(t)) \, dt.
$$</p>
<h2>[S10.11] Changement de variable</h2>
<h3>Cas d'une fonction continue</h3>
<p>Soit $f \in \mathscr{C}^0(I, E)$ et $\varphi \in \mathscr{C}^1([a; b], \mathbb{R})$ telle que $\varphi([a; b]) \subset I$. Alors :
$$\int_{\varphi(a)}^{\varphi(b)} f(t) \, dt = \int_a^b \varphi'(u) \cdot f[\varphi(u)] \, du.
$$</p>
<h3>Extension aux fonctions continues par morceaux</h3>
<p>Soit $f \in \mathscr{M}(I, E)$ et $\varphi \in \mathscr{C}^1([a; b], \mathbb{R})$, strictement monotone, telle que $\varphi([a; b]) \subset I$. Alors :
$$\int_{\varphi(a)}^{\varphi(b)} f(t) \, dt = \int_a^b \varphi'(u) \cdot f[\varphi(u)] \, du.
$$</p>
<h2>[S10.12] Formules de Taylor</h2>
<h3>Formule de Taylor avec reste intégral</h3>
<p>Soit $f: I \to E$, de classe $\mathscr{C}^n$ ($n \in \mathbb{N}$) sur $I$ et de classe $\mathscr{C}^{n+1}$ par morceaux sur $I$. Alors, pour tous $a, b \in I$ :
$$f(b) = \sum_{k=0}^n \frac{(b-a)^k}{k!} f^{(k)}(a) + \int_a^b \frac{(b-t)^n}{n!} f^{(n+1)}(t) \, dt.
$$</p>
<h3>Inégalité de Taylor-Lagrange</h3>
<p>Soit $f: I \to E$, de classe $\mathscr{C}^n$ ($n \in \mathbb{N}$) sur $I$ et de classe $\mathscr{C}^{n+1}$ par morceaux sur $I$. Alors, pour tous $a, b \in I$ :
$$\left\| f(b) - \sum_{k=0}^n \frac{(b-a)^k}{k!} f^{(k)}(a) \right\| \leq \frac{|b-a|^{n+1}}{(n+1)!} \left\| f^{(n+1)} \right\|_\infty^{[a; b]}.
$$</p>
<h3>Formule de Taylor-Young</h3>
<p>Soit $f: I \to E$, de classe $\mathscr{C}^n$ ($n \in \mathbb{N}$) sur $I$. Alors, pour tous $a, x \in I$ :
$$f(x) = \sum_{k=0}^n \frac{(x-a)^k}{k!} f^{(k)}(a) + (x-a)^n \varepsilon(x-a),
$$
où $\varepsilon$ est une fonction à valeurs dans $E$ telle que $\lim_{h \to 0} \varepsilon(h) = 0$.</p>

<!-- Image omitted: image0010122 -->
<!-- Page 118 -->
<h2>[S10.13] Primitives usuelles</h2>
<p>Dans le tableau ci-dessous, on rappelle quelques primitives de fonctions usuelles. Les domaines de définition ne sont pas mentionnés. Le réel $a$ qui intervient est supposé non nul.</p>

<table>
<thead>
<tr>
<th>Fonction</th>
<th>Primitive</th>
<th>Fonction</th>
<th>Primitive</th>
</tr>
</thead>
<tbody>
<tr>
<td>$(ax + b)^\alpha$ $(\alpha \neq -1)$</td>
<td>$\frac{(ax + b)^{\alpha + 1}}{a(\alpha + 1)}$</td>
<td>$\text{sh}(ax + b)$</td>
<td>$\frac{1}{a} \text{ch}(ax + b)$</td>
</tr>
<tr>
<td>$\frac{1}{ax + b}$</td>
<td>$\frac{1}{a} \ln|ax + b|$</td>
<td>$\text{ch}(ax + b)$</td>
<td>$\frac{1}{a} \text{sh}(ax + b)$</td>
</tr>
<tr>
<td>$e^{ax + b}$</td>
<td>$\frac{1}{a} e^{ax + b}$</td>
<td>$\frac{1}{\text{sh} x}$</td>
<td>$\ln \left| \text{th} \frac{x}{2} \right|$</td>
</tr>
<tr>
<td>$\ln x$</td>
<td>$x \ln x - x$</td>
<td>$\frac{1}{\text{ch} x}$</td>
<td>$2 \text{Arctan}(e^x)$</td>
</tr>
<tr>
<td>$\sin(ax + b)$</td>
<td>$-\frac{1}{a} \cos(ax + b)$</td>
<td>$\frac{1}{\text{sh}^2 x}$</td>
<td>$-\coth x$</td>
</tr>
<tr>
<td>$\cos(ax + b)$</td>
<td>$\frac{1}{a} \sin(ax + b)$</td>
<td>$\frac{1}{\text{ch}^2 x}$</td>
<td>$\text{th} x$</td>
</tr>
<tr>
<td>$\frac{1}{\sin x}$</td>
<td>$\ln \left| \tan \frac{x}{2} \right|$</td>
<td>$\text{th} x$</td>
<td>$\ln(\text{ch} x)$</td>
</tr>
<tr>
<td>$\frac{1}{\cos x}$</td>
<td>$\ln \left| \tan \left( \frac{x}{2} + \frac{\pi}{4} \right) \right|$</td>
<td>$\frac{1}{x^2 + a^2}$</td>
<td>$\frac{1}{a} \text{Arctan} \left( \frac{x}{a} \right)$</td>
</tr>
<tr>
<td>$\frac{1}{\sin^2 x}$</td>
<td>$-\cot x$</td>
<td>$\frac{1}{x^2 - a^2}$</td>
<td>$\frac{1}{2a} \ln \left| \frac{x - a}{x + a} \right|$</td>
</tr>
<tr>
<td>$\frac{1}{\cos^2 x}$</td>
<td>$\tan x$</td>
<td>$\frac{1}{\sqrt{x^2 + a^2}}$</td>
<td>$\ln(x + \sqrt{x^2 + a^2})$</td>
</tr>
<tr>
<td>$\tan x$</td>
<td>$-\ln |\cos x|$</td>
<td>$\frac{1}{\sqrt{x^2 - a^2}}$</td>
<td>$\ln |x + \sqrt{x^2 - a^2}|$</td>
</tr>
<tr>
<td>$\tan^2 x$</td>
<td>$\tan x - x$</td>
<td>$\frac{1}{\sqrt{a^2 - x^2}}$</td>
<td>$\text{Arcsin} \left( \frac{x}{|a|} \right)$</td>
</tr>
</tbody>
</table>

<p>On retiendra qu'une primitive de $\frac{1}{x^2 - a^2}$ (qui intervient fréquemment) se retrouve facilement en écrivant $\frac{1}{x^2 - a^2} = \frac{1}{2a} \left( \frac{1}{x - a} - \frac{1}{x + a} \right)$.</p>

<!-- Image omitted: image0010123 -->
<!-- Page 119 -->
<h1>Thème 11 - Séries et familles sommables</h1>
<p>Dans tout ce chapitre, $\mathbb{K}$ désigne $\mathbb{R}$Ou $\mathbb{C}$, et $(E, \| \cdot \|)$ est un $\mathbb{K}$-espace vectoriel normé.</p>
<h2>[S11.1] Définition de la convergence d'une série</h2>
<ul>
<li>Soit $(u_n)_{n \in \mathbb{N}}$ une suite d'éléments de $E$. On appelle série de terme général $u_n$ et l'on note $\sum u_n$ la suite des sommes partielles $(S_n)_{n \in \mathbb{N}}$ définie par :
$$\forall n \in \mathbb{N}, \quad S_n = \sum_{k=0}^{n} u_k.
$$
On dit alors que la série $\sum u_n$ converge si la suite $(S_n)_{n \in \mathbb{N}}$ converge dans $E$. Dans ce cas, on appelle somme de la série $\sum u_n$ la limite de la suite $(S_n)_{n \in \mathbb{N}}$; cette somme est notée $\sum_{n=0}^{+\infty} u_n$.</li>
<li>La nature (convergence ou divergence) de la série n'est pas modifiée si l'on part d'un entier $n_0 \neq 0$ (mais, en cas de convergence, la valeur de la somme peut être modifiée).</li>
<li>On fera particulièrement attention aux notations : l'écriture $\sum u_n$Ou $\sum_{n \in \mathbb{N}} u_n$ est une simple abréviation pour dire « la série de terme général $u_n$ », et ne doit pas être utilisée dans des calculs; l'écriture $\sum_{n=0}^{N} u_n$ désigne une « vraie » somme, et la notation $\sum_{n=0}^{+\infty} u_n$, qui ne doit être utilisée que si la série converge, désigne la limite de la suite des sommes partielles.</li>
</ul>
<h2>[S11.2] Séries télescopiques</h2>
<p>Il s'agit de séries de la forme $\sum v_n$, où $v_n = u_n - u_{n-1}$ ($n \geq 1$).<br>
Les sommes partielles sont alors : $V_n = \sum_{k=1}^{n} v_k = u_n - u_0$ donc on peut énoncer le théorème suivant :<br>
La série $\sum_{n \in \mathbb{N}^*} (u_n - u_{n-1})$ converge si et seulement si la suite $(u_n)$ converge et, dans ce cas : $\sum_{k=1}^{+\infty} (u_k - u_{k-1}) = \lim_{n \to +\infty} u_n - u_0$. (Note: Corrigé n à +infini dans la somme) </p>
<ul>
<li>Cette propriété, qui peut sembler simpliste, est en réalité très importante : elle permet non seulement de calculer la somme de certaines séries, mais aussi, dans certains cas, d'étudier la convergence d'une suite en la ramenant à celle d'une série, donc en utilisant les nombreux outils supplémentaires disponibles sur les séries.</li>
</ul>

<!-- Image omitted: image0010124 -->
<!-- Page 120 -->
<h2>[S11.3] Condition nécessaire de convergence d'une série</h2>
<p>Si la série $\sum u_n$ converge alors $\lim_{n \to +\infty} u_n = 0_{E}$.<br>
$\checkmark$ Cela équivaut au résultat essentiel suivant : une série dont le terme général ne tend pas vers $0$ est divergente; on parle de divergence grossière.<br>
Mais attention : on ne peut rien dire a priori d'une série dont le terme général tend vers $0$. Il faut d'ailleurs retenir l'exemple de la série harmonique $\sum \frac{1}{n}$, dont le terme général tend vers zéro mais qui est divergente.</p>
<h2>[S11.4] Opérations algébriques sur les séries</h2>
<p>Soit $(u_n)_{n \in \mathbb{N}}$ et $(v_n)_{n \in \mathbb{N}}$ deux suites d'éléments de $E$ et $\lambda$ un scalaire.</p>
<ul>
<li>Si les séries $\sum u_n$ et $\sum v_n$ convergent alors la série $\sum (\lambda u_n + v_n)$ converge et :
$$\sum_{n=0}^{+\infty} (\lambda u_n + v_n) = \lambda \sum_{n=0}^{+\infty} u_n + \sum_{n=0}^{+\infty} v_n.
$$</li>
<li>Si la série $\sum u_n$ converge et si la série $\sum v_n$ diverge, alors la série $\sum (u_n + v_n)$ diverge.<br>
$\checkmark$On ne peut rien dire en général de la somme de deux séries divergentes.</li>
<li>On suppose ici $E$ de dimension finie $p \in \mathbb{N}^*$, muni d'une base $B = (e_1, \ldots, e_p)$. Si $(u_n)_{n \in \mathbb{N}} \in E^{\mathbb{N}}$, on peut considérer ses suites coordonnées $(u_n^{(i)})_{n \in \mathbb{N}}$ définies par :
$$\forall n \in \mathbb{N}, u_n = \sum_{i=1}^{p} u_n^{(i)} e_i.
$$
Alors la série $\sum u_n$ converge dans $E$ si et seulement si pour tout $i \in 〚 1 ; p 〛$ la série numérique $\sum u_n^{(i)}$ converge dans $\mathbb{K}$.<br>
Et dans ce cas l'on a :
$$\sum_{n=0}^{+\infty} u_n = \sum_{i=1}^{p} \left( \sum_{n=0}^{+\infty} u_n^{(i)} \right) e_i.
$$
$\checkmark$ Ce résultat permet de ramener l'étude d'une série vectorielle, dans le cas de la dimension finie, à celle de ses séries coordonnées. Cependant, pour étudier une série $\sum u_n$ à valeurs dans $E$, on préférera dans la plupart des cas démontrer l'absolue convergence (voir ci-après) en considérant la série des normes $\sum \| u_n \|$.</li>
</ul>
<h2>[S11.5] Convergence absolue</h2>
<p>Une série $\sum u_n$ de vecteurs de l'espace vectoriel normé $(E, \| \cdot \|)$ est dite absolument convergente si la série de nombres réels positifs $\sum \| u_n \|$ est convergente.</p>

<!-- Image omitted: image0010125 -->
<!-- Page 121 -->
<p><strong>Théorème</strong> : si $E$ est de dimension finie, toute série $\sum u_n$ absolument convergente est convergente.<br>
Dans ce cas on a de plus :
$$\left\| \sum_{n=0}^{+\infty} u_n \right\| \leq \sum_{n=0}^{+\infty} \| u_n \|.
$$
$\checkmark$ La réciproque de ce théorème est fausse; il faut connaître le contre-exemple classique : la série harmonique alternée $\sum \frac{(-1)^n}{n}$ est convergente alors que la série harmonique $\sum \frac{1}{n}$ est divergente.</p>
<h2>[S11.6] Règles de comparaison pour les séries à termes réels positifs</h2>
<p>Nous rappelons ici les règles de comparaison vues en $1^{re}$ année pour les séries à termes réels positifs.<br>
Pour une série $\sum u_n$ à valeurs dans un espace vectoriel normé $E$, on pourra chercher à utiliser ces règles en les appliquant à la série $\sum \| u_n \|$ afin de démontrer l'absolue convergence de la série (donc la convergence lorsque $E$ est de dimension finie).</p>
<ul>
<li><strong>Règle de comparaison n° 1</strong><br>
Une série à termes réels positifs est convergente si et seulement si la suite de ses sommes partielles est majorée.</li>
<li><strong>Règle de comparaison n° 2</strong><br>
Soit $(u_n)$ et $(v_n)$ deux suites de nombres réels telles que :
$$0 \leq u_n \leq v_n \quad \text{à partir d'un certain rang.}
$$
<ul>
<li>Si la série $\sum v_n$ converge, la série $\sum u_n$ converge.</li>
<li>Si la série $\sum u_n$ diverge, la série $\sum v_n$ diverge.</li>
</ul>
</li>
<li><strong>Règle de comparaison n° 3</strong><br>
Soit $(u_n)$ une suite à valeurs réelles et $(v_n)$ une suite de nombres réels positifs. On suppose qu'il existe un réel $\lambda \neq 0$ tel que : $u_n \underset{+\infty}{\sim} \lambda v_n$.<br>
Alors les séries $\sum u_n$ et $\sum v_n$ sont de même nature.<br>
$\checkmark$ Il est important de se rappeler que cette règle ne s'applique qu'à des séries de signe constant (au moins à partir d'un certain rang).</li>
<li><strong>Règle de comparaison n° 4</strong><br>
Soit $(u_n)$ une suite à valeurs réelles et $(v_n)$ une suite de nombres réels positifs. On suppose que : $u_n \underset{+\infty}{=} O(v_n)$ (ou que $u_n \underset{+\infty}{=} o(v_n)$).<br>
Alors, si la série $\sum v_n$ converge, la série $\sum u_n$ converge (absolument).<br>
$\checkmark$On fera très attention là encore lors de l'utilisation de cette règle : la série $\sum u_n$ est à termes réels quelconques, mais il est indispensable que la série $\sum v_n$ à laquelle on la compare soit, elle, à termes réels positifs.</li>
</ul>

<!-- Image omitted: image0010126 -->
<!-- Page 122 -->
<ul>
<li><strong>Règle de d'Alembert</strong><br>
Soit $\sum u_n$ une série à termes réels strictement positifs. On suppose que $\lim_{n \to +\infty} \left( \frac{u_{n+1}}{u_n} \right) = \ell$ existe dans $\overline{\mathbb{R}}$ ($\ell \in [0; +\infty]$).
<ul>
<li>Si $\ell < 1$, la série $\sum u_n$ converge.</li>
<li>Si $\ell > 1$, la série $\sum u_n$ diverge.</li>
</ul>
$\checkmark$ Lorsque $\ell = 1$, on ne peut rien dire a priori. Exemples : $\sum \frac{1}{n}$ et $\sum \frac{1}{n^2}$.</li>
</ul>
<h2>[S11.7] Comparaison à une intégrale</h2>
<ul>
<li>Soit $f$ une fonction continue par morceaux sur un intervalle de la forme $[n_0; +\infty[$ ($n_0 \in \mathbb{N}$), à valeurs réelles positives et décroissante. Alors :
$$\text{la série } \sum_{n \geq n_0} f(n) \text{ converge } \Longleftrightarrow \int_{n_0}^{+\infty} f \text{ existe}.
$$
$\checkmark$ Il est important de retenir non seulement ce théorème, mais aussi et surtout sa démonstration. En effet, la méthode consistant à encadrer le terme général $f(n)$ par des intégrales de $f$ sur des segments permet d'obtenir plus généralement des encadrements des sommes partielles ou du reste de la série lorsqu'elle converge.<br>
Il faut donc retenir (ou savoir retrouver) l'encadrement suivant, valable pour tout $n \geq n_0 + 1$ :
$$\int_n^{n+1} f(t) \, dt \leq f(n) \leq \int_{n-1}^n f(t) \, dt.
$$
Il est illustré par la figure ci-contre.<br>
<!-- Image omitted: Diagramme de comparaison série-intégrale --></li>
</ul>
<ul>
<li><strong>Application : les séries de Riemann</strong><br>
La série $\sum_{n \in \mathbb{N}^*} \frac{1}{n^\alpha}$ converge si et seulement si $\alpha > 1$.</li>
<li><strong>Généralisation</strong><br>
Soit $f$ une fonction continue par morceaux sur un intervalle de la forme $[n_0; +\infty[$ ($n_0 \in \mathbb{N}$), à valeurs réelles positives et décroissante. Alors la série de terme général :
$$w_n = \int_{n-1}^n f(t) \, dt - f(n) \quad (n \geq n_0 + 1)
$$
est convergente.</li>
</ul>

<!-- Image omitted: image0010127 -->
<!-- Page 123 -->
<ul>
<li><strong>Application : la constante d'Euler</strong><br>
Il existe un réel $\gamma$ tel que, pour $n \in \mathbb{N}^*$ :
$$1 + \frac{1}{2} + \cdots + \frac{1}{n} \underset{n \to +\infty}{=} \ln n + \gamma + o(1).$$</li>
</ul>
<h2>[S11.8] Séries géométriques</h2>
<p>Soit $(A, +, \cdot, \times)$ une algèbre normée de dimension finie.<br>
Pour tout $x \in A$ tel que $\|x\| < 1$, la série géométrique $\sum_{n \in \mathbb{N}} x^n$ est absolument convergente (donc convergente). De plus, $1_A - x$ est inversible, et :
$$(1_A - x)^{-1} = \sum_{n=0}^{+\infty} x^n.$$
Enfin, l'application $x \mapsto (1_A - x)^{-1}$ est continue sur la boule unité ouverte $B(0_A, 1)$.</p>
<h2>[S11.9] Série exponentielle</h2>
<p>Soit $(A, +, \cdot, \times)$ une algèbre normée de dimension finie.<br>
Pour tout $x \in A$, la série $\sum_{n \in \mathbb{N}} \frac{1}{n!} x^n$ est absolument convergente (donc convergente). On pose alors :
$$\forall x \in A, \exp(x) = \sum_{n=0}^{+\infty} \frac{1}{n!} x^n.$$
La fonction exponentielle $x \mapsto \exp(x)$ est continue sur $A$.</p>
<ul>
<li>Lorsque $x$ est un nombre réel, l'inégalité de Taylor-Lagrange permet de démontrer que cette définition coïncide avec celle de la fonction exponentielle que vous connaissiez déjà.<br>
Exemple : on peut munir l'algèbre $\mathcal{M}_p(\mathbb{K})$ ($p \in \mathbb{N}^*$) d'une norme matricielle, c'est-à-dire qui vérifie :
$$\forall A, B \in \mathcal{M}_p(\mathbb{K}), \|AB\| \leq \|A\| \|B\|.$$
Le résultat précédent permet alors de définir l'exponentielle d'une matrice carrée, et plus généralement l'exponentielle d'un endomorphisme d'un $\mathbb{K}$-espace vectoriel de dimension finie.</li>
</ul>
<h2>[S11.10] Produit de Cauchy</h2>
<p>Soit $(u_n)_{n \in \mathbb{N}}$ et $(v_n)_{n \in \mathbb{N}}$ deux suites à valeurs dans une algèbre normée $(A, +, \cdot, \times)$ de dimension finie.<br>
On appelle série produit de Cauchy des séries de terme général $u_n$ d'une part et de terme général $v_n$ d'autre part, la série de terme général $w_n$ avec :
$$\forall n \in \mathbb{N}, w_n = \sum_{k=0}^{n} u_k \times v_{n-k} = \sum_{\substack{p+q=n \\ p, q \in \mathbb{N}}} u_p \times v_q.$$</p>

<!-- Image omitted: image0010128 -->
<!-- Page 124 -->
<p>Si $\sum u_n$ et $\sum v_n$ sont absolument convergentes, alors $\sum w_n$ est absolument convergente, et de plus :
$$\sum_{n=0}^{+\infty} w_n = \left( \sum_{n=0}^{+\infty} u_n \right) \left( \sum_{n=0}^{+\infty} v_n \right).
$$
Ce théorème permet de démontrer que si $x$ et $y$ sont deux éléments qui commutent d'une algèbre normée $(A, +, \cdot, \times)$ de dimension finie, on a :
$$\exp(x + y) = \exp(x) \times \exp(y).
$$
On en déduit que pour tout $x \in A$, $\exp(x)$ est inversible, d'inverse $\exp(-x)$.</p>
<h2>[S11.11] Séries alternées de nombres réels</h2>
<p>Une série à termes réels $\sum u_n$ est dite alternée si la suite $((-1)^n u_n)$ est de signe constant.</p>
<ul>
<li><strong>Critère spécial des séries alternées, ou critère de Leibniz</strong><br>
Soit $\sum u_n$ une série alternée. On suppose que :
<ul>
<li>la suite $(|u_n|)$ est décroissante ;</li>
<li>$\lim_{n \to +\infty} u_n = 0$.</li>
</ul>
Alors la série $\sum u_n$ converge.<br>
$\checkmark$ Il est important de noter que ce critère ne donne qu'une condition suffisante de convergence pour une série alternée. Par exemple, la série de terme général $u_n = \frac{(-1)^n}{n + (-1)^n}$ $(n \geq 2)$ est convergente, (on le montre en effectuant un développement limité), cependant elle ne vérifie pas les hypothèses du critère de Leibniz (la suite $(|u_n|)$ n'étant pas décroissante).</li>
<li><strong>Résultats complémentaires importants</strong><br>
Soit $\sum_{n \in \mathbb{N}} u_n$ une série alternée qui vérifie les hypothèses du critère spécial, et soit $S = \sum_{n=0}^{+\infty} u_n$ sa somme. On a les résultats suivants.
<ol type="i">
<li>$S$ est comprise entre deux sommes partielles d'indices consécutifs.</li>
<li>$S$ est du signe de $u_0$, et $|S| \leq |u_0|$.</li>
<li>Si l'on note $R_n = \sum_{k=n+1}^{+\infty} u_k$ le reste d'ordre $n$, alors $R_n$ est du signe de $u_{n+1}$ et $|R_n| \leq |u_{n+1}|$.</li>
</ol>
</li>
</ul>

<!-- Image omitted: image0010129 -->
<!-- Page 125 -->
<h2>[S11.12] Sommation des relations de comparaison</h2>
<h4>Cas des séries convergentes</h4>
<p>Soit $\sum_{n \in \mathbb{N}} u_n$ une série à termes réels et $\sum_{n \in \mathbb{N}} v_n$ une série à termes réels positifs (au moins à partir d'un certain rang) et convergente.<br>
On suppose que, lorsque $n \to +\infty$, $u_n = O(v_n)$ (respectivement $u_n = o(v_n)$), respectivement $u_n \sim v_n$.<br>
Alors :</p>
<ul>
<li>la série $\sum_{n \in \mathbb{N}} u_n$ est absolument convergente;</li>
<li>si l'on note, pour tout entier $n$, $r_n = \sum_{k=n+1}^{+\infty} u_k$ et $r'_n = \sum_{k=n+1}^{+\infty} v_k$, on a :
$r_n = O(r'_n)$ (respectivement $r_n = o(r'_n)$), respectivement $r_n \sim r'_n$.</li>
</ul>
<h4>Cas des séries divergentes</h4>
<p>Soit $\sum_{n \in \mathbb{N}} u_n$ une série à termes réels et $\sum_{n \in \mathbb{N}} v_n$ une série à termes réels positifs (au moins à partir d'un certain rang), toutes deux divergentes.<br>
On suppose que, lorsque $n \to +\infty$, $u_n = O(v_n)$ (respectivement $u_n = o(v_n)$), respectivement $u_n \sim v_n$.<br>
Alors, si l'on note, pour tout entier $n$, $S_n = \sum_{k=0}^n u_k$ et $S'_n = \sum_{k=0}^n v_k$, on a :
$S_n = O(S'_n)$ (respectivement $S_n = o(S'_n)$), respectivement $S_n \sim S'_n$.</p>
<h2>[S11.13] Ensembles dénombrables</h2>
<ul>
<li>Deux ensembles $E$ et $F$ sont dits équipotents s'il existe une bijection de $E$ sur $F$. C'est une relation d'équivalence.<br>
Dans le cas où $E$ et $F$ sont finis, ils sont équipotents si et seulement si ils ont même cardinal.</li>
<li>Un ensemble est dit dénombrable s'il est équipotent à $\mathbb{N}$.<br>
$E$ est dénombrable si et seulement si on peut le décrire sous la forme
$$E = \{x_n, \, n \in \mathbb{N}\}.$$
Toute partie infinie d'un ensemble dénombrable est dénombrable.</li>
<li>Un ensemble $E$ est dit au plus dénombrable s'il est équipotent à une partie de $\mathbb{N}$. Cela équivaut à dire qu'il est soit fini, soit dénombrable.<br>
$E$ est au plus dénombrable si et seulement si on peut le décrire sous la forme
$$E = \{x_i, \, i \in I\} \quad \text{où} \quad I \text{ est une partie de } \mathbb{N}.$$</li>
</ul>
<h4>Ensembles usuels</h4>
<ul>
<li>$\mathbb{Z}$ est dénombrable.</li>
</ul>

<!-- Image omitted: image0010130 -->
<!-- Page 126 -->
<ul>
<li>$N \times N$ est dénombrable.</li>
<li>Plus généralement, si $E$ et $F$ sont dénombrables, leur produit $E \times F$ l'est aussi.</li>
<li>$\mathbb{Q}$ est dénombrable.</li>
<li>Une réunion finie ou dénombrable d'ensembles dénombrables est un ensemble dénombrable.</li>
<li>L'ensemble $\mathcal{P}(\mathbb{N})$ des parties de $\mathbb{N}$ n'est pas dénombrable.</li>
<li>L'ensemble $\{0,1\}^{\mathbb{N}}$ des suites à valeurs dans $\{0,1\}$ n'est pas dénombrable.</li>
<li>$\mathbb{R}$ n'est pas dénombrable.</li>
</ul>
<h2>[S11.14] Familles sommables</h2>
<ul>
<li><strong>Famille sommable de réels positifs</strong><br>
Soit $I$ un ensemble dénombrable d'indices, et soit $(u_i)_{i \in I}$ une famille de réels positifs ou nuls.<br>
On dit que la famille est sommable s'il existe un réel positif $M$ tel que
$$\sum_{i \in J} u_i \leq M
$$pour toute partie finie $J$ incluse dans $I$.<br>
Dans ces conditions, la borne supérieure de l'ensemble des sommes finies $\sum_{i \in J} u_i$ (J parcourant l'ensemble de toutes les parties finies de $I$) est appelée la somme de la famille $(u_i)_{i \in I}$, et est notée $\sum_{i \in I} u_i$.<br>
Dans le cas où la famille n'est pas sommable, on écrira $\sum_{i \in I} u_i = +\infty$.</li>
<li><strong>Famille sommable de nombres réels</strong><br>
Soit $(u_i)_{i \in I}$ une famille de nombres réels quelconque, indexée par un ensemble dénombrable $I$.<br>
La famille $(u_i)_{i \in I}$ est dite sommable lorsque la famille $(|u_i|)_{i \in I}$ est une famille sommable de réels positifs.<br>
Rappelons que, pour tout nombre réel $x$, on pose :
$$x^+ = \max(x, 0) \quad \text{et} \quad x^- = \max(-x, 0),
$$de sorte que $x^+$ et $x^-$ sont des réels positifs tels que :
$$x = x^+ - x^- \quad \text{et} \quad |x| = x^+ + x^-.
$$
Pour que la famille de nombres réels $(u_i)_{i \in I}$ soit sommable, il faut et il suffit que les deux familles de nombres réels positifs $(u_i^+)_{i \in I}$ et $(u_i^-)_{i \in I}$ le soient.<br>
On posera alors, dans ce cas :
$$\sum_{i \in I} u_i = \sum_{i \in I} u_i^+ - \sum_{i \in I} u_i^-.
$$</li>
</ul>

<!-- Image omitted: image0010131 -->
<!-- Page 127 -->
<h3>Famille sommable de nombres complexes</h3>
<p>Soit $(u_i)_{i \in I}$ une famille de nombres complexes, indexée par un ensemble dénombrable $I$.<br>
La famille $(u_i)_{i \in I}$ est dite sommable lorsque la famille $(|u_i|)_{i \in I}$ est une famille sommable de réels positifs.<br>
Pour que la famille de nombres complexes $(u_i)_{i \in I}$ soit sommable, il faut et il suffit que les deux familles de nombres réels $(Re(u_i))_{i \in I}$ et $(Im(u_i))_{i \in I}$ le soient.<br>
On posera alors, dans ce cas : $\sum_{i \in I} u_i = \sum_{i \in I} Re(u_i) + i \sum_{i \in I} Im(u_i)$.</p>
<h3>Linéarité de la somme</h3>
<p>Soient $(u_i)_{i \in I}$ et $(v_i)_{i \in I}$ deux familles sommables de nombres complexes, et $\lambda, \mu$ deux nombres complexes.<br>
Alors la famille $(\lambda u_i + \mu v_i)_{i \in I}$ est sommable et :
$$\sum_{i \in I} \lambda u_i + \mu v_i = \lambda \sum_{i \in I} u_i + \mu \sum_{i \in I} v_i.$$</p>
<h3>Lien avec les séries</h3>
<p>Dans le cas où $I = \mathbb{N}$On a le résultat suivant.<br>
Une suite $(u_n)_{n \in \mathbb{N}}$ est sommable si et seulement si la série $\sum u_n$ est absolument convergente, et, dans ce cas, on a :
$$\sum_{n \in \mathbb{N}} u_n = \sum_{n=0}^{+\infty} u_n.$$</p>
<h3>Commutativité généralisée</h3>
<p>Pour qu'une famille dénombrable $(u_i)_{i \in I}$ de nombres complexes soit sommable, il faut et il suffit que pour une/toute bijection $\varphi$ de $\mathbb{N}$ sur $I$ la série de terme général $u_{\varphi(n)}$ soit absolument convergente. Et dans ce cas l'on a :
$$\sum_{i \in I} u_i = \sum_{n=0}^{+\infty} u_{\varphi(n)},$$
la somme ne dépendant pas de la bijection $\varphi: \mathbb{N} \to I$ choisie.</p>
<h3>Associativité généralisée, ou sommation par paquets</h3>
<p>Soit $(u_i)_{i \in I}$ une famille dénombrable de nombres complexes, et $(I_n)_{n \in \mathbb{N}}$ une partition dénombrable de $I$.<br>
La famille $(u_i)_{i \in I}$ est sommable si et seulement si :</p>
<ol type="i">
<li>chaque sous-famille $(u_i)_{i \in I_n}$ est sommable;</li>
<li>la série $\sum_{n \in \mathbb{N}} \left( \sum_{i \in I_n} |u_i| \right)$ est convergente.</li>
</ol>
<p>Dans ce cas, l'on a :
$$\sum_{n=0}^{+\infty} \left( \sum_{i \in I_n} u_i \right) = \sum_{i \in I} u_i.$$</p>

<!-- Image omitted: image0010132 -->
<!-- Page 128 -->
<h3>Suites doubles sommables</h3>
<p>On prend ici $I = \mathbb{N} \times \mathbb{N}$. Une famille $(u_{p,q})_{(p,q) \in \mathbb{N}^2}$ est appelée une suite double.</p>
<h4>Théorème de Fubini</h4>
<p>La suite double $(u_{p,q})_{(p,q) \in \mathbb{N}^2}$ est sommable si et seulement si les deux conditions suivantes sont réalisées :</p>
<ul>
<li>pour tout $q$ fixé dans $\mathbb{N}$, la série $\sum_{p \in \mathbb{N}} |u_{p,q}|$ est convergente; (Note: Correction de u_pq à |u_pq|)</li>
<li>la série de terme général $S_q = \sum_{p=0}^{+\infty} |u_{p,q}|$ est convergente.</li>
</ul>
<p>Dans ces conditions :</p>
<ul>
<li>pour tout $p$ fixé dans $\mathbb{N}$, la série $\sum_{q \in \mathbb{N}} |u_{p,q}|$ est convergente; (Note: Correction)</li>
<li>la série de terme général $S'_p = \sum_{q=0}^{+\infty} |u_{p,q}|$ est convergente;</li>
<li>on a les égalités :
$$
\sum_{p=0}^{+\infty} S'_p = \sum_{p=0}^{+\infty} \left( \sum_{q=0}^{+\infty} |u_{p,q}| \right) = \sum_{q=0}^{+\infty} S_q = \sum_{q=0}^{+\infty} \left( \sum_{p=0}^{+\infty} |u_{p,q}| \right)
$$
$$
\sum_{(p,q) \in \mathbb{N}^2} u_{p,q} = \sum_{p=0}^{+\infty} \left( \sum_{q=0}^{+\infty} u_{p,q} \right) = \sum_{q=0}^{+\infty} \left( \sum_{p=0}^{+\infty} u_{p,q} \right)
$$</li>
</ul>
<h3>Suite double produit</h3>
<p>Un cas particulier important et fréquent d'application du théorème de Fubini est le suivant.<br>
Soient $(a_n)_{n \in \mathbb{N}}$ et $(b_n)_{n \in \mathbb{N}}$ deux suites de nombres complexes.<br>
Pour que la suite double $(a_p b_q)_{(p,q) \in \mathbb{N}^2}$ soit sommable il faut et il suffit que les séries $\sum_{p \in \mathbb{N}} a_p$ et $\sum_{q \in \mathbb{N}} b_q$ soient absolument convergentes, et on a alors :
$$
\sum_{(p,q) \in \mathbb{N}^2} a_p b_q = \left( \sum_{p=0}^{+\infty} a_p \right) \left( \sum_{q=0}^{+\infty} b_q \right)
$$</p>

<!-- Image omitted: image0010133 -->
<!-- Page 129 -->
<h1>Thème 12 - Suites et séries de fonctions</h1>
<p>Dans ce chapitre, les fonctions sont définies sur une partie non vide $A$ d'un $\mathbb{K}$-espace vectoriel normé de dimension finie $E$ et à valeurs dans un $\mathbb{K}$-espace vectoriel normé de dimension finie $F$.<br>
Les normes sur ces espaces étant équivalentes, les notions introduites ne dépendent pas des normes choisies.</p>
<h2>[S12.1] Convergence d'une suite de fonctions</h2>
<ul>
<li>Soit $(f_n)_{n \in \mathbb{N}}$ une suite d'applications définies sur $A$, à valeurs dans $F$. On dit que cette suite converge simplement sur $A$ si et seulement si :
$$\forall x \in A, \lim_{n \to +\infty} f_n(x) \text{ existe (dans } F\text{)}.
$$
Dans ce cas, on peut définir une application $f : A \to F$ par :
$$\forall x \in A, f(x) = \lim_{n \to +\infty} f_n(x).
$$
$f$ s'appelle la limite simple de la suite $(f_n)_{n \in \mathbb{N}}$.</li>
<li>On dit que la suite $(f_n)_{n \in \mathbb{N}}$ converge uniformément sur $A$ vers $f$ si, à partir d'un certain rang, la suite $(f_n - f)$ est bornée sur $A$ et si :
$$\lim_{n \to +\infty} \|f_n - f\|_{\infty}^A = 0,
$$
où $\|f_n - f\|_{\infty}^A$ désigne $\sup_{x \in A} \|f_n(x) - f(x)\|_F$ (norme de la convergence uniforme). (Note: Correction de l'indice A dans la norme)</li>
</ul>
<p>✓ Lorsque l'on étudie la convergence uniforme, il est indispensable de préciser sur quelle partie elle a lieu; la phrase « $(f_n)_{n \in \mathbb{N}}$ converge uniformément vers $f$ » ne veut rien dire si l'on ne précise pas « sur $A$ ».</p>
<h2>[S12.2] Théorème de la double limite</h2>
<p>Soit $(f_n)_{n \in \mathbb{N}}$ une suite d'applications de $A$ dans $F$, qui converge uniformément sur $A$ vers une application $f : A \to F$.<br>
Soit $a$ un point adhérent à $A$. On suppose que, pour tout entier $n$ (au moins à partir d'un certain rang), la limite $\lim_{\substack{x \to a \\ x \in A}} f_n(x) = \ell_n$ existe.<br>
Alors la suite $(\ell_n)_{n \in \mathbb{N}}$ converge vers un élément $\ell \in F$, et de plus : $\lim_{\substack{x \to a \\ x \in A}} f(x) = \ell$.<br>
Autrement dit, en abrégé : $\lim_{\substack{x \to a \\ x \in A}} \left( \lim_{n \to +\infty} f_n(x) \right) = \lim_{n \to +\infty} \left( \lim_{\substack{x \to a \\ x \in A}} f_n(x) \right)$.<br>
✓ Lorsque $f$ est définie sur un intervalle de $\mathbb{R}$ qui est un voisinage de $\pm \infty$, le résultat de ce théorème peut s'étendre aux cas $a = \pm \infty$.</p>

<!-- Image omitted: image0010134 -->
<!-- Page 130 -->
<h2>[S12.3] Continuité de la fonction limite</h2>
<ul>
<li>Soit $a \in A$ et $(f_n)_{n \in \mathbb{N}}$ une suite d'applications de $A$ dans $F$, continues en $a$, qui converge vers une application $f : A \to F$, la convergence étant uniforme sur un voisinage de $a$.<br>
Alors $f$ est continue en $a$.</li>
<li>Soit $(f_n)_{n \in \mathbb{N}}$ une suite d'applications continues de $A$ dans $F$, qui converge vers une application $f : A \to F$, la convergence étant uniforme au voisinage de tout point de $A$. Alors $f$ est continue sur $A$.<br>
$\checkmark$ Ce théorème peut dans certains cas servir à démontrer qu'il n'y a pas convergence uniforme, en utilisant un raisonnement par l'absurde.</li>
</ul>
<h2>[S12.4] Intégration sur un segment</h2>
<ul>
<li>Soit $(f_n)$ une suite d'applications continues définies sur un intervalle $I$ de $\mathbb{R}$ et à valeurs dans $F$, et soit $a \in I$. On suppose que la suite $(f_n)$ converge simplement sur $I$ vers une application $f : I \to F$, la convergence étant uniforme sur tout segment inclus dans $I$.<br>
Pour tout $n \in \mathbb{N}$ et tout $x \in I$On pose :
$$g_n(x) = \int_a^x f_n(t) \, dt \quad \text{et} \quad g(x) = \int_a^x f(t) \, dt.$$
Alors la suite $(g_n)$ converge uniformément vers $g$ sur tout segment de $I$.</li>
<li><strong>Cas particulier</strong><br>
Soit $(f_n)$ une suite d'applications continues définies sur un segment $[a ; b]$ à valeurs dans $F$, qui converge uniformément sur $[a ; b]$ vers $f : [a ; b] \to F$. Alors :
$$\lim_{n \to +\infty} \int_a^b f_n(t) \, dt = \int_a^b f(t) \, dt.$$</li>
</ul>
<h2>[S12.5] Dérivation</h2>
<ul>
<li><strong>Dérivabilité de la limite d'une suite de fonctions de classe $\mathscr{C}^1$</strong><br>
Soit $(f_n)_{n \in \mathbb{N}}$ une suite d'applications définies sur un intervalle $I$ de $\mathbb{R}$, à valeurs dans $F$. On suppose que :
<ul>
<li>les $f_n$ sont de classe $\mathscr{C}^1$ sur $I$;</li>
<li>la suite $(f_n)_{n \in \mathbb{N}}$ converge simplement sur $I$ vers une fonction $f$;</li>
<li>la suite des dérivées $(f'_n)_{n \in \mathbb{N}}$ converge vers une application $g : I \to F$, la convergence étant uniforme sur tout segment inclus dans $I$.</li>
</ul>
Alors, la fonction $f$ est de classe $\mathscr{C}^1$ sur $I$, la suite $(f_n)_{n \in \mathbb{N}}$ converge uniformément vers $f$ sur tout segment inclus dans $I$, et, pour tout $x \in I$, $f'(x) = g(x)$ (soit, en abrégé, $(\lim f_n)' = \lim f'_n$).</li>
</ul>

<!-- Image omitted: image0010135 -->
<!-- Page 131 -->
<h3>Dérivabilité de la limite d'une suite de fonctions de classe $\mathscr{C}^k$</h3>
<p>Soit $(f_n)_{n \in \mathbb{N}}$ une suite de fonctions de classe $\mathscr{C}^k$ ($k \in \mathbb{N}^*$) sur un intervalle $I$ de $\mathbb{R}$, à valeurs dans $F$. On suppose que :</p>
<ul>
<li>pour tout $j \in 〚 0 ; k - 1 〛$, la suite de fonctions $(f_n^{(j)})_{n \in \mathbb{N}}$ converge simplement sur $I$;</li>
<li>la suite de fonctions $(f_n^{(k)})_{n \in \mathbb{N}}$ converge simplement sur $I$ vers une fonction $g$, la convergence étant uniforme sur tout segment inclus dans $I$.</li>
</ul>
<p>Alors, la fonction $f = \lim_{n \to +\infty} f_n$ est de classe $\mathscr{C}^k$ sur $I$, on a $f^{(k)} = g$, et pour $j \in 〚 0 ; k 〛$, chaque suite $(f_n^{(j)})_{n \in \mathbb{N}}$ converge uniformément vers $f^{(j)}$ sur tout segment inclus dans $I$.<br>
$\checkmark$ Pour les deux théorèmes précédents, plutôt que de démontrer la convergence uniforme sur tout segment de $I$, on peut se contenter de montrer la convergence uniforme sur tous les intervalles d'une famille $(J_\alpha)$ telle que $\bigcup_\alpha J_\alpha = I$.</p>
<h2>[S12.6] Approximation uniforme</h2>
<ul>
<li><strong>Approximation uniforme d'une fonction continue par morceaux par des fonctions en escalier</strong><br>
Toute fonction continue par morceaux sur un segment $[a ; b]$ à valeurs dans un espace vectoriel normé $F$ est limite uniforme sur $[a ; b]$ d'une suite de fonctions en escalier sur $[a ; b]$.</li>
<li><strong>Théorème de Weierstrass</strong><br>
Toute fonction continue sur un segment $[a ; b]$ à valeurs dans $\mathbb{C}$ est limite uniforme sur $[a ; b]$ d'une suite de fonctions polynomiales sur $[a ; b]$.</li>
</ul>
<h2>[S12.7] Convergence d'une série de fonctions</h2>
<p>Soit $(u_n)_{n \in \mathbb{N}}$ une suite d'applications définies sur une partie $A$ d'un $\mathbb{K}$-espace vectoriel normé $E$, à valeurs dans un $\mathbb{K}$-espace vectoriel normé $F$. On peut alors considérer la suite de fonctions $(S_n)_{n \in \mathbb{N}}$ définies par :
$$\forall x \in A, \quad S_n(x) = \sum_{k=0}^{n} u_k(x).
$$
Étudier la série de fonctions $\sum_{n \in \mathbb{N}} u_n$, c'est étudier la suite de fonctions $(S_n)_{n \in \mathbb{N}}$.</p>
<ul>
<li><strong>Convergence simple</strong><br>
On dit que la série de fonctions $\sum_{n \in \mathbb{N}} u_n$ converge simplement sur $A$ s'il existe une application $S : A \to F$ telle que la suite de fonctions $(S_n)_{n \in \mathbb{N}}$ converge simplement sur $A$ vers $S$.</li>
</ul>

<!-- Image omitted: image0010136 -->
<!-- Page 132 -->
<p>Cela signifie donc que, pour tout $x \in A$, la série $\sum_{n \in \mathbb{N}} u_n(x)$, à valeurs dans $F$, converge et que $S(x) = \sum_{n=0}^{+\infty} u_n(x)$.<br>
$S$ s'appelle alors la somme de la série de fonctions $\sum_{n \in \mathbb{N}} u_n$. On définit également le reste d'ordre $n$ : $R_n = S - S_n = \sum_{k=n+1}^{+\infty} u_k$. La suite de fonctions $(R_n)_{n \in \mathbb{N}}$ converge simplement sur $A$ vers la fonction nulle.</p>
<h3>Convergence uniforme</h3>
<p>On dit que la série de fonctions $\sum_{n \in \mathbb{N}} u_n$ converge uniformément sur $A$ s'il existe une application $S : A \to F$ telle que la suite de fonctions $(S_n)_{n \in \mathbb{N}}$ converge uniformément sur $A$ vers $S$.<br>
Cela équivaut à dire que la série de fonctions $\sum_{n \in \mathbb{N}} u_n$ converge simplement sur $A$ et que la suite des restes $(R_n)_{n \in \mathbb{N}}$ converge uniformément sur $A$ vers la fonction nulle (c'est-à-dire $\lim_{n \to +\infty} \|R_n\|_{\infty}^A = 0$).</p>
<ul>
<li>La convergence uniforme sur $I$ de la série de fonctions $\sum_{n \in \mathbb{N}} u_n$ implique nécessairement la convergence uniforme de la suite $(u_n)$ vers la fonction nulle sur $I$ (puisque $u_n = R_{n-1} - R_n$).<br>
En raisonnant par l'absurde, cette remarque peut être utilisée pour montrer qu'une série de fonctions ne converge pas uniformément sur $I$ : il suffit pour cela de trouver une suite $(x_n)$ d'éléments de $I$ telle que la suite $(u_n(x_n))$ ne converge pas vers 0.</li>
</ul>
<h3>Convergence normale</h3>
<p>On dit que la série de fonctions $\sum_{n \in \mathbb{N}} u_n$ converge normalement sur $A$ si :</p>
<ul>
<li>les fonctions $u_n$ sont bornées sur $A$ (au moins à partir d'un certain rang),</li>
<li>et la série numérique $\sum_{n \geq 0} \|u_n\|_{\infty}^A$ est convergente (en notant toujours $\|u_n\|_{\infty}^A = \sup_{x \in A} \|u_n(x)\|_F$).</li>
</ul>
<p>Si la série de fonctions $\sum_{n \in \mathbb{N}} u_n$ est normalement convergente sur $A$, alors :</p>
<ol type="i">
<li>pour tout $x \in A$, la série $\sum_{n \in \mathbb{N}} u_n(x)$ est absolument convergente dans $F$ (donc convergente puisque $F$ est de dimension finie) ;</li>
<li>la série de fonctions $\sum_{n \in \mathbb{N}} u_n$ est uniformément convergente sur $A$.</li>
</ol>

<!-- Image omitted: image0010137 -->
<!-- Page 133 -->
<h2>[S12.8] Convergence uniforme d'une série de fonctions et limite</h2>
<p>Soit $\sum_{n \in \mathbb{N}} u_n$ une série de fonctions définies sur $A$ à valeurs dans $F$, et soit $a \in \bar{A}$. On suppose que, pour tout entier $n$, la limite $\lim_{x \to a} u_n(x) = \ell_n$ existe dans $F$, et que la série $\sum_{n \in \mathbb{N}} u_n$ est uniformément convergente sur $A$. Notons $S = \sum_{n=0}^{+\infty} u_n$. Alors :
$$\sum_{n \in \mathbb{N}} \ell_n \text{ converge et } \lim_{x \to a} S(x) = \sum_{n=0}^{+\infty} \ell_n
$$
(soit, en abrégé : $\lim_{x \to a} \left( \sum_{n=0}^{+\infty} u_n \right) = \sum_{n=0}^{+\infty} \lim_{x \to a} u_n$).<br>
Ce résultat porte le nom de théorème de la double limite.</p>
<h2>[S12.9] Convergence uniforme d'une série de fonctions et continuité</h2>
<p>Soit $\sum_{n \in \mathbb{N}} u_n$ une série de fonctions définies sur $A$, à valeurs dans $F$. Si les $u_n$ sont continues sur $A$ et si la série converge uniformément au voisinage de tout point de $A$, alors sa somme $S$ est continue sur $A$.</p>
<h2>[S12.10] Convergence d'une série de fonctions et intégration sur un segment</h2>
<p>Soit $\sum_{n \in \mathbb{N}} u_n$ une série de fonctions définies sur un segment $[a; b] \subset \mathbb{R}$, à valeurs dans $F$. On suppose que les $u_n$ sont continues sur $[a; b]$, et que la série $\sum_{n \in \mathbb{N}} u_n$ converge uniformément sur $[a; b]$. Notons $S = \sum_{n=0}^{+\infty} u_n$. Alors $S$ est continue sur $[a; b]$, la série $\sum_{n \in \mathbb{N}} \int_a^b u_n(t) \, dt$ converge, et :
$$\int_a^b S(t) \, dt = \sum_{n=0}^{+\infty} \int_a^b u_n(t) \, dt.
$$
Remarque : le théorème s'applique également lorsque les $u_n$ sont seulement continues par morceaux, mais il faut alors vérifier la continuité par morceaux de $S$, celle-ci n'étant plus assurée par la convergence uniforme.</p>
<h2>[S12.11] Convergence d'une série de fonctions et dérivation</h2>
<ul>
<li>Soit $\sum_{n \in \mathbb{N}} u_n$ une série de fonctions définies sur un intervalle $I$ de $\mathbb{R}$, à valeurs dans $F$. On suppose que :</li>
</ul>

<!-- Image omitted: image0010138 -->
<!-- Page 134 -->
<ul>
<li>les $u_n$ sont de classe $\mathscr{C}^1$ sur $I$;</li>
<li>la série de fonctions $\sum_{n \in \mathbb{N}} u_n$ converge simplement sur $I$; on notera $S$ sa somme;</li>
<li>la série de fonctions $\sum_{n \in \mathbb{N}} u_n'$ converge simplement sur $I$, la convergence étant uniforme sur tout segment inclus dans $I$.</li>
</ul>
<p>Alors :</p>
<ul>
<li>la fonction $S$ est de classe $\mathscr{C}^1$ sur $I$;</li>
<li>la série de fonctions $\sum_{n \in \mathbb{N}} u_n$ converge uniformément sur tout segment de $I$;</li>
<li>pour tout $x \in I$, on a : $S'(x) = \sum_{n=0}^{+\infty} u_n'(x)$.</li>
</ul>
<ul>
<li>Comme pour les suites de fonctions, on a également un énoncé pour démontrer que la somme d'une série de fonctions est de classe $\mathscr{C}^k$.<br>
Soit $\sum_{n \in \mathbb{N}} u_n$ une série de fonctions définies sur un intervalle $I$, à valeurs dans $F$. On suppose que :
<ul>
<li>les $u_n$ sont de classe $\mathscr{C}^k$ sur $I$;</li>
<li>$\forall j \in 〚 0 ; k-1 〛$, la série de fonctions $\sum_{n \in \mathbb{N}} u_n^{(j)}$ converge simplement sur $I$;</li>
<li>la série de fonctions $\sum_{n \in \mathbb{N}} u_n^{(k)}$ converge simplement sur $I$, la convergence étant uniforme sur tout segment inclus dans $I$.</li>
</ul>
Alors la fonction somme $S$ est de classe $\mathscr{C}^k$ sur $I$, chaque série $\sum_{n \in \mathbb{N}} u_n^{(j)}$ avec $j \in 〚 0 ; k 〛$ converge uniformément vers $S^{(j)}$ sur tout segment de $I$ et :
$$\forall j \in 〚 0 ; k 〛, \forall x \in I, S^{(j)}(x) = \sum_{n=0}^{+\infty} u_n^{(j)}(x).
$$
$\checkmark$ Pour les deux théorèmes précédents, plutôt que de démontrer la convergence uniforme sur tout segment de $I$, on peut se contenter de montrer la convergence uniforme sur tous les intervalles d'une famille $(J_\alpha)$ telle que $\bigcup_\alpha J_\alpha = I$.</li>
</ul>

<!-- Image omitted: image0010139 -->
<!-- Page 135 -->
<h1>Thème 13 - Séries entières</h1>
<h2>[S13.1] Rayon de convergence d'une série entière</h2>
<ul>
<li>Une série entière est une série de fonctions de la forme :
$$\sum_{n \in \mathbb{N}} a_n z^n, $$
où $z$ est la variable complexe et $(a_n)_{n \in \mathbb{N}}$ une suite de nombres complexes.</li>
<li><strong>Lemme d'Abel</strong><br>
S'il existe un complexe non nul $z_0$ tel que la suite $(a_n z_0^n)_{n \in \mathbb{N}}$ soit bornée, alors pour tout nombre complexe $z$ tel que $|z| < |z_0|$, la série numérique $\sum_{n \in \mathbb{N}} a_n z^n$ est absolument convergente.</li>
<li><strong>Définition du rayon de convergence</strong><br>
Soit $\sum_{n \in \mathbb{N}} a_n z^n$ une série entière. L'ensemble des réels positifs $r$ tels que la suite $(a_n r^n)_{n \in \mathbb{N}}$ soit bornée est un intervalle contenant 0.<br>
On appelle alors rayon de convergence $R$ de cette série entière la borne supérieure (dans $\mathbb{R}$) de cet intervalle :
$$ R = \sup \{ r \in \mathbb{R}_+ \mid (a_n r^n) \text{ bornée} \} \quad (R \in \mathbb{R}_+ \cup \{+\infty\}). $$</li>
<li><strong>Disque de convergence</strong><br>
Soit $\sum_{n \in \mathbb{N}} a_n z^n$ une série entière de rayon de convergence $R$.
<ul>
<li>Si $R

  ```html
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MATHS MP/MP* - Savoir & Faire en Prépas (Suite 3)</title>
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css">
    <!-- KaTeX JS -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"></script>
    <!-- KaTeX auto-render extension -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 2em;
            max-width: 800px; /* Limit width for readability */
            margin-left: auto;
            margin-right: auto;
        }
        h1, h2, h3, h4, h5, h6 {
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }
        h1 { border-bottom: 2px solid #ccc; padding-bottom: 0.3em; }
        h2 { border-bottom: 1px solid #eee; padding-bottom: 0.2em; }
        p { margin-bottom: 1em; }
        ul, ol { margin-left: 2em; margin-bottom: 1em; }
        li { margin-bottom: 0.5em; }
        blockquote {
            margin-left: 2em;
            padding-left: 1em;
            border-left: 3px solid #eee;
            font-style: italic;
            color: #555;
        }
        code {
            background-color: #f4f4f4;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
        }
        pre code {
            display: block;
            padding: 1em;
            overflow-x: auto;
        }
       .katex-display { /* Style for KaTeX display math */
            display: block;
            margin-top: 1em;
            margin-bottom: 1em;
            overflow-x: auto; /* Handle long formulas */
            overflow-y: hidden;
        }
        .center { text-align: center; }
        .small { font-size: 0.8em; }
        .isbn { font-family: monospace; }
        table {
            border-collapse: collapse;
            margin-bottom: 1em;
            width: 100%;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 0.5em;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>

<!-- Continuation from page 135 -->
<ul>
<li>Si $R > 0$, alors pour tout $z$ tel que $|z| < R$, la série $\sum_{n \in \mathbb{N}} a_n z^n$ est absolument convergente.</li>
<li>Si $R < +\infty$ alors pour tout $z$ tel que $|z| > R$, la série $\sum_{n \in \mathbb{N}} a_n z^n$ est (grossièrement) divergente.</li>
<li>Si $R = 0$, la série ne converge que pour $z = 0$.</li>
<li>Si $R = +\infty$, la série est convergente (absolument) pour tout $z \in \mathbb{C}$.</li>
</ul>
La boule ouverte $B(0, R)$ dans $\mathbb{C}$ s'appelle le disque de convergence.<br>
✓ On ne peut rien dire dans le cas général du comportement de la série entière sur le cercle d'incertitude $\{ z \in \mathbb{C} \mid |z| = R \}$.</li>
<li><strong>Autres caractérisations du rayon de convergence</strong><br>
Le rayon de convergence $R$ d'une série entière $\sum_{n \in \mathbb{N}} a_n z^n$ peut être défini par l'une des propriétés suivantes, toutes équivalentes :
$$ R = \sup \left\{ |z|, z \in \mathbb{C} \mid \text{la série } \sum_{n \in \mathbb{N}} a_n z^n \text{ est absolument convergente} \right\}; $$
</li>
</ul>

<!-- Image omitted: image0010140 -->
<!-- Page 136 -->
<ul>
<li>$R = \sup \left\{ |z|, z \in \mathbb{C} \left| \text{la série } \sum_{n \in \mathbb{N}} a_n z^n \text{ est convergente} \right. \right\}$;</li>
<li>$R = \sup \left\{ |z|, z \in \mathbb{C} \left| \lim_{n \to +\infty} a_n z^n = 0 \right. \right\}$;</li>
<li>$R = \sup \left\{ |z|, z \in \mathbb{C} \left| \text{la suite } (a_n z^n) \text{ est bornée} \right. \right\}$ (définition).</li>
</ul>
<h3>Utilisation des règles de comparaison</h3>
<p>Soient $\sum_{n \in \mathbb{N}} a_n z^n$ et $\sum_{n \in \mathbb{N}} b_n z^n$ deux séries entières de rayons de convergence respectifs $R_a$ et $R_b$.</p>
<ul>
<li>Si $a_n \sim b_n$, alors $R_a = R_b$.</li>
<li>Si $|a_n| \leq |b_n|$ (au moins à partir d'un certain rang), alors $R_a \geq R_b$.</li>
<li>Si $a_n = O(b_n)$ (ou $a_n = o(b_n)$), alors $R_a \geq R_b$.</li>
</ul>
<h3>Critère de d'Alembert</h3>
<p>Si la suite $(a_n)_{n \in \mathbb{N}}$ ne s'annule pas (au moins à partir d'un certain rang), et si la suite $\left( \left| \frac{a_{n+1}}{a_n} \right| \right)_{n \in \mathbb{N}}$ possède une limite $\ell \in \overline{\mathbb{R}}$ lorsque $n \to +\infty$, alors la série entière $\sum_{n \in \mathbb{N}} a_n z^n$ a pour rayon de convergence : $R = \frac{1}{\ell}$.</p>
<blockquote>
<p>On fera particulièrement attention à ne pas utiliser ce critère dans le cas où la série entière peut avoir des coefficients nuls. Dans ce cas, il convient d'utiliser la règle de d'Alembert pour les séries numériques (cf. [S11.6] page 122) pour étudier la convergence de la série $\sum |a_n z^n|$.</p>
</blockquote>
<h2>[S13.2] Opérations sur les séries entières</h2>
<h3>Somme de deux séries entières</h3>
<p>Soient $\sum_{n \in \mathbb{N}} a_n z^n$ et $\sum_{n \in \mathbb{N}} b_n z^n$ deux séries entières de rayons de convergence respectifs $R_a$ et $R_b$.</p>
<ul>
<li>Si $R_a \neq R_b$, la série entière $\sum_{n \in \mathbb{N}} (a_n + b_n) z^n$ a pour rayon de convergence $R = \min(R_a, R_b)$.</li>
<li>Si $R_a = R_b$, la série entière $\sum_{n \in \mathbb{N}} (a_n + b_n) z^n$ a un rayon de convergence $R \geq R_a$.</li>
<li>Dans les deux cas on a, pour tout $z$ tel que $|z| < \min(R_a, R_b)$ :
$$\sum_{n=0}^{+\infty} (a_n + b_n) z^n = \sum_{n=0}^{+\infty} a_n z^n + \sum_{n=0}^{+\infty} b_n z^n.
$$</li>
</ul>
<h3>Produit de Cauchy de deux séries entières</h3>
<p>Soient $\sum_{n \in \mathbb{N}} a_n z^n$ et $\sum_{n \in \mathbb{N}} b_n z^n$ deux séries entières de rayons de convergence respectifs $R_a$ et $R_b$.<br>
Posons, pour tout $n \in \mathbb{N}$ : $c_n = \sum_{k=0}^{n} a_k b_{n-k} = \sum_{\substack{p, q \in \mathbb{N} \\ p+q=n}} a_p b_q$.</p>

<!-- Image omitted: image0010141 -->
<!-- Page 137 -->
<ul>
<li>Le rayon de convergence $R_c$ de la série entière $\sum_{n \in \mathbb{N}} c_n z^n$ est tel que $R_c \ge \min(R_a, R_b)$. (Note: corrigé de > à >=)</li>
<li>De plus, pour tout complexe $z$ tel que $|z| < \min(R_a, R_b)$, on a :
$$\sum_{n=0}^{+\infty} c_n z^n = \left( \sum_{n=0}^{+\infty} a_n z^n \right) \left( \sum_{n=0}^{+\infty} b_n z^n \right).
$$</li>
</ul>
<h2>[S13.3] Convergence normale d'une série entière</h2>
<p>La série entière $\sum_{n \in \mathbb{N}} a_n z^n$ est normalement convergente sur tout compact inclus dans son disque ouvert de convergence.</p>
<ul>
<li>Il n'y a pas nécessairement convergence normale ni même convergence uniforme de la série entière sur tout le disque de convergence. Le contre-exemple qui suit est classique, et doit être su.<br>
La série entière de la variable réelle $\sum_{n \in \mathbb{N}^*} (-1)^{n-1} \frac{x^n}{n}$ converge simplement vers $x \mapsto \ln(1 + x)$ sur $]-1; 1]$.<br>
Il n'y a pas convergence normale sur $]-1; 1[$ (car $\| u_n \|_{\infty}^{]-1; 1[} = \frac{1}{n}$).<br>
Il n'y a pas convergence uniforme sur $]-1; 1[$ (car le reste $\sum_{k=n+1}^{+\infty} (-1)^{k-1} \frac{x^k}{k}$ n'est pas borné au voisinage de -1).<br>
Il y a cependant convergence uniforme sur tout segment $[a; 1]$ avec $-1 < a < 1$ (cela se montre en utilisant la majoration du reste d'une série alternée).</li>
</ul>
<h2>[S13.4] Continuité de la somme d'une série entière</h2>
<p>La fonction somme d'une série entière $z \mapsto \sum_{n=0}^{+\infty} a_n z^n$ est continue sur son disque ouvert de convergence.</p>
<p><strong>Conséquence</strong><br>
Si $f$ est la somme d'une série entière de la variable réelle $x$, $f$ admet au voisinage de 0 un développement limité à tout ordre $p \in \mathbb{N}^*$, obtenu par troncature de la série entière : $f(x) = \sum_{n=0}^{p} a_n x^n + O(x^{p+1})$.</p>
<h2>[S13.5] Dérivation</h2>
<ul>
<li><strong>Séries dérivée et primitive</strong><br>
Soit $\sum_{n \in \mathbb{N}} a_n z^n$ une série entière.
<ul>
<li>On appelle série dérivée de cette série la série entière :
$$\sum_{n \geq 1} n a_n z^{n-1} = \sum_{n \geq 0} (n + 1) a_{n+1} z^n.
$$</li>
</ul>
</li>
</ul>

<!-- Image omitted: image0010142 -->
<!-- Page 138 -->
<ul>
<li>On appelle série primitive de cette série la série entière :
$$\sum_{n \geqslant 0} \frac{a_n}{n+1} z^{n+1} = \sum_{n \geqslant 1} \frac{a_{n-1}}{n} z^n.
$$</li>
<li>Une série entière, sa série dérivée et sa série primitive ont même rayon de convergence.</li>
<li><strong>Dérivation d'une série entière de la variable réelle</strong><br>
Soit $(a_n)$ une suite d'éléments de $\mathbb{C}$, et $\sum_{n \in \mathbb{N}} a_n x^n$ une série entière de la variable réelle $x$, de rayon de convergence $R > 0$. Posons, pour $x \in ]-R ; R[$, $f(x) = \sum_{n=0}^{+\infty} a_n x^n$. Alors $f$ est de classe $\mathscr{C}^1$ sur $]-R ; R[$ et :
$$\forall x \in ]-R ; R[, f'(x) = \sum_{n=1}^{+\infty} n a_n x^{n-1} = \sum_{n=0}^{+\infty} (n+1) a_{n+1} x^n.
$$
<strong>Conséquence</strong><br>
Avec les mêmes notations, la fonction $f$ est de classe $\mathscr{C}^{\infty}$ sur sur $]-R ; R[$ et, pour tout entier naturel $k$ et pour tout $x \in ]-R ; R[$ :
$$f^{(k)}(x) = \sum_{n=k}^{+\infty} \frac{n!}{(n-k)!} a_n x^{n-k} = \sum_{n=0}^{+\infty} \frac{(n+k)!}{n!} a_{n+k} x^n.
$$
Avec les mêmes notations on a aussi :
$$\forall k \in \mathbb{N}, a_k = \frac{f^{(k)}(0)}{k!}.
$$</li>
<li><strong>Unicité du développement en série entière</strong><br>
Soient deux séries entières $f(x) = \sum_{n=0}^{+\infty} a_n x^n$ et $g(x) = \sum_{n=0}^{+\infty} b_n x^n$, de rayons de convergence non nuls.<br>
Si $f$ et $g$ coïncident dans un voisinage de $0$, on a $a_n = b_n$ pour tout entier $n$.</li>
</ul>
<h2>[S13.6] Intégration d'une série entière de la variable réelle</h2>
<p>Soit $(a_n)$ une suite d'éléments de $\mathbb{C}$, et $\sum_{n \in \mathbb{N}} a_n x^n$ une série entière de la variable réelle $x$, de rayon de convergence $R > 0$. Posons, pour $x \in ]-R ; R[$, $f(x) = \sum_{n=0}^{+\infty} a_n x^n$.</p>
<ul>
<li>Pour tout segment $[a ; b] \subset ]-R ; R[$, on a : $\int_a^b f(x) \, \mathrm{d}x = \sum_{n=0}^{+\infty} \int_a^b a_n x^n \, \mathrm{d}x$.</li>
<li>En particulier, pour tout $x \in ]-R ; R[$, on a : $\int_0^x f(t) \, \mathrm{d}t = \sum_{n=0}^{+\infty} \frac{a_n x^{n+1}}{n+1}$.</li>
</ul>

<!-- Image omitted: image0010143 -->
<!-- Page 139 -->
<h2>[S13.7] Fonction développable en série entière</h2>
<p>On dit qu'une fonction $f : \mathbb{C} \rightarrow \mathbb{C}$ est développable en série entière dans un voisinage $V$ de 0 s'il existe une série entière $\sum_{n \in \mathbb{N}} a_n z^n$ de rayon de convergence $R > 0$ telle que, pour tout $z \in V$, $f(z) = \sum_{n=0}^{+\infty} a_n z^n$.<br>
D'après les résultats précédents, on a donc nécessairement, dans le cas d'une fonction d'une variable réelle :</p>
<ul>
<li>$f$ est de classe $\mathscr{C}^{\infty}$ sur $]-R ; R[$ ;</li>
<li>$\forall n \in \mathbb{N}, a_n = \frac{f^{(n)}(0)}{n!}$.</li>
</ul>
<p>Ainsi, la série entière $\sum_{n \in \mathbb{N}} a_n x^n$ n'est autre que la série de Taylor de $f$ en 0.<br>
$\checkmark$ Cette propriété n'admet pas de réciproque.</p>
<ul>
<li>Il existe des fonctions de classe $\mathscr{C}^{\infty}$ au voisinage de 0 dont la série de Taylor en 0 a un rayon de convergence nul.</li>
<li>Il existe des fonctions de classe $\mathscr{C}^{\infty}$ au voisinage de 0 dont la série de Taylor en 0 a un rayon de convergence non nul, mais dont la somme ne coïncide pas avec $f$ (sauf en 0).</li>
</ul>
<h2>[S13.8] Développements en série entière usuels</h2>
<p>Les développements en série entière des fonctions usuelles sont rappelés page suivante. Quelques remarques concernant ce tableau.</p>
<ul>
<li>Les développements en série entière de $\sqrt{1+x}$ et de $\frac{1}{\sqrt{1+x}}$ n'y figurent pas, car ils s'obtiennent directement à partir du développement de $(1+x)^{\alpha}$ pour $\alpha = \pm \frac{1}{2}$. Cependant, il faut savoir écrire ces développements sous forme synthétique. On trouve :
$\forall x \in ]-1 ; 1[, \sqrt{1+x} = 1 + \sum_{n=1}^{+\infty} (-1)^{n-1} \frac{(2n-2)!}{2^{2n-1} n! (n-1)!} x^n$,<br>
et<br>
$\forall x \in ]-1 ; 1[, \frac{1}{\sqrt{1+x}} = 1 + \sum_{n=1}^{+\infty} (-1)^n \frac{(2n)!}{2^{2n} (n!)^2} x^n$.</li>
<li>De même, le développement en série entière de $\operatorname{Arcsin}$ s'obtient simplement en intégrant celui de $\frac{1}{\sqrt{1-x^2}}$.<br>
Il faut savoir le retrouver ; on obtient :
$\forall x \in ]-1 ; 1[, \operatorname{Arcsin}(x) = \sum_{n=0}^{+\infty} \frac{(2n)!}{2^{2n} (n!)^2} \frac{x^{2n+1}}{2n+1}$.</li>
</ul>

<!-- Image omitted: image0010144 -->
<!-- Page 140 -->
<h3>Développements en série entière usuels</h3>

<table>
<thead>
<tr>
<th>Fonction</th>
<th>Développement en série entière</th>
<th>Domaine</th>
</tr>
</thead>
<tbody>
<tr>
<td>$e^x$</td>
<td>$\sum_{n=0}^\infty \frac{x^n}{n!}$</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>$\sinh(x)$</td>
<td>$\sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)!}$</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>$\cosh(x)$</td>
<td>$\sum_{n=0}^\infty \frac{x^{2n}}{(2n)!}$</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>$\sin(x)$</td>
<td>$\sum_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{(2n+1)!}$</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>$\cos(x)$</td>
<td>$\sum_{n=0}^\infty (-1)^n \frac{x^{2n}}{(2n)!}$</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>$\frac{1}{1-x}$</td>
<td>$\sum_{n=0}^\infty x^n$</td>
<td>$]-1; 1[$</td>
</tr>
<tr>
<td>$\frac{1}{1+x}$</td>
<td>$\sum_{n=0}^\infty (-1)^n x^n$</td>
<td>$]-1; 1[$</td>
</tr>
<tr>
<td>$\ln(1+x)$</td>
<td>$\sum_{n=1}^\infty (-1)^{n-1} \frac{x^n}{n}$ (Note: Correction (-1)^{n+1} à (-1)^{n-1})</td>
<td>$]-1; 1]$</td>
</tr>
<tr>
<td>$\frac{1}{2} \ln\left(\frac{1+x}{1-x}\right)$</td>
<td>$\sum_{n=0}^\infty \frac{x^{2n+1}}{2n+1}$</td>
<td>$]-1; 1[$</td>
</tr>
<tr>
<td>$\arctan(x)$</td>
<td>$\sum_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{2n+1}$</td>
<td>$[-1; 1]$</td>
</tr>
<tr>
<td>$(1+x)^\alpha$</td>
<td>$\sum_{n=0}^\infty \binom{\alpha}{n} x^n$ avec $\binom{\alpha}{n} = \frac{\alpha (\alpha-1) \cdots (\alpha-n+1)}{n!}$</td>
<td>$]-1; 1[$</td>
</tr>
</tbody>
</table>

<!-- Image omitted: image0010145 -->
<!-- Page 141 -->
<h1>Thème 14 - Intégration sur un intervalle quelconque</h1>
<p>On considère dans ce chapitre des applications à valeurs dans $\mathbb{K} = \mathbb{R}$Ou $\mathbb{C}$.</p>
<h2>[S14.1] Définitions</h2>
<ul>
<li>Soit $f$ une fonction continue par morceaux sur un intervalle $[a;b[$ avec $-\infty < a < b \leq +\infty$, à valeurs dans $\mathbb{K}$.<br>
On dit que l'intégrale généralisée (ou impropre)$\int_{a}^{b} f(t) \, dt$ converge si :
$$\lim_{x \to b^{-}} \int_{a}^{x} f(t) \, dt \text{ existe.}
$$
S'il en est ainsi, cette intégrale est notée :
$$\int_{a}^{b} f(t) \, dt \text{ ou } \int_{a}^{b} f \text{ ou } \int_{[a;b[} f.
$$
Dans le cas contraire on dit que l'intégrale généralisée $\int_{a}^{b} f$ diverge.</li>
<li><strong>Intégrale faussement impropre</strong><br>
Soit $f$ continue par morceaux sur $[a;b[$ avec $a$ et $b$ finis, à valeurs dans $\mathbb{K}$.<br>
Si $f$ admet une limite finie en $b^{-}$, alors l'intégrale $\int_{[a;b[} f$ converge.<br>
De plus, si l'on note $\tilde{f}$ le prolongement de $f$ par continuité en $b$, on a :
$$\int_{[a;b[} f = \int_{a}^{b} \tilde{f}.
$$
$\checkmark$ Attention à ne pas utiliser ce résultat dans le cas où $b = +\infty$ !<br>
Une fonction définie sur un intervalle de la forme $[a;+\infty[$ peut avoir une limite en $+\infty$ sans que son intégrale converge. Il existe même des fonctions dont l'intégrale converge et qui n'ont pas de limite en $+\infty$.<br>
Ainsi, contrairement aux séries, il n'y a aucun lien entre limite en $+\infty$ et existence de l'intégrale dans le cas général.</li>
<li>Soit $f$ une fonction continue par morceaux sur $[a;b[$ avec $-\infty < a < b \leq +\infty$, à valeurs dans $\mathbb{K}$, et soit $c \in ]a;b[$. L'intégrale généralisée $\int_{a}^{b} f(t) \, dt$ converge si et seulement si l'intégrale $\int_{c}^{b} f(t) \, dt$ converge et, dans ce cas :
$$\int_{a}^{b} f(t) \, dt = \int_{a}^{c} f(t) \, dt + \int_{c}^{b} f(t) \, dt.
$$
$\checkmark$ Ainsi, pour une fonction continue par morceaux sur $[a;b[$, l'existence de l'intégrale est une notion locale au voisinage de $b$.</li>
</ul>

<!-- Image omitted: image0010146 -->
<!-- Page 142 -->
<ul>
<li>Soit $f$ une fonction continue par morceaux sur un intervalle $]a ; b]$ avec $-\infty \leqslant a < b < +\infty$, à valeurs dans $\mathbb{K}$.<br>
On dit que l'intégrale généralisée $\int_{a}^{b} f(t) \, dt$ converge si :
$$\lim_{x \to a^{+}} \int_{x}^{b} f(t) \, dt \quad \text{existe.}
$$
S'il en est ainsi, cette intégrale est notée :
$$\int_{a}^{b} f(t) \, dt \quad \text{ou} \quad \int_{a}^{b} f \quad \text{ou} \quad \int_{]a ; b]} f.
$$
Dans le cas contraire on dit que l'intégrale $\int_{a}^{b} f$ diverge.</li>
<li>Soit $f$ une application continue par morceaux sur un intervalle $]a ; b[$ avec $-\infty \leqslant a < b \leqslant +\infty$ à valeurs dans $\mathbb{K}$. Les propriétés suivantes sont équivalentes :
<ol type="i">
<li>il existe $c \in ]a ; b[$ tel que les intégrales $\int_{a}^{c} f(t) \, dt$ et $\int_{c}^{b} f(t) \, dt$ convergent ;</li>
<li>pour tout $c \in ]a ; b[$, les intégrales $\int_{a}^{c} f(t) \, dt$ et $\int_{c}^{b} f(t) \, dt$ convergent.</li>
</ol>
Si tel est le cas, la somme des deux intégrales généralisées $\int_{a}^{c} f(t) \, dt + \int_{c}^{b} f(t) \, dt$ ne dépend pas de $c$. On la note :
$$\int_{a}^{b} f(t) \, dt \quad \text{ou} \quad \int_{a}^{b} f \quad \text{ou} \quad \int_{]a ; b[} f,
$$
et on dit alors que l'intégrale généralisée $\int_{a}^{b} f$ converge.<br>
On dira donc que l'intégrale $\int_{a}^{b} f$ diverge s'il existe $c \in ]a ; b[$ tel que l'une au moins des intégrales $\int_{a}^{c} f(t) \, dt$Ou $\int_{c}^{b} f(t) \, dt$ diverge.</li>
</ul>
<h2>[S14.2] Propriétés</h2>
<p>Les propriétés qui suivent sont énoncées dans le cas d'une fonction continue par morceaux sur un intervalle de la forme $]a ; b[$ avec $-\infty \leqslant a < b \leqslant +\infty$. On obtient bien sûr des résultats analogues pour les deux autres cas d'intégrale généralisée.</p>
<ul>
<li>Soit $f$ et $g$ deux fonctions continues par morceaux sur $]a ; b[$ à valeurs dans $\mathbb{K}$ et $\lambda, \mu$ deux scalaires.<br>
On suppose que les intégrales $\int_{a}^{b} f(t) \, dt$ et $\int_{a}^{b} g(t) \, dt$ convergent.<br>
Alors l'intégrale généralisée $\int_{a}^{b} (\lambda f + \mu g)(t) \, dt$ converge et :
$$\int_{a}^{b} (\lambda f + \mu g)(t) \, dt = \lambda \int_{a}^{b} f(t) \, dt + \mu \int_{a}^{b} g(t) \, dt.
$$</li>
</ul>

<!-- Image omitted: image0010147 -->
<!-- Page 143 -->
<p>En d'autres termes : le sous-ensemble de $\mathscr{C}\mathscr{M}(]a;b[, \mathbb{K})$ formé des fonctions dont l'intégrale converge est un sous-espace vectoriel de $\mathscr{C}\mathscr{M}(]a;b[, \mathbb{K})$, et sur cet espace, l'application $f \mapsto \int_a^b f$ est une forme linéaire.<br>
$\checkmark$ Si $f$ a une intégrale divergente sur $[a;b[$ et $g$ une intégrale convergente, alors l'intégrale de $f + g$ est divergente.<br>
On ne peut cependant rien dire a priori de la somme de deux intégrales divergentes.</p>
<h3>Positivité</h3>
<ul>
<li>Soit $f$ une fonction continue par morceaux sur $[a;b[$, à valeurs réelles.
<ul>
<li>Si $f \geq 0$ sur $[a;b[$ et si l'intégrale de $f$ est convergente, alors $\int_a^b f(t) \, dt \geq 0$.</li>
<li>Soient $f$ et $g$ deux fonctions continues par morceaux sur $[a;b[$, à valeurs réelles. Si $f(t) \leq g(t)$ pour tout $t \in [a;b[$ et si les intégrales de $f$ et de $g$ convergent, alors $\int_a^b f(t) \, dt \leq \int_a^b g(t) \, dt$.</li>
</ul>
</li>
<li>Soit $f$ une fonction continue sur $[a;b[$, à valeurs réelles positives, telle que l'intégrale de $f$ sur $[a;b[$ est convergente.<br>
Alors : $\int_a^b f(t) \, dt = 0 \Longrightarrow f = 0$ sur $[a;b[$.</li>
</ul>
<h3>Cas d'une fonction à valeurs dans $\mathbb{C}$</h3>
<p>Soit $f$ une fonction continue par morceaux sur $[a;b[$, à valeurs dans $\mathbb{C}$. Pour que l'intégrale de $f$ sur $[a;b[$ soit convergente, il faut et il suffit que les intégrales de $\mathscr{R}e(f)$ et de $\mathscr{I}m(f)$ le soient, et, dans ce cas on a
$$\int_a^b f(t) \, dt = \int_a^b \mathscr{R}e(f(t)) \, dt + i \int_a^b \mathscr{I}m(f(t)) \, dt.
$$</p>
<h2>[S14.3] Règles de convergence pour les fonctions à valeurs positives</h2>
<p>Les propriétés qui suivent sont énoncées dans le cas d'une fonction continue par morceaux sur un intervalle de la forme $[a;b[$ avec $-\infty < a < b \leq +\infty$. On obtient des résultats analogues pour une fonction continue par morceaux sur un intervalle de la forme $]a;b]$ avec $-\infty \leq a < b < +\infty$.<br>
$\checkmark$ Cependant, dans le cas d'une fonction définie sur un intervalle ouvert $]a;b[$, il faut faire deux études séparées, l'une au voisinage de $a$ et l'autre au voisinage de $b$.</p>
<ul>
<li>Soit $f$ continue par morceaux sur $[a;b[$, avec $-\infty < a < b \leq +\infty$, à valeurs réelles positives.<br>
Soit l'application $F : x \in [a;b[ \mapsto \int_a^x f(t) \, dt$.<br>
Alors l'intégrale $\int_a^b f(t) \, dt$ converge si et seulement si $F$ est majorée et, dans ce cas, $\int_a^b f(t) \, dt = \lim_{x \to b^-} F(x)$.<br>
Dans le cas contraire, $\lim_{x \to b^-} \int_a^x f(t) \, dt = +\infty$.</li>
</ul>

<!-- Image omitted: image0010148 -->
<!-- Page 144 -->
<ul>
<li>Soient $f$ et $g$ continues par morceaux sur $[a ; b[$, avec $-\infty < a < b \leq +\infty$, à valeurs réelles.<br>
On suppose qu'il existe $c \in [a ; b[$ tel que :
$$\forall t \in [c ; b[, 0 \leq f(t) \leq g(t).
$$
<ul>
<li>Si $\int_{a}^{b} g(t) \, dt$ converge, alors $\int_{a}^{b} f(t) \, dt$ converge.</li>
<li>Si $\int_{a}^{b} f(t) \, dt$ diverge, alors $\int_{a}^{b} g(t) \, dt$ diverge.</li>
</ul>
</li>
<li>Soient $f$ et $g$ continues par morceaux sur $[a ; b[$, avec $-\infty < a < b \leq +\infty$, à valeurs réelles, positives au voisinage de $b$.
<ul>
<li>Si $f \underset{b}{\sim} g$ (ou $f \underset{b^{-}}{=} O(g)$) et si $\int_{a}^{b} g(t) \, dt$ converge, alors $\int_{a}^{b} f(t) \, dt$ converge. (Note: Correction, $\sim$ ou O)</li>
<li>Si $f \underset{b^{-}}{=} o(g)$ et si $\int_{a}^{b} g(t) \, dt$ converge, alors $\int_{a}^{b} f(t) \, dt$ converge.</li>
</ul>
</li>
<li>Soient $f$ et $g$ continues par morceaux sur $[a ; b[$, avec $-\infty < a < b \leq +\infty$, à valeurs réelles.<br>
On suppose $g$ positive au voisinage de $b$. S'il existe une constante réelle $k \neq 0$ telle que $f \underset{b^{-}}{\sim} k g$, alors les intégrales de $f$ et $g$ sur $[a ; b[$ sont de même nature.</li>
</ul>
<h2>[S14.4] Intégrales de référence</h2>
<p>Pour pouvoir utiliser les critères de comparaison précédents, il faut connaître un certain nombre d'intégrales de référence.</p>
<ul>
<li><strong>Exponentielle et logarithme</strong>
<ul>
<li>$\int_{0}^{1} \ln t \, dt$ converge (et $\int_{0}^{1} \ln t \, dt = -1$).</li>
<li>$\int_{0}^{+\infty} e^{-at} \, dt$ converge si et seulement si $a > 0$ (et dans ce cas $\int_{0}^{+\infty} e^{-at} \, dt = \frac{1}{a}$).</li>
</ul>
</li>
<li><strong>Fonctions de Riemann</strong><br>
Il s'agit des fonctions $t \mapsto \frac{1}{t^{\alpha}}$ pour $t > 0$ et $\alpha \in \mathbb{R}$.
<ul>
<li>$\int_{1}^{+\infty} \frac{dt}{t^{\alpha}}$ converge si et seulement si $\alpha > 1$.</li>
<li>$\int_{0}^{1} \frac{dt}{t^{\alpha}}$ converge si et seulement si $\alpha < 1$.</li>
</ul>
</li>
<li>Et plus généralement, si $a$ et $b$ sont des réels finis tels que $a < b$, alors :
$$\int_{a}^{b} \frac{dt}{(t - a)^{\alpha}} \quad \text{converge si et seulement si} \quad \alpha < 1.
$$
(Note: Correction pour la borne inférieure)</li>
</ul>

<!-- Image omitted: image0010149 -->
<!-- Page 145 -->
<h2>[S14.5] Changement de variable dans une intégrale généralisée</h2>
<p>Soit $f$ une fonction continue par morceaux sur un intervalle $] \alpha ; \beta [$ à valeurs réelles ou complexes, et $\varphi$ une bijection strictement monotone d'un intervalle $] a ; b [$ sur $] \alpha ; \beta [$, de classe $\mathscr{C}^1$ sur $] a ; b [$.<br>
Alors l'intégrale $\int_{\alpha}^{\beta} f$ est convergente si et seulement si l'intégrale $\int_{a}^{b} (f \circ \varphi) \varphi'$ l'est, et, dans ce cas :
$$\int_{\alpha}^{\beta} f(t) \, dt = \int_{a}^{b} f \circ \varphi (u) \varphi'(u) \, du.
$$</p>
<h2>[S14.6] Intégration par parties</h2>
<p>Soient $f$ et $g$ deux fonctions continues et de classe $\mathscr{C}^1$ par morceaux sur un intervalle $] a ; b [$. Si $\lim_{t \to b^-} f(t) g(t)$ et $\lim_{t \to a^+} f(t) g(t)$ existent, alors les intégrales $\int_{a}^{b} f' g$ et $\int_{a}^{b} f g'$ sont de même nature, et, lorsqu'elles convergent, on a : (Note: Corrigé pour inclure la limite en a)
$$\int_{a}^{b} f'(t) g(t) \, dt = \left[ f(t) g(t) \right]_{a^+}^{b^-} - \int_{a}^{b} f(t) g'(t) \, dt.
$$
Ce que l'on écrit encore $\int_{a}^{b} f'(t) g(t) \, dt = \left[ f(t) g(t) \right]_{a}^{b} - \int_{a}^{b} f(t) g'(t) \, dt$.<br>
Ce théorème s'adapte sans difficulté au cas d'un intervalle de la forme $] a ; b]$ ou $[a ; b[$.</p>
<ul>
<li>Pour appliquer ce théorème il est indispensable de vérifier proprement l'existence de la limite (ou des limites). En cas de doute, on reviendra à la formule sur un segment, et à la fin des calculs on fera un (ou des) passage(s) à la limite.</li>
</ul>
<h2>[S14.7] Intégrales absolument convergentes</h2>
<p>Lorsqu'une fonction n'est pas à valeurs réelles positives (ou de signe constant), les théorèmes de comparaison vus plus haut ne s'appliquent pas. On peut cependant s'y ramener dans certains cas grâce à la définition et au théorème suivants.</p>
<ul>
<li><strong>Définition</strong><br>
Soit $f$ une fonction continue par morceaux sur un intervalle $[a ; b[$ avec $-\infty < a < b \leq +\infty$, à valeurs dans $\mathbb{K}$.<br>
On dit que l'intégrale $\int_{a}^{b} f(t) \, dt$ est absolument convergente si l'intégrale $\int_{a}^{b} |f(t)| \, dt$ est convergente ($|f(t)|$ désigne la valeur absolue de $f(t)$ si $f$ est à valeurs réelles, et son module si $f$ est à valeurs dans $\mathbb{C}$).<br>
Lorsque l'intégrale $\int_{a}^{b} f(t) \, dt$ est absolument convergente, on dit aussi que $f$ est intégrable sur $[a ; b[$.<br>
On a bien sûr une définition analogue dans le cas d'une fonction définie sur un intervalle de la forme $] a ; b]$Ou $] a ; b [$.</li>
</ul>

<!-- Image omitted: image0010150 -->
<!-- Page 146 -->
<h3>Théorème</h3>
<p>Soit $f$ une fonction continue par morceaux sur un intervalle $[a ; b[$ avec $-\infty < a < b \leqslant +\infty$, à valeurs dans $\mathbb{K}$.<br>
Si l'intégrale $\int_{a}^{b} f$ est absolument convergente, elle est convergente, et alors :
$$\left| \int_{a}^{b} f(t) \, \mathrm{d}t \right| \leqslant \int_{a}^{b} |f(t)| \, \mathrm{d}t.
$$
$\checkmark$ La réciproque du théorème précédent est fausse.<br>
Il existe en effet des intégrales qui sont convergentes sans être absolument convergentes (une telle intégrale est dite semi-convergente). Exemple : $\int_{0}^{+\infty} \frac{\sin t}{t} \, \mathrm{d}t$ est convergente mais pas absolument convergente.</p>
<ul>
<li>Le théorème précédent permet d'utiliser certains critères de comparaison même lorsqu'on ne connaît pas le signe de la fonction; on a en particulier le résultat suivant.<br>
Soient $f$ et $g$ continues par morceaux sur $[a ; b[$, avec $-\infty < a < b \leqslant +\infty$.<br>
On suppose (seulement)$g$ à valeurs réelles positives au voisinage de $b$.<br>
Si $f \underset{b^{-}}{=} O(g)$Ou si $f \underset{b^{-}}{=} o(g)$ et si $\int_{a}^{b} g(t) \, \mathrm{d}t$ converge, alors $\int_{a}^{b} f(t) \, \mathrm{d}t$ est absolument convergente.</li>
</ul>
<h2>[S14.8] Intégration des relations de comparaison</h2>
<p>Les théorèmes suivants permettent d'obtenir une estimation du « reste » d'une intégrale convergente ou des « intégrales partielles » d'une intégrale divergente. Ces théorèmes sont à mettre en parallèle avec ceux concernant la sommation des relations de comparaison pour les séries, rappelés en [S11.12].<br>
Dans ce qui suit, $\mathbb{K}$ désigne $\mathbb{R}$Ou $\mathbb{C}$, et $[a ; b[$ un intervalle de $\mathbb{R}$ avec $a < b \leqslant +\infty$. On obtient des résultats similaires dans le cas de fonctions définies sur un intervalle $]a ; b]$ avec $-\infty \leqslant a < b$.</p>
<h3>Cas des fonctions intégrables</h3>
<ul>
<li>Soit $f$ une fonction continue par morceaux sur $[a ; b[$ à valeurs dans $\mathbb{K}$ et $g$ une fonction continue par morceaux sur $[a ; b[$ à valeurs réelles positives.<br>
On suppose que $g$ est intégrable sur $[a ; b[$ et que $f(x) \underset{x \to b^{-}}{=} O(g(x))$ (respectivement $f(x) \underset{x \to b^{-}}{=} o(g(x))$).<br>
Alors $f$ est intégrable sur $[a ; b[$ et
$$\int_{x}^{b} f(t) \, \mathrm{d}t \underset{x \to b^{-}}{=} O\left( \int_{x}^{b} g(t) \, \mathrm{d}t \right)
$$
(respectivement $\int_{x}^{b} f(t) \, \mathrm{d}t \underset{x \to b^{-}}{=} o\left( \int_{x}^{b} g(t) \, \mathrm{d}t \right)$).</li>
</ul>

<!-- Image omitted: image0010151 -->
<!-- Page 147 -->
<ul>
<li>Soit $f$ une fonction continue par morceaux sur $[a ; b[$ à valeurs dans $\mathbb{R}$ et $g$ une fonction continue par morceaux sur $[a ; b[$ à valeurs <em>réelles positives</em>.<br>
On suppose que $g$ est intégrable sur $[a ; b[$ et que $f(x) \underset{x \to b^-}{\sim} g(x)$.<br>
Alors $f$ est intégrable sur $[a ; b[$ et l'on a : $\int_x^b f(t) \, dt \underset{x \to b^-}{\sim} \int_x^b g(t) \, dt$. (Note: correction de a à x dans la borne inf)</li>
</ul>
<h3>Cas des fonctions non intégrables</h3>
<ul>
<li>Soit $f$ une fonction continue par morceaux sur $[a ; b[$ à valeurs dans $\mathbb{R}$ et $g$ une fonction continue par morceaux sur $[a ; b[$ à valeurs <em>réelles positives</em>.<br>
On suppose que : $g$ n'est <em>pas</em> intégrable sur $[a ; b[$ et que $f(x) \underset{x \to b^-}{=} O(g(x))$ (respectivement $f(x) \underset{x \to b^-}{=} o(g(x))$).<br>
Alors :
$$\int_a^x f(t) \, dt \underset{x \to b^-}{=} O\left(\int_a^x g(t) \, dt\right)
$$
(respectivement $\int_a^x f(t) \, dt \underset{x \to b^-}{=} o\left(\int_a^x g(t) \, dt\right)$).</li>
<li>Soit $f$ une fonction continue par morceaux sur $[a ; b[$ à valeurs dans $\mathbb{R}$ et $g$ une fonction continue par morceaux sur $[a ; b[$ à valeurs <em>réelles positives</em>.<br>
On suppose que $g$ n'est pas intégrable sur $[a ; b[$ et que $f(x) \underset{x \to b^-}{\sim} g(x)$.<br>
Alors $f$ n'est pas intégrable sur $[a ; b[$ et $\int_a^x f(t) \, dt \underset{x \to b^-}{\sim} \int_a^x g(t) \, dt$.</li>
</ul>
<h2>[S14.9] Suite de fonctions intégrables</h2>
<p>Contrairement à l'intégration sur un segment (cf. [S12.2]), même la convergence uniforme d'une suite de fonctions $(f_n)_{n \in \mathbb{N}}$ vers une fonction $f$ ne suffit pas, lorsque l'intervalle $I$ est quelconque, à assurer que :
$$\lim_{n \to +\infty} \int_I f_n = \int_I f.
$$
Il nous faut un résultat plus puissant; il s'agit du théorème de convergence dominée de Lebesgue.<br>
Soit $(f_n)_{n \in \mathbb{N}}$ une suite de fonctions continues par morceaux sur un intervalle $I$, à valeurs dans $\mathbb{K}$. (Note: Correction de R à K) On suppose que :</p>
<ul>
<li>la suite de fonctions $f_n$ converge simplement sur $I$ vers une fonction $f$ continue par morceaux sur $I$;</li>
<li>il existe une fonction $\varphi$ continue par morceaux sur $I$, à valeurs réelles positives, et intégrable sur $I$, telle que : $\forall n \in \mathbb{N}, |f_n| \leq \varphi$ sur $I$ (hypothèse de domination).</li>
</ul>
<p>Alors les $f_n$ sont intégrables sur $I$, $f$ est intégrable sur $I$, et
$$\int_I f = \lim_{n \to +\infty} \int_I f_n
$$
(l'existence de cette limite est assurée par le théorème).</p>

<!-- Image omitted: image0010152 -->
<!-- Page 148 -->
<h2>[S14.10] Série de fonctions intégrables</h2>
<p>Deux théorèmes sont disponibles pour calculer l'intégrale de la somme d'une série de fonctions, en plus de celui déjà vu en [S12.10] dans le cas où il y a convergence uniforme sur un segment.</p>
<h3>Théorème d'intégration terme à terme</h3>
<p>Soit $(u_n)_{n \in \mathbb{N}}$ une suite de fonctions continues par morceaux sur un intervalle $I$, à valeurs dans $\mathbb{K}$. (Note: Correction de C à K) On suppose que :</p>
<ul>
<li>$\forall n \in \mathbb{N}$, $u_n$ est intégrable sur $I$;</li>
<li>la série de fonctions $\sum u_n$ converge simplement sur $I$, et sa fonction somme $s$ est continue par morceaux sur $I$;</li>
<li>la série $\sum \int_I |u_n|$ converge.</li>
</ul>
<p>Alors $s = \sum_{n=0}^{+\infty} u_n$ est intégrable sur $I$, et $\int_I s = \sum_{n=0}^{+\infty} \int_I u_n$ (cette série étant absolument convergente).</p>
<h3>Application du théorème de convergence dominée</h3>
<p>Soit $(u_n)_{n \in \mathbb{N}}$ une suite de fonctions continues par morceaux sur un intervalle $I$, à valeurs dans $\mathbb{K}$. (Note: Correction de C à K) On notera, pour $n \in \mathbb{N}$, $s_n = \sum_{k=0}^n u_k$. On suppose que :</p>
<ul>
<li>la série de fonctions $\sum u_n$ converge simplement sur $I$, et sa fonction somme $s = \sum_{n=0}^{+\infty} u_n$ est continue par morceaux sur $I$;</li>
<li>il existe une fonction $\varphi$ continue par morceaux sur $I$, à valeurs réelles positives, et intégrable sur $I$, telle que : $\forall n \in \mathbb{N}, |s_n| \leq \varphi$ sur $I$ (hypothèse de domination).</li>
</ul>
<p>Alors toutes les fonctions $u_n$, ainsi que $s$, sont intégrables sur $I$, et $\int_I s = \sum_{n=0}^{+\infty} \int_I u_n$ (la convergence de cette série étant assurée par le théorème).</p>
<h2>[S14.11] Intégrales dépendant d'un paramètre</h2>
<h3>Continuité</h3>
<p>Soit $f$ une fonction définie sur $A \times I$, où $A$ est une partie d'un espace vectoriel normé de dimension finie et où $I$ est un intervalle de $\mathbb{R}$, à valeurs dans $\mathbb{K} = \mathbb{R}$Ou $\mathbb{C}$. On suppose que :</p>
<ul>
<li>$\forall x \in A$, $t \mapsto f(x, t)$ est continue par morceaux sur $I$;</li>
<li>$\forall t \in I$, la fonction $x \mapsto f(x, t)$ est continue sur $A$;</li>
<li>il existe une fonction $\varphi$, continue par morceaux sur $I$, à valeurs dans $\mathbb{R}_+$ et intégrable sur $I$ telle que :
$\forall (x, t) \in A \times I, |f(x, t)| \leq \varphi(t)$ (hypothèse de domination).</li>
</ul>

<!-- Image omitted: image0010153 -->
<!-- Page 149 -->
<p>Alors la fonction $g: x \mapsto \int_I f(x, t) \, dt$ est définie et continue sur $A$.<br>
<strong>Remarque</strong> : le résultat de ce théorème reste vrai si l'on suppose seulement que l'hypothèse de domination est vérifiée pour $(x, t) \in V \times I$, pour tout voisinage $V$ de tout point de $A$ (domination locale).<br>
$\checkmark$ La remarque précédente est très importante, et simplifie souvent beaucoup la recherche d'une fonction dominante.</p>
<h3>Passage à la limite</h3>
<p>Le résultat suivant étend le théorème de convergence dominée.<br>
Soit $f$ une fonction définie sur $A \times I$, où $A$ et $I$ sont des intervalles de $\mathbb{R}$, à valeurs dans $\mathbb{K} = \mathbb{R}$Ou $\mathbb{C}$. Soit $a \in \bar{A}$. On suppose que :</p>
<ul>
<li>$\forall x \in A, t \mapsto f(x, t)$ est continue par morceaux sur $I$;</li>
<li>$\forall t \in I, \lim_{x \to a} f(x, t) = \ell(t)$, avec $\ell$ continue par morceaux sur $I$;</li>
<li>il existe une fonction $\varphi$, continue par morceaux sur $I$, à valeurs dans $\mathbb{R}_+$ et intégrable sur $I$ telle que :
$\forall (x, t) \in A \times I, |f(x, t)| \leq \varphi(t)$ (hypothèse de domination).</li>
</ul>
<p>Alors $\ell$ est intégrable sur $I$ et : $\lim_{x \to a} \int_I f(x, t) \, dt = \int_I \ell(t) \, dt$.<br>
Remarque : le résultat de ce théorème reste vrai si l'on suppose seulement que l'hypothèse de domination est vérifiée pour $(x, t) \in V \times I$, avec $V$ voisinage de $a$ dans $A$.</p>
<h3>Dérivabilité</h3>
<p>Soit $f$ une fonction définie sur $J \times I$, où $I$ et $J$ sont des intervalles réels, à valeurs dans $\mathbb{K} = \mathbb{R}$Ou $\mathbb{C}$. On suppose que :</p>
<ul>
<li>$\forall x \in J, t \mapsto f(x, t)$ est continue par morceaux et intégrable sur $I$;</li>
<li>$\forall t \in I$, la fonction $x \mapsto f(x, t)$ est dérivable sur $J$;</li>
<li>pour tout $x \in J$, la fonction $t \mapsto \frac{\partial f}{\partial x}(x, t)$ est continue par morceaux sur $I$;</li>
<li>pour tout $t \in I$, la fonction $x \mapsto \frac{\partial f}{\partial x}(x, t)$ est continue sur $J$;</li>
<li>il existe une fonction $\varphi$, continue par morceaux sur $I$, à valeurs dans $\mathbb{R}_+$ et intégrable sur $I$ telle que :
$\forall (x, t) \in J \times I, \left| \frac{\partial f}{\partial x}(x, t) \right| \leq \varphi(t)$ (hypothèse de domination).</li>
</ul>
<p>Alors la fonction $g: x \mapsto \int_I f(x, t) \, dt$ est de classe $\mathcal{C}^1$ sur $J$, et :
$$\forall x \in J, g'(x) = \int_I \frac{\partial f}{\partial x}(x, t) \, dt \quad \text{(formule de Leibniz)}.
$$
<strong>Remarque</strong> : le résultat de ce théorème reste vrai si l'on suppose seulement que l'hypothèse de domination est vérifiée pour $(x, t) \in K \times I$, pour tout segment $K$ inclus dans $J$.<br>
$\checkmark$ Si l'on veut démontrer que la fonction $g$ est de classe $\mathcal{C}^1$, appliquez directement ce dernier théorème sans passer par le premier !</p>

<!-- Image omitted: image0010154 -->
<!-- Page 150 -->
<h3>Généralisation</h3>
<p>Le résultat suivant se démontre aisément par récurrence à partir du précédent.<br>
Soit $f$ une fonction définie sur $J \times I$, où $I$ et $J$ sont des intervalles réels, à valeurs dans $\mathbb{K} = \mathbb{R}$Ou $\mathbb{C}$. Soit $n \in \mathbb{N}^*$.<br>
On suppose que :</p>
<ul>
<li>$f, \frac{\partial f}{\partial x}, \ldots, \frac{\partial^n f}{\partial x^n}$ existent, sont continues par rapport à $x$ sur $J$ et continues par morceaux par rapport à $t$ sur $I$;</li>
<li>$\forall x \in J$, les fonctions $t \mapsto f(x,t), t \mapsto \frac{\partial f}{\partial x}(x,t), \ldots, t \mapsto \frac{\partial^{n-1} f}{\partial x^{n-1}}(x,t)$ sont intégrables sur $I$;</li>
<li>il existe une fonction $\varphi$, continue par morceaux sur $I$, à valeurs dans $\mathbb{R}_+$ et intégrable sur $I$ telle que :
$$
\forall (x,t) \in J \times I, \left| \frac{\partial^n f}{\partial x^n}(x,t) \right| \leq \varphi(t) \quad (\text{hypothèse de domination}).
$$</li>
</ul>
<p>Alors la fonction $g : x \mapsto \int_I f(x,t) \, dt$ est de classe $\mathcal{C}^n$ sur $J$, et :
$$
\forall k \in 〚 0; n 〛, \forall x \in J, g^{(k)}(x) = \int_I \frac{\partial^k f}{\partial x^k}(x,t) \, dt.
$$
<strong>Remarque</strong> : le résultat de ce théorème reste vrai si l'on suppose seulement que l'hypothèse de domination est vérifiée pour $(x,t) \in K \times I$, pour tout segment $K$ de $J$.<br>
$\checkmark$ Si l'on veut démontrer que $g$ est de classe $\mathcal{C}^n$ voire $\mathcal{C}^\infty$, il faut utiliser directement ce dernier théorème.</p>

<!-- Image omitted: image0010155 -->
<!-- Page 151 -->
<h1>Thème 15 - Espaces probabilisés</h1>
<p>Avant d'aborder les probabilités, il nous a paru indispensable de faire quelques rappels de 1re année concernant les dénombrements et les coefficients binomiaux. Ce chapitre utilise aussi les résultats sur les ensembles dénombrables, les familles sommables et les séries doubles qui ont été rappelés en [S11.13] et [S11.14].</p>
<h2>[S15.1] Dénombrement</h2>
<h3>$p$-listes</h3>
<p>Soient $n$ et $p$ deux entiers strictement positifs et $E$ un ensemble à $n$ éléments.<br>
Une $p$-liste d'éléments de $E$ est un $p$-uplet $(x_1, \ldots, x_p)$ d'éléments de $E$.</p>
<ul>
<li>Dans une $p$-liste, les éléments ne sont pas nécessairement distincts, mais le rang des éléments dans la $p$-liste est important.<br>
Le nombre de $p$-listes d'un ensemble $E$ à $n$ éléments est $\text{Card}(E^p) = n^p$.</li>
</ul>
<h3>$p$-listes d'éléments distincts de $E$, ou arrangements</h3>
<p>Soient $n$ et $p$ deux entiers tels que $0 \leq p \leq n$ et $E$ un ensemble à $n$ éléments.<br>
Une $p$-liste d'éléments distincts de $E$Ou $p$-arrangement est une $p$-liste $(x_1, \ldots, x_p)$ d'éléments de $E$ telle que $x_i \neq x_j$ pour $i \neq j$.</p>
<ul>
<li>Dans un $p$-arrangement, les éléments sont tous distincts, et le rang des éléments dans la liste est important.<br>
Si $\text{Card } E = n$, le nombre de $p$-listes d'éléments distincts de $E$ est le nombre noté $A_n^p$ égal à :
$$A_n^p = n(n-1) \cdots (n-p+1) = \frac{n!}{(n-p)!}$$</li>
</ul>
<h3>Permutations</h3>
<p>Une permutation d'un ensemble $E$ de cardinal $n$ est une $n$-liste d'éléments distincts de $E$.<br>
Le nombre de permutations d'un ensemble $E$ de cardinal $n$ est : $A_n^n = n!$.</p>
<h3>Parties d'un ensemble</h3>
<p>Soit $E$ un ensemble de cardinal $n$ ($n \in \mathbb{N}$) et $\mathcal{P}(E)$ l'ensemble des parties de $E$. Alors : $\text{Card } \mathcal{P}(E) = 2^n$.</p>
<h3>Parties de $E$ à $p$ éléments, combinaisons</h3>
<p>Soit $p \in \mathbb{N}$. On appelle combinaison à $p$ éléments d'un ensemble $E$ toute partie de $E$ contenant $p$ éléments.</p>
<ul>
<li>Dans une combinaison, les éléments sont tous distincts et leur rang est sans importance.<br>
Si $\text{Card } E = n$, le nombre de combinaisons à $p$ éléments de $E$ ($p \leq n$) se note $\binom{n}{p}$ et l'on a :
$$\binom{n}{p} = \frac{1}{p!} A_n^p = \frac{n!}{p!(n-p)!}$$</li>
</ul>

<!-- Image omitted: image0010156 -->
<!-- Page 152 -->
<p>On posera $\binom{0}{0} = 1$.<br>
Si $n \in \mathbb{N}$ et $p \in \mathbb{Z}$ sont tels que $p < 0$Ou $p > n$, on posera $\binom{n}{p} = 0$.</p>
<h3>Propriétés des coefficients binomiaux</h3>
<ol>
<li>Pour $n \in \mathbb{N}$, $\binom{n}{0} = \binom{n}{n} = 1$; $\binom{n}{1} = n$; $\binom{n}{2} = \frac{n(n-1)}{2}$.</li>
<li>Pour $n \in \mathbb{N}$ et $k \in \mathbb{Z}$, $\binom{n}{k} = \binom{n}{n-k}$.</li>
<li>Formule sans nom :
$$
\forall n \in \mathbb{N}, \forall k \in \mathbb{Z}, k \binom{n}{k} = n \binom{n-1}{k-1}.
$$</li>
<li>Formule de Pascal :
$$
\forall n \in \mathbb{N}, \forall k \in \mathbb{Z}, \binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}.
$$</li>
<li>Généralisation de la formule de Pascal :
$$
\text{pour } 0 \leq p \leq n, \sum_{k=p}^{n} \binom{k}{p} = \binom{n+1}{p+1}.
$$</li>
<li>Formule de Vandermonde :
$$
\forall (n, m, k) \in \mathbb{N}^3, \sum_{i=0}^{k} \binom{n}{i} \binom{m}{k-i} = \binom{n+m}{k}.
$$</li>
<li>Cas particulier : $\sum_{i=0}^{n} \binom{n}{i}^2 = \binom{2n}{n}$.</li>
<li>Formule du binôme :
$$
\forall n \in \mathbb{N}, \forall (a, b) \in \mathbb{C}^2, \quad (a+b)^n = \sum_{k=0}^{n} \binom{n}{k} a^{k} b^{n-k}.
$$ (Note: Correction des exposants a^{n-k} b^k à a^k b^{n-k})</li>
</ol>
<h3>Applications de $E_p$ dans $F_n$</h3>
<p>Soient $p$ et $n$ deux entiers strictement positifs, $E_p$ un ensemble de cardinal $p$ dont les éléments sont notés $(e_1, \ldots, e_p)$ et $F_n$ un ensemble de cardinal $n$.<br>
À toute $p$-liste $(y_1, \ldots, y_p)$ d'éléments de $F_n$ correspond une et une seule application de $E_p$ dans $F_n$ définie par : $f(e_i) = y_i$ pour tout $i$.<br>
On en déduit les résultats suivants.</p>
<ul>
<li>Le nombre d'applications de $E_p$ dans $F_n$ est égal à : $n^p$.</li>
<li>Le nombre d'injections de $E_p$ dans $F_n$ ($n \geq p$) est égal à : $A_n^p$.</li>
<li>Le nombre de bijections de $E_n$ dans $F_n$ ($n = p$) est égal à : $A_n^n = n!$.</li>
</ul>

<!-- Image omitted: image0010157 -->
<!-- Page 153 -->
<h2>[S15.2] Espace probabilisable</h2>
<ul>
<li>Soient $\Omega$ un ensemble appelé univers et $\mathcal{A}$ une partie de $\mathcal{P}(\Omega)$, c'est-à-dire un ensemble de parties de $\Omega$.<br>
On dit que $\mathcal{A}$ est une tribu ou une $\sigma$-algèbre de parties de $\Omega$ si :
<ul>
<li>$\Omega \in \mathcal{A}$;</li>
<li>si $A \in \mathcal{A}$ alors $\bar{A} \in \mathcal{A}$ ($\bar{A}$ désigne le complémentaire de $A$ dans $\Omega$);</li>
<li>pour toute partie $I$ de $\mathbb{N}$, et toute famille $(A_i)_{i \in I}$ d'éléments de $\mathcal{A}$, $\bigcup_{i \in I} A_i \in \mathcal{A}$.</li>
</ul>
Les éléments de $\mathcal{A}$ sont appelés des événements, et le couple $(\Omega, \mathcal{A})$ s'appelle un espace probabilisable.</li>
</ul>
<h3>Propriétés</h3>
<p>Soient $\Omega$ un ensemble et $\mathcal{A}$ une tribu de partie de $\Omega$.</p>
<ul>
<li>$\emptyset \in \mathcal{A}$.</li>
<li>Si $A$ et $B$ appartiennent à $\mathcal{A}$, alors $A \cup B$, $A \cap B$ et $A \setminus B$ sont dans $\mathcal{A}$.</li>
<li>Si $I$ est une partie de $\mathbb{N}$ et si pour tout $i \in I$, $A_i \in \mathcal{A}$ alors $\bigcap_{i \in I} A_i \in \mathcal{A}$.</li>
</ul>
<h3>Un peu de vocabulaire</h3>
<table>
<thead>
<tr>
<th>Terminologie probabiliste</th>
<th>Terminologie ensembliste</th>
</tr>
</thead>
<tbody>
<tr>
<td>événement certain</td>
<td>$\Omega$</td>
</tr>
<tr>
<td>événement impossible</td>
<td>$\emptyset$</td>
</tr>
<tr>
<td>événement élémentaire $\omega$</td>
<td>singleton $\{\omega\}$</td>
</tr>
<tr>
<td>événement contraire de $A$ : $\bar{A}$</td>
<td>complémentaire de $A$ dans $\Omega$</td>
</tr>
<tr>
<td>événement $A$Ou $B$</td>
<td>$A \cup B$</td>
</tr>
<tr>
<td>événement $A$ et $B$</td>
<td>$A \cap B$</td>
</tr>
<tr>
<td>les événements $A$ et $B$ sont incompatibles</td>
<td>$A \cap B = \emptyset$</td>
</tr>
<tr>
<td>$A$ implique $B$</td>
<td>$A \subset B$</td>
</tr>
<tr>
<td>système complet d'événements</td>
<td>partition</td>
</tr>
</tbody>
</table>
<h2>[S15.3] Espace probabilisé</h2>
<ul>
<li>Soit $(\Omega, \mathcal{A})$ un espace probabilisable. On appelle probabilité sur $(\Omega, \mathcal{A})$ toute application $P$ de $\mathcal{A}$ dans $[0;1]$ vérifiant :
<ol type="i">
<li>$P(\Omega) = 1$;</li>
<li>si $(A_n)_{n \in \mathbb{N}}$ est une suite d'événements deux à deux incompatibles alors :
$$P\left(\bigcup_{n=0}^{+\infty} A_n \right) = \sum_{n=0}^{+\infty} P(A_n) \quad \text{(}\sigma\text{-additivité)}
$$
(cette écriture sous-entend que la série converge).</li>
</ol>
</li>
</ul>

<!-- Image omitted: image0010158 -->
<!-- Page 154 -->
<p>Le triplet $(\Omega, \mathcal{A}, P)$ est alors appelé espace probabilisé.</p>
<h3>Germe de probabilité</h3>
<p>Soit $\Omega$ un ensemble au plus dénombrable, et $\{p_{\omega} \mid \omega \in \Omega\}$ une famille sommable de réels positifs telle que $\sum_{\omega \in \Omega} p_{\omega} = 1$.<br>
Alors il existe une unique probabilité $P$ sur $(\Omega, \mathcal{P}(\Omega))$ telle que :
$$\forall \omega \in \Omega, P(\{\omega\}) = p_{\omega}.
$$
Cette probabilité est alors définie par :
$$\forall A \in \mathcal{P}(\Omega), P(A) = \sum_{\omega \in A} p_{\omega}.
$$
$\checkmark$ Ce théorème permet donc de généraliser au cas où $\Omega$ est dénombrable ce que vous avez vu en $1^{re}$ année dans le cas fini : pour définir une probabilité, il suffit de la définir pour les événements élémentaires, sous réserve bien sûr que toutes ces probabilités soient des réels positifs et que leur somme soit une famille sommable de somme 1.</p>
<h3>Propriétés</h3>
<p>Soit $(\Omega, \mathcal{A}, P)$ un espace probabilisé et soient $A$ et $B$ deux événements.</p>
<ul>
<li>$P(\emptyset) = 0$.</li>
<li>$P(\overline{A}) = 1 - P(A)$.</li>
<li>Si $A$ et $B$ sont incompatibles, $P(A \cup B) = P(A) + P(B)$.</li>
<li>Plus généralement : $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.</li>
<li>$P(A \setminus B) = P(A) - P(A \cap B)$.</li>
<li>Si $A \subset B$ alors $P(A) \leq P(B)$ (croissance).</li>
<li>Si $(A_i)_{i \in I}$ ($I$ partie de $\mathbb{N}$) est un système complet d'événements, $\sum_{i \in I} P(A_i) = 1$.</li>
</ul>
<p>Soit $(\Omega, \mathcal{A}, P)$ un espace probabilisé.</p>
<ul>
<li>Si $(A_n)_{n \in \mathbb{N}}$ est une suite croissante d'événements de $\mathcal{A}$ ($A_n \subset A_{n+1}$) alors la suite $(P(A_n))_{n \in \mathbb{N}}$ converge et :
$$P\left(\bigcup_{n=0}^{+\infty} A_n\right) = \lim_{n \to +\infty} P(A_n) \quad \text{(continuité croissante)}.
$$</li>
<li>Si $(A_n)_{n \in \mathbb{N}}$ est une suite décroissante d'événements de $\mathcal{A}$ ($A_{n+1} \subset A_n$) alors la suite $(P(A_n))_{n \in \mathbb{N}}$ converge et :
$$P\left(\bigcap_{n=0}^{+\infty} A_n\right) = \lim_{n \to +\infty} P(A_n) \quad \text{(continuité décroissante)}.
$$</li>
<li>Si $(A_n)_{n \in \mathbb{N}}$ est une suite d'événements de $\mathcal{A}$ alors :
$$P\left(\bigcup_{k=0}^{+\infty} A_k\right) = \lim_{n \to +\infty} P\left(\bigcup_{k=0}^{n} A_k\right), \quad P\left(\bigcap_{k=0}^{+\infty} A_k\right) = \lim_{n \to +\infty} P\left(\bigcap_{k=0}^{n} A_k\right).
$$</li>
</ul>

<!-- Image omitted: image0010159 -->
<!-- Page 155 -->
<ul>
<li>Si $(A_n)_{n \in \mathbb{N}}$ est une suite d'événements de $\mathcal{A}$ (Note: Correction A à $\mathcal{A}$) alors :
<ul>
<li>Pour tout $n \in \mathbb{N}$, $P\left(\bigcup_{k=0}^{n} A_k\right) \leqslant \sum_{k=0}^{n} P(A_k)$ (sous-additivité finie).</li>
<li>$P\left(\bigcup_{k=0}^{+\infty} A_k\right) \leqslant \sum_{n=0}^{+\infty} P(A_n)$ (sous-additivité), cette dernière somme pouvant être $+\infty$.</li>
</ul>
</li>
<li>Un événement $A \in \mathcal{A}$ est dit quasi-impossible si $P(A) = 0$ et quasi-certain si $P(A) = 1$.<br>
Un système quasi-complet d'événements est une famille $(A_i)_{i \in I}$ ($I \subset \mathbb{N}$) d'éléments de $\mathcal{A}$, non vides, deux à deux incompatibles, telle que $P\left(\bigcup_{i \in I} A_i\right) = 1$.</li>
</ul>
<h2>[S15.4] Probabilité conditionnelle</h2>
<ul>
<li>Soit $(\Omega, \mathcal{A}, P)$ un espace probabilisé et $B$ un événement de probabilité non nulle.<br>
Pour tout événement $A$, on pose $P_B(A) = \frac{P(A \cap B)}{P(B)}$.<br>
Alors $P_B$ est une probabilité sur $(\Omega, \mathcal{A})$ appelée probabilité conditionnelle relative à $B$Ou encore probabilité sachant $B$. $P_B(A)$ est aussi noté $P(A \mid B)$.<br>
Si $P(A) \neq 0$, on peut aussi écrire $P(B \mid A) = \frac{P(A \cap B)}{P(A)}$ et donc :
$$
P(A) \times P(B \mid A) = P(B) \times P(A \mid B).
$$
Comme $P_B$ est une probabilité, toutes les propriétés vues en [S15.3] peuvent lui être appliquées (par exemple : $P(\overline{A} \mid B) = 1 - P(A \mid B)$).</li>
</ul>
<h3>Formule des probabilités composées</h3>
<p>Directement d'après la définition, on a pour $A, B \in \mathcal{A}$ :
$$
P(A \cap B) = P(B)P(A \mid B).
$$
Plus généralement, si $(A_i)_{1 \leqslant i \leqslant n}$ est une famille d'événements tels que $P(A_1 \cap A_2 \cap \ldots \cap A_{n-1}) \neq 0$, on a :
$$
P\left(\bigcap_{i=1}^{n} A_i\right) = P(A_1)P_{A_1}(A_2)P_{A_1 \cap A_2}(A_3) \ldots P_{A_1 \cap A_2 \cap \ldots \cap A_{n-1}}(A_n),
$$
ce qui s'écrit aussi :
$$
P\left(\bigcap_{i=1}^{n} A_i\right) = P(A_1)P(A_2 \mid A_1)P(A_3 \mid A_1 \cap A_2) \ldots P(A_n \mid A_1 \cap A_2 \cap \ldots \cap A_{n-1}).
$$</p>
<h3>Formule des probabilités totales</h3>
<p>Soit $(A_i)_{i \in I}$ un système quasi-complet d'événements de probabilités non nulles.<br>
Alors pour tout événement $B$On a :
$$
P(B) = \sum_{i \in I} P(A_i \cap B) = \sum_{i \in I} P(B \mid A_i)P(A_i).
$$</p>

<!-- Image omitted: image0010160 -->
<!-- Page 156 -->
<p>(lorsque $I$ est infini dénombrable, cela sous-entend que la famille écrite est sommable).<br>
On peut interpréter cette formule en disant que $B$ est un effet provenant de différentes causes $A_i$.<br>
Un cas particulier important de la formule des probabilités totales est celui où l'on prend comme système complet d'événements un système de la forme $\{A, \bar{A}\}$. La formule s'écrit alors :
$$P(B) = P(B \mid A) P(A) + P(B \mid \bar{A}) P(\bar{A}).$$</p>
<h3>Formule de Bayes</h3>
<p>On a vu que, si $A$ et $B$ sont deux événements de probabilités non nulles, on a :
$$P(A \mid B) = \frac{P(A) \times P(B \mid A)}{P(B)} \quad (*)$$
Si maintenant on considère un système complet ou quasi-complet d'événements $(A_i)_{i \in I}$ (avec toujours $I$ partie de $\mathbb{N}$), et si $A$ est l'un de ces événements, soit $A = A_j$ pour un $j \in I$, la formule des probabilités totales appliquée à l'événement $B$ donne la formule de Bayes.<br>
Soient $(A_i)_{i \in I}$ un système quasi-complet d'événements de probabilités non nulles et $B$ un événement de probabilité non nulle. On a :
$$P(A_j \mid B) = \frac{P(A_j) P(B \mid A_j)}{\sum_{i \in I} P(A_i) P(B \mid A_i)}.$$
Cette formule s'appelle aussi formule de probabilités des causes.</p>
<ul>
<li>Cette formule est affreuse! Il vaut mieux retenir la formule $(*)$, beaucoup plus simple et qui est une conséquence directe de la définition d'une probabilité conditionnelle, puis obtenir la valeur de $P(B)$ par la formule des probabilités totales.</li>
</ul>
<h2>[S15.5] Indépendance d'événements</h2>
<ul>
<li>Soit $(\Omega, \mathcal{A}, P)$ un espace probabilisé. On dit que deux événements $A$ et $B$ sont indépendants pour la probabilité $P$ si :
$$P(A \cap B) = P(A) \times P(B).$$
On a donc alors $P(B \mid A) = P(B)$ et $P(A \mid B) = P(A)$ (si $P(A), P(B) \neq 0$).<br>
Si $A$ et $B$ sont deux événements indépendants alors $\bar{A}$ et $B$, $A$ et $\bar{B}$ et $\bar{A}$ et $\bar{B}$ le sont aussi.</li>
<li>Ne pas confondre « événements indépendants » et « événements incompatibles »! D'ailleurs, il est facile de vérifier que deux événements incompatibles sont indépendants si et seulement si l'un d'entre eux est quasi-impossible.<br>
De plus, la notion d'indépendance dépend de la probabilité $P$ alors que la notion d'incompatibilité est purement ensembliste.</li>
</ul>

<!-- Image omitted: image0010161 -->
<!-- Page 157 -->
<ul>
<li>Soient $(A_i)_{i \in I}$ une famille d'évènements (avec $I \subset \mathbb{N}$, fini ou non).
<ul>
<li>On dit que les évènements sont deux à deux indépendants pour la probabilité $P$ si pour tout $i \neq j$ $A_i$ et $A_j$ sont indépendants.</li>
<li>On dit que les évènements sont mutuellement indépendants (ou tout simplement indépendants) pour la probabilité $P$ si pour tout ensemble fini d'indices $J \subset I$, $P\left(\bigcap_{j \in J} A_j\right) = \prod_{j \in J} P(A_j)$.</li>
</ul>
$\checkmark$ Il est clair que si des évènements sont mutuellement indépendants, ils le sont deux à deux, mais la réciproque est fausse.</li>
<li>Soient $(A_i)_{i \in I}$ une famille d'évènements indépendants (avec $I \subset \mathbb{N}$, fini ou non). Soit $(B_i)_{i \in I}$ une famille d'évènements telle que :
$$\forall i \in I, B_i = A_i \quad \text{ou} \quad B_i = \overline{A_i}. $$
Alors les $B_i$ sont indépendants.</li>
</ul>

<!-- Image omitted: image0010162 -->
<!-- Page 158 -->
<h1>Thème 16 - Variables aléatoires discrètes</h1>
<h2>[S16.1] Variables aléatoires discrètes</h2>
<h3>Définitions</h3>
<ul>
<li>Soit $(\Omega, \mathcal{A})$, un espace probabilisable. On appelle variable aléatoire discrète définie sur $(\Omega, \mathcal{A})$ à valeurs dans un ensemble $E$ toute application $X$ de $\Omega$ dans $E$ telle que :
<ul>
<li>son image $X(\Omega)$ est au plus dénombrable (on écrira donc $X(\Omega) = \{x_i, i \in I\}$ avec $I \subset \mathbb{N}$) ;</li>
<li>pour tout $x \in X(\Omega)$, l'ensemble $X^{-1}(\{x\}) = \{\omega \in \Omega \mid X(\omega) = x\}$ appartient à $\mathcal{A}$ (c'est-à-dire est un événement).</li>
</ul>
Cet ensemble se note, en abrégé : $(X = x)$.<br>
On a alors, plus généralement, le résultat suivant :<br>
pour toute partie $A \subset E$, l'ensemble $X^{-1}(A) = \{\omega \in \Omega \mid X(\omega) \in A\}$ est un événement de $\mathcal{A}$, que l'on note en abrégé : $(X \in A)$.</li>
<li>Si $X(\Omega)$ est un ensemble fini, on dit que $X$ est une variable aléatoire finie.</li>
<li>Si $X(\Omega)$ est une partie de $\mathbb{R}$, on dit que $X$ est une variable aléatoire réelle.</li>
<li>Si $X$ est constante, on dit que c'est une variable aléatoire certaine.</li>
</ul>
<h3>Fonction d'une variable aléatoire</h3>
<p>Soient $X$ une variable aléatoire discrète sur un espace probabilisé $(\Omega, \mathcal{A}, P)$ à valeurs dans un ensemble $E$ et $f$ une fonction définie sur $X(\Omega)$ à valeurs dans un ensemble $F$.<br>
Alors $f \circ X$ est une variable aléatoire discrète sur $(\Omega, \mathcal{A}, P)$ (on la notera simplement $f(X)$).</p>
<h3>Loi d'une variable aléatoire</h3>
<p>Soit $(\Omega, \mathcal{A}, P)$ un espace probabilisé, et $X$ une variable aléatoire discrète à valeurs dans un ensemble $E$.<br>
L'application $f : \begin{cases} X(\Omega) & \to \mathbb{R} \\ x & \mapsto P(X = x) \end{cases}$ s'appelle la loi de probabilité de $X$.<br>
$\checkmark$ Pour répondre à la question « déterminer la loi de $X$ », il faut commencer par donner clairement $X(\Omega)$. Ensuite, pour tout élément $x_i$ de cet ensemble $X(\Omega)$, il faut donner $P(X = x_i)$.<br>
Le résultat suivant est extrêmement important et permet en particulier de vérifier la cohérence des résultats obtenus.<br>
Soit $X$ une variable aléatoire discrète. Si $X(\Omega) = \{x_i, i \in I\}$ alors la famille d'événements $(X = x_i)_{i \in I}$ est un système complet d'événements. En particulier on a $\sum_{i \in I} P(X = x_i) = 1$.<br>
De plus, puisque $(X = x_i)_{i \in I}$ est un système complet d'événements, on peut appliquer la formule des probabilités totales pour n'importer quel événement $A$ :
$$P(A) = \sum_{i \in I} P(A \cap (X = x_i)) = \sum_{i \in I} P(A \mid X = x_i) P(X = x_i).$$</p>

<!-- Image omitted: image0010163 -->
<!-- Page 159 -->
<h3>Germe d'une variable aléatoire</h3>
<p>Soit $(\Omega, \mathcal{A})$ un espace probabilisable, $\{x_i, i \in I\}$ un ensemble au plus dénombrable, et $(p_i)_{i \in I}$ une famille sommable de réels positifs telle que $\sum_{i \in I} p_i = 1$.<br>
Alors il existe une probabilité $P$ sur $(\Omega, \mathcal{A})$ et une variable aléatoire discrète $X$ sur $(\Omega, \mathcal{A}, P)$, à valeurs dans $\{x_i, i \in I\}$, telles que :
$$\forall i \in I, \quad P(X = x_i) = p_i.
$$
Ce résultat permet ainsi d'étudier une variable aléatoire définie par sa loi, sans avoir à préciser l'espace probabilisé sur lequel on travaille.</p>
<h3>Fonction de répartition d'une variable aléatoire réelle</h3>
<p>Soit $X$ une variable aléatoire discrète réelle. On appelle fonction de répartition de $X$ l'application $F : \mathbb{R} \to \mathbb{R}$ définie par :
$$F(x) = P(X \leq x).
$$</p>
<h4>Propriétés</h4>
<ul>
<li>$F$ est une fonction en escalier ;</li>
<li>$\forall x \in \mathbb{R}, F(x) \in [0; 1]$;</li>
<li>$F$ est croissante ;</li>
<li>$\lim_{x \to -\infty} F(x) = 0$ et $\lim_{x \to +\infty} F(x) = 1$;</li>
<li>$F$ est continue à droite en tout point ;</li>
<li>si $a$ et $b$ sont des réels tels que $a < b$, $P(a < X \leq b) = F(b) - F(a)$;</li>
<li>en notant $X(\Omega) = \{x_i, i \in I\}$, les $x_i$ étant rangés par ordre croissant, pour tout $i \in I$ tel que $i - 1 \in I$ (on a donc $x_{i-1} < x_i$), on a :
$$P(X = x_i) = F(x_i) - F(x_{i-1}).
$$</li>
</ul>
<h2>[S16.2] Lois discrètes finies usuelles</h2>
<h3>Loi uniforme</h3>
<p>On dit qu'une variable aléatoire sur un espace probabilisé $(\Omega, \mathcal{A}, P)$ suit la loi uniforme sur $[[1 ; n]]$, et l'on note $X \hookrightarrow \mathcal{U}(〚1 ; n〛)$ si :</p> (Note: [[ ]] remplacé par 〚 〛)
<ol type="i">
<li>$X(\Omega) = 〚1 ; n〛$</li>
<li>$\forall x \in X(\Omega), P(X = x) = \frac{1}{n}$.</li>
</ol>
<p>Situation : tirage au hasard d'un entier parmi $〚1 ; n〛$ (ou une boule dans une urne...), tous les tirages étant équiprobables.</p>
<h3>Loi de Bernoulli</h3>
<p>Soit $p \in [0 ; 1]$. On dit qu'une variable aléatoire sur un espace probabilisé $(\Omega, \mathcal{A}, P)$ suit la loi de Bernoulli de paramètre $p$, et l'on note $X \hookrightarrow \mathcal{B}(p)$ si :</p>
<ol type="i">
<li>$X(\Omega) = \{0, 1\}$</li>
<li>$P(X = 1) = p$ et $P(X = 0) = 1 - p$.</li>
</ol>
<p>Les cas $p = 0$ et $p = 1$ n'étant pas intéressants, on considère en général $p \in ]0 ; 1[$.<br>
Situation : expérience de type succès-échec (pile ou face, tirage d'une boule noire ou blanche...).</p>

<!-- Image omitted: image0010164 -->
<!-- Page 160 -->
<h3>Loi binomiale</h3>
<p>Soit $n$ un entier non nul et $p$ un réel dans $[0 ; 1]$. On dit qu'une variable aléatoire sur un espace probabilisé $(\Omega, \mathcal{A}, P)$ suit la loi binomiale de paramètres $n$ et $p$, et l'on note $X \hookrightarrow \mathcal{B}(n, p)$ si :</p>
<ol type="i">
<li>$X(\Omega) = 〚0 ; n〛$</li>
<li>$\forall k \in 〚0 ; n〛, P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}$.</li>
</ol>
<p>On note traditionnellement $q = 1 - p$ de sorte que $P(X = k) = \binom{n}{k} p^k q^{n - k}$.<br>
Situation : expérience de type succès-échec que l'on effectue $n$ fois dans les mêmes conditions; $X$ désigne le nombre de fois où l'expérience a amené un succès (lancers de pièces, tirage avec remise de boules dans une urne...).</p>
<h2>[S16.3] Lois discrètes infinies usuelles</h2>
<h3>Loi géométrique</h3>
<p>Soit $p \in ]0 ; 1[$. On dit qu'une variable aléatoire sur un espace probabilisé $(\Omega, \mathcal{A}, P)$ suit la loi géométrique de paramètre $p$, et l'on note $X \hookrightarrow \mathcal{G}(p)$ si :</p>
<ol type="i">
<li>$X(\Omega) = \mathbb{N}^*$</li>
<li>$\forall n \in \mathbb{N}^*, P(X = n) = (1 - p)^{n - 1} p$.</li>
</ol>
<p>Il est facile de vérifier que $\sum_{n=1}^{+\infty} P(X = n)$ est bien égal à 1 (série géométrique).<br>
<strong>Situation</strong> : expérience de type succès-échec que l'on répète dans des conditions identiques; $X$ désigne le nombre d'épreuves effectuées jusqu'à ce que l'on obtienne un succès pour la première fois.<br>
Une propriété importante de la loi géométrique est d'être <em>sans mémoire</em>. En effet, les épreuves étant indépendantes, la loi de probabilité du nombre d'épreuves à répéter jusqu'à l'obtention d'un premier succès est la même quel que soit le nombre d'échecs obtenus auparavant.<br>
Mathématiquement, cela se traduit par :
$\forall (n, m) \in \mathbb{N}^{*2}, P(X = n + m \mid X > m) = P(X = n)$,<br>
ou, puisque $P(X > n) = \sum_{k=n+1}^{+\infty} P(X = k)$ par :
$\forall (n, m) \in \mathbb{N}^{*2}, P(X > n + m \mid X > m) = P(X > n)$.<br>
La loi géométrique est la seule loi de probabilité discrète qui possède cette propriété.<br>
Pour une loi géométrique, l'événement $(X > n)$ lorsque $n \in \mathbb{N}^*$ représente le fait de n'avoir que des échecs lors des $n$ premières épreuves; on a donc $P(X > n) = (1 - p)^n$ (et cela reste vrai pour $n = 0$).<br>
Il est facile de vérifier que l'on a bien : $(1 - p)^n = \sum_{k=n+1}^{+\infty} P(X = k)$.</p>

<!-- Image omitted: image0010165 -->
<!-- Page 161 -->
<h3>Loi de Poisson</h3>
<p>Soit $\lambda$ un réel strictement positif. On dit qu'une variable aléatoire sur un espace probabilisé $(\Omega, \mathcal{A}, P)$ suit la loi de Poisson de paramètre $\lambda$, et l'on note $X \hookrightarrow \mathcal{P}(\lambda)$ si :</p>
<ol type="i">
<li>$X(\Omega) = \mathbb{N}$</li>
<li>$\forall n \in \mathbb{N}, P(X = n) = \frac{e^{-\lambda} \lambda^n}{n!}$.</li>
</ol>
<p><strong>Situation</strong> : la loi de Poisson sert à estimer le nombre de réalisations d'un événement dans un intervalle de temps donné (arrivée de clients au guichet d'une banque en une heure, arrivée de malades aux urgences d'un hôpital en une nuit...). Le paramètre $\lambda$ correspond au nombre moyen d'événements survenus. Ce $n$ est bien sûr qu'une approximation, qui est justifiée par le résultat suivant.</p>
<h3>Approximation d'une loi binomiale par une loi de Poisson</h3>
<p>Si $(X_n)$ est une suite de variables aléatoires telle que $X_n$ suit la loi binomiale $\mathcal{B}(n, p_n)$ avec $\lim_{n \to +\infty} n p_n = \lambda > 0$, alors la suite $(X_n)$ converge simplement vers une variable aléatoire $X$ qui suit la loi de Poisson de paramètre $\lambda$ :
$\forall k \in \mathbb{N}, \lim_{n \to +\infty} P(X_n = k) = \lim_{n \to +\infty} \binom{n}{k} p_n^k (1 - p_n)^{n-k} = \frac{e^{-\lambda} \lambda^k}{k!} = P(X = k)$.<br>
On considère en général que l'on peut approcher la loi binomiale de paramètre $(n, p)$ par la loi de Poisson de paramètre $\lambda = np$ lorsque $n \geq 30$ et $p \leq 0.1$ avec $np \leq 10$. L'avantage de cette approximation est que la loi de Poisson est bien plus rapide à calculer que la loi binomiale.</p>
<h2>[S16.4] Couples de variables aléatoires discrètes</h2>
<h3>Loi d'un couple de variables aléatoires</h3>
<p>Soit $(\Omega, \mathcal{A}, P)$, un espace probabilisé et $E$ un ensemble. Soient $X$ et $Y$ deux variables aléatoires discrètes sur $(\Omega, \mathcal{A})$, à valeurs dans $E$.<br>
On appelle loi conjointe du couple de variables aléatoires discrètes $(X, Y)$ la donnée de l'ensemble des couples $((x_i, y_j), p_{i,j})$Où
$X(\Omega) = \{x_i, i \in I\}, Y(\Omega) = \{y_j, j \in J\}$ ($I, J$ parties de $\mathbb{N}$)
et $p_{i,j} = P((X, Y) = (x_i, y_j)) = P((X = x_i) \cap (Y = y_j))$. (Note: Correction de la notation d'intersection)</p>
<p>Les lois de $X$ et $Y$ s'appellent alors les lois marginales du couple $(X, Y)$.<br>
Pour tout indice $j \in J$ tel que $P(Y = y_j)$ soit non nul, on appelle loi conditionnelle de $X$ sachant $(Y = y_j)$ l'application de $X(\Omega)$ dans $[0; 1]$ définie par la relation :
$x_i \mapsto P_{(Y = y_j)}(X = x_i) = P(X = x_i \mid Y = y_j) = \frac{P((X = x_i) \cap (Y = y_j))}{P(Y = y_j)}$.<br>
Pour tout indice $i \in I$ tel que $P(X = x_i)$ soit non nul, on appelle loi conditionnelle de $Y$ sachant $(X = x_i)$ l'application de $Y(\Omega)$ dans $[0; 1]$ définie par la relation :
$y_j \mapsto P_{(X = x_i)}(Y = y_j) = P(Y = y_j \mid X = x_i) = \frac{P((X = x_i) \cap (Y = y_j))}{P(X = x_i)}$.</p>

<!-- Image omitted: image0010166 -->
<!-- Page 162 -->
<h3>Lien entre lois conjointes et lois marginales</h3>
<p>Il est important de bien savoir jongler entre loi du couple, loi marginale et loi conditionnelle. Il s'agit en fait uniquement d'appliquer la définition des probabilités conditionnelles ou alors d'appliquer la formule des probabilités totales. On a donc les relations suivantes.</p>
<ul>
<li>Lois marginales à partir de la loi du couple :
$$P(X = x_i) = \sum_{j \in J} P((X = x_i) \cap (Y = y_j))
$$
$$P(Y = y_j) = \sum_{i \in I} P((X = x_i) \cap (Y = y_j))
$$</li>
<li>Lois marginales à partir des lois conditionnelles :
$$P(X = x_i) = \sum_{j \in J} P(Y = y_j)P(X = x_i | Y = y_j)
$$
$$P(Y = y_j) = \sum_{i \in I} P(X = x_i)P(Y = y_j | X = x_i)
$$</li>
<li>Loi du couple à partir des lois conditionnelles :
$$P((X = x_i) \cap (Y = y_j)) = P(X = x_i)P(Y = y_j | X = x_i)
$$
$$P((X = x_i) \cap (Y = y_j)) = P(Y = y_j)P(X = x_i | Y = y_j)
$$</li>
</ul>
<h3>Indépendance de variables aléatoires</h3>
<ul>
<li>Soit $(X, Y)$ un couple de variables aléatoires discrètes définies sur un espace probabilisé $(\Omega, \mathcal{A}, P)$.<br>
On dit que les variables $X$ et $Y$ sont indépendantes si :
$$\forall i \in I, \forall j \in J, P((X = x_i) \cap (Y = y_j)) = P(X = x_i)P(Y = y_j).
$$
Cela équivaut à dire que la loi conjointe est le produit des lois marginales.<br>
$X$ et $Y$ sont indépendantes si et seulement si :
$$\forall A \subset X(\Omega), \forall B \subset Y(\Omega), P((X \in A) \cap (Y \in B)) = P(X \in A) \times P(Y \in B).
$$</li>
<li>Soient $X_1, \ldots, X_n$ $n$ variables aléatoires discrètes. On dit que les variables $X_1, \ldots, X_n$ sont mutuellement indépendantes (ou tout simplement indépendantes) lorsque pour tout $(x_1, \ldots, x_n) \in X_1(\Omega) \times \cdots \times X_n(\Omega)$ :
$$P((X_1 = x_1) \cap \cdots \cap (X_n = x_n)) = P(X_1 = x_1) \times \cdots \times P(X_n = x_n).
$$</li>
</ul>
<h3>Fonction de deux variables aléatoires indépendantes</h3>
<ul>
<li>Soient $X$ et $Y$ deux variables aléatoires discrètes sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, indépendantes. Soient $f$ et $g$ deux applications définies sur $X(\Omega)$ et $Y(\Omega)$ respectivement.<br>
Alors les variables aléatoires $f(X)$ et $g(Y)$ sont indépendantes.<br>
Le résultat du théorème précédent s'étend à plus de deux variables aléatoires.</li>
</ul>

<!-- Image omitted: image0010167 -->
<!-- Page 163 -->
<ul>
<li>Soient $X_1, \ldots, X_n$ $n$ variables aléatoires discrètes mutuellement indépendantes et soit $p \in 〚 2 ; n-1 〛$. Alors toute variable aléatoire fonction des variables $X_1, \ldots, X_p$ est indépendante de toute variable aléatoire fonction des variables $X_{p+1}, \ldots, X_n$.</li>
</ul>
<h3>Loi d'une fonction de deux variables aléatoires</h3>
<p>Soit $X$ et $Y$ deux variables aléatoires discrètes et $g$ une fonction quelconque définie sur $X(\Omega) \times Y(\Omega)$. Alors $Z = g(X,Y)$ est une variable aléatoire discrète.</p>
<h3>Somme de variables aléatoires</h3>
<ul>
<li>Soient $(\Omega, \mathcal{A}, P)$, un espace probabilisé et $(X, Y)$ un couple de variables aléatoires discrètes sur $\Omega$, à valeurs dans $\mathbb{R}$. Notons $X(\Omega) = \{x_i, i \in I\}$ et $Y(\Omega) = \{y_j, j \in J\}$Où $I, J$ sont deux parties de $\mathbb{N}$.<br>
Soit $Z = X + Y$. On sait déjà que $Z$ est une variable aléatoire réelle discrète. Pour tout $z \in Z(\Omega)$, l'événement $(Z = z)$ est la réunion disjointe et au plus dénombrable des événements $(X = x_i) \cap (Y = y_j)$ pour tous les couples $(i, j) \in I \times J$ tels que $x_i + y_j = z$. On aura donc :
$$P(Z = z) = \sum_{(i,j)|x_i + y_j = z} P((X = x_i) \cap (Y = y_j)).$$
Dans le cas où les variables $X$ et $Y$ sont indépendantes on aura donc aussi :
$$P(Z = z) = \sum_{(i,j)|x_i + y_j = z} P(X = x_i) P(Y = y_j).$$</li>
<li>Si $X_1, \ldots, X_n$ sont $n$ variables aléatoires mutuellement indépendantes suivant la loi de Bernoulli de paramètre $p$, la variable aléatoire $X = X_1 + X_2 + \cdots + X_n$ suit la loi binomiale $B(n, p)$.</li>
<li>Si $X_1, \ldots, X_k$ sont $k$ variables aléatoires mutuellement indépendantes suivant les lois binomiales de paramètres respectifs $(n_1, p), \ldots, (n_k, p)$ alors la variable aléatoire $X_1 + \cdots + X_k$ suit la loi binomiale de paramètre $\left( \sum_{i=1}^{k} n_i, p \right)$.</li>
<li>Si $X$ et $Y$ sont deux variables aléatoires indépendantes suivant les lois de Poisson de paramètres respectifs $\lambda$ et $\mu$ alors $X + Y$ suit la loi de Poisson de paramètre $\lambda + \mu$.</li>
</ul>
<h2>[S16.5] Espérance d'une variable aléatoire discrète</h2>
<p>Les variables aléatoires considérées dans ce paragraphe sont à valeurs réelles.</p>
<h3>Définition</h3>
<p>Soit $X$ une variable aléatoire réelle discrète sur un espace probabilisé $(\Omega, \mathcal{A}, P)$.</p>
<ul>
<li>Si $X(\Omega)$ est fini et si $(x_i, p_i)_{1 \leq i \leq n}$ est la loi de $X$, alors l'espérance de $X$ est :
$$E(X) = \sum_{i=1}^{n} p_i x_i = \sum_{i=1}^{n} x_i P(X = x_i).$$ (Note: Correction de 1 <= i <= n)</li>
</ul>

<!-- Image omitted: image0010168 -->
<!-- Page 164 -->
<ul>
<li>Si $X(\Omega)$ est infini ($X(\Omega) = \{x_i, i \in I\}$ avec $I$ dénombrable), et si $(x_i, p_i)_{i \in I}$ est la loi de $X$, alors on dira que $X$ admet une espérance si la famille $(x_i p_i)_{i \in I}$ est sommable. Dans ce cas l'espérance de $X$ est :
$$E(X) = \sum_{i \in I} x_i p_i = \sum_{i \in I} x_i P(X = x_i).
$$
Remarque : une variable aléatoire discrète finie a toujours une espérance ce qui n'est pas forcément le cas pour une variable aléatoire discrète infinie.</li>
</ul>
<h3>Cas d'une variable aléatoire à valeurs entières</h3>
<p>Soit $X$ une variable aléatoire discrète sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, à valeurs dans $\mathbb{N}$.<br>
$X$ admet une espérance si et seulement si la série de terme général $P(X \geq k)$ converge, et dans ce cas :
$$E(X) = \sum_{k=1}^{+\infty} P(X \geq k) = \sum_{k=0}^{+\infty} P(X > k).
$$</p>
<h3>Positivité de l'espérance</h3>
<p>Soit $X$ une variable aléatoire réelle discrète sur un espace probabilisé $(\Omega, \mathcal{A}, P)$. Si $X \geq 0$ (c'est-à-dire si $X(\omega) \geq 0$ pour tout $\omega \in \Omega$) et si son espérance existe, alors $E(X) \geq 0$.<br>
De plus, si $X \geq 0$ et si $E(X) = 0$, alors $P(X = 0) = 1$ (on dit que $X$ est quasi certainement nulle).</p>
<h3>Théorème de transfert</h3>
<p>Soit $X$ une variable aléatoire discrète sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, à valeurs dans un ensemble $E$. Notons $X(\Omega) = \{x_i, i \in I\}$ avec $I$ dénombrable. Soit $f$ une application de $X(\Omega)$ dans $\mathbb{R}$. La variable aléatoire réelle $f(X)$ admet une espérance si et seulement si la famille $(f(x_i) P(X = x_i))_{i \in I}$ est sommable, et dans ce cas :
$$E(f(X)) = \sum_{i \in I} f(x_i) P(X = x_i).
$$
L'intérêt de ce théorème est de pouvoir calculer l'espérance de $f(X)$ sans avoir besoin de calculer sa loi, seulement en connaissant la loi de $X$.</p>
<h3>Linéarité de l'espérance</h3>
<p>Soient $X$ et $Y$ deux variables aléatoires réelles discrètes sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, et soit $\lambda \in \mathbb{R}$.<br>
Si les espérances de $X$ et de $Y$ existent, alors l'espérance de $\lambda X + Y$ existe et
$$E(\lambda X + Y) = \lambda E(X) + E(Y).
$$
Soit $X$ une variable aléatoire discrète dont l'espérance existe. Puisque $E(1) = 1$ il résulte de l'égalité précédente que la variable aléatoire $X - E(X)$ est d'espérance nulle. On l'appelle variable aléatoire centrée associée à $X$.</p>

<!-- Image omitted: image0010169 -->
<!-- Page 165 -->
<h2>Croissance de l'espérance</h2>
<ul>
<li>Soient $X$ et $Y$ deux variables aléatoires réelles discrètes sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, dont l'espérance existe.<br>
Si $X \leqslant Y$ sur $\Omega$, alors $E(X) \leqslant E(Y)$.</li>
<li>Soient $X$ et $Y$ deux variables aléatoires réelles discrètes sur un espace probabilisé $(\Omega, \mathcal{A}, P)$.<br>
On suppose que l'espérance de $Y$ existe, et que $|X| \leqslant Y$ sur $\Omega$.<br>
Alors l'espérance de $X$ existe.<br>
Cas particulier : si $X$ est une variable aléatoire discrète bornée, son espérance existe.</li>
</ul>
<h2>Produit de deux variables aléatoires indépendantes</h2>
<p>Soient $X$ et $Y$ deux variables aléatoires réelles discrètes sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, dont l'espérance existe.<br>
Si $X$ et $Y$ sont indépendantes, la variable aléatoire $Z = XY$ possède une espérance finie et l'on a :
$$E(XY) = E(X)E(Y).$$</p>
<h2>Inégalité de Markov</h2>
<p>Soit $X$ une variable aléatoire discrète réelle positive, dont l'espérance existe.<br>
Alors, pour tout réel $a > 0$ :
$$P(X \geqslant a) \leqslant \frac{E(X)}{a}.$$</p>
<h2>[S16.6] Moments d'une variable aléatoire réelle discrète</h2>
<p>Soit $(\Omega, \mathcal{A}, P)$ un espace probabilisé, $X$ une variable aléatoire réelle discrète et soit $r \geqslant 1$ un entier.</p>
<ul>
<li>Si $X(\Omega)$ est fini et si $(x_i, p_i)_{1 \leqslant i \leqslant n}$ est la loi de $X$, alors le moment d'ordre $r$ de $X$ est :
$$m_r(X) = E(X^r) = \sum_{i=1}^{n} p_i x_i^r.$$</li>
<li>Si $X(\Omega)$ est infini et si $(x_i, p_i)_{i \in I}$ est la loi de $X$, alors on dira que $X$ admet un moment d'ordre $r$ si la famille $(x_i^r p_i)_{i \in I}$ est sommable. Dans ce cas le moment d'ordre $r$ de $X$ est :
$$m_r(X) = E(X^r) = \sum_{i \in I} x_i^r p_i = \sum_{i \in I} x_i^r P(X = x_i).$$</li>
</ul>
<h3>Théorème</h3>
<p>Si $X$ admet un moment d'ordre $r \geqslant 2$, elle admet des moments d'ordre $s$ pour tout $s \in 〚 1 ; r 〛$.</p>

<!-- Image omitted: image0010170 -->
<!-- Page 166 -->
<h2>[S16.7] Variance</h2>
<ul>
<li>Soit $X$ une variable aléatoire réelle dont le moment d'ordre 2, c'est-à-dire $E(X^2)$, existe.
<ul>
<li>La variable aléatoire $X - E(X)$ admet aussi un moment d'ordre 2, appelé variance de $X$. Ainsi :
$$V(X) = E\left((X - E(X))^2\right).
$$</li>
<li>On a la relation :
$$V(X) = E(X^2) - (E(X))^2 \quad \text{(formule de Koenig-Huygens)}.
$$
On appelle alors écart-type de $X$ le réel $\sigma(X) = \sqrt{V(X)}$.</li>
</ul>
</li>
<li>Soit $X$ une variable aléatoire réelle dont la variance $V(X)$ existe. Si $a$ et $b$ sont deux réels, la variable $aX + b$ admet aussi une variance et :
$$V(aX + b) = a^2V(X).
$$</li>
<li><strong>Inégalité de Bienaymé-Tchebychev</strong><br>
Soit $X$ une variable aléatoire réelle discrète admettant un moment d'ordre 2. Alors, pour tout réel $\epsilon > 0$ :
$$P(|X - E(X)| \geq \epsilon) \leq \frac{V(X)}{\epsilon^2}.
$$
Cette inégalité montre que la variance permet de mesurer la dispersion de la variable $X$ autour de sa moyenne.<br>
<strong>Conséquence</strong><br>
Soit $X$ une variable aléatoire réelle discrète admettant un moment d'ordre 2. Notons $m = E(X)$.<br>
Si $V(X) = 0$ alors $P(X = m) = 1$ ($X$ est quasi certainement nulle).</li>
</ul>
<h2>[S16.8] Moments des lois usuelles</h2>
<ul>
<li><strong>Loi uniforme</strong><br>
Soit $X \hookrightarrow \mathcal{U}(〚1 ; n〛)$. Alors : $E(X) = \frac{n + 1}{2}$ et $V(X) = \frac{n^2 - 1}{12}$.</li>
<li><strong>Loi binomiale</strong><br>
Soit $X \hookrightarrow \mathcal{B}(n, p)$. Alors : $E(X) = np$ et $V(X) = npq$.</li>
<li><strong>Loi géométrique</strong><br>
Soit $X \hookrightarrow \mathcal{G}(p)$. Alors : $E(X) = \frac{1}{p}$ et $V(X) = \frac{q}{p^2}$.</li>
<li><strong>Loi de Poisson</strong><br>
Soit $X \hookrightarrow \mathcal{P}(\lambda)$. Alors : $E(X) = \lambda$ et $V(X) = \lambda$.</li>
</ul>

<!-- Image omitted: image0010171 -->
<!-- Page 167 -->
<h2>[S16.9] Covariance</h2>
<ul>
<li>Soient $X$ et $Y$ deux variables aléatoires réelles discrètes sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, ayant chacune un moment d'ordre 2. Alors l'espérance de $XY$ existe et
$$(E(XY))^2 \leq E(X^2)E(Y^2)$$
(inégalité de Cauchy-Schwarz).</li>
<li>Si $X$ et $Y$ sont deux variables aléatoires réelles admettant un moment d'ordre 2, alors la variable aléatoire $X + Y$ admet aussi un moment d'ordre 2.</li>
<li>Si $X$ et $Y$ sont deux variables aléatoires réelles admettant chacune un moment d'ordre 2, on appelle covariance de $X$ et $Y$ le réel :
$$\text{cov}(X, Y) = E[(X - E(X))(Y - E(Y))] $$
(l'existence est assurée par les propriétés précédentes). Un calcul simple montre que l'on a aussi :
$$\text{cov}(X, Y) = E(XY) - E(X)E(Y). $$
On a alors la relation :
$$ V(X + Y) = V(X) + V(Y) + 2\text{cov}(X, Y). $$</li>
</ul>
<h3>Propriétés</h3>
<p>Soit $(\Omega, \mathcal{A}, P)$ un espace probabilisé.</p>
<ul>
<li>L'ensemble $\mathcal{V}(\Omega, \mathbb{R})$ des variables aléatoires réelles discrètes sur $\Omega$ est un sous-espace vectoriel de $\mathcal{A}(\Omega, \mathbb{R})$.</li>
<li>L'ensemble $\mathcal{V}^1(\Omega, \mathbb{R})$ des variables aléatoires réelles discrètes sur $\Omega$ admettant une espérance est un sous-espace vectoriel de $\mathcal{V}(\Omega, \mathbb{R})$.</li>
<li>L'ensemble $\mathcal{V}^2(\Omega, \mathbb{R})$ des variables aléatoires réelles discrètes sur $\Omega$ admettant un moment d'ordre 2 est un sous-espace vectoriel de $\mathcal{V}^1(\Omega, \mathbb{R})$.</li>
<li>L'application $(X, Y) \mapsto \text{cov}(X, Y)$ est une forme bilinéaire symétrique positive (mais non définie) sur l'espace vectoriel $\mathcal{V}^2(\Omega, \mathbb{R})$, et pour tout $X \in \mathcal{V}^2(\Omega, \mathbb{R})$,
$$\text{cov}(X, X) = V(X). $$</li>
<li>Si $X$ et $Y$ appartiennent à $\mathcal{V}^2(\Omega, \mathbb{R})$, on a l'inégalité de Cauchy-Schwarz :
$$ |\text{cov}(X, Y)| \leq \sigma(X)\sigma(Y). $$</li>
<li>Si $X$ et $Y$ sont deux variables aléatoires réelles admettant chacune un moment d'ordre 2 et indépendantes, alors leur covariance est nulle. Il en résulte que l'on a alors $V(X + Y) = V(X) + V(Y)$.<br>
$$\checkmark$$ La réciproque de cette propriété est fausse : deux variables aléatoires peuvent avoir une covariance nulle sans être indépendantes.</li>
</ul>

<!-- Image omitted: image0010172 -->
<!-- Page 168 -->
<h3>Généralisation</h3>
<p>Si $X_1, \ldots, X_n$ sont $n$ variables aléatoires réelles possédant toutes un moment d'ordre 2, en utilisant la bilinéarité de la covariance on obtient :
$$ V(X_1 + \cdots + X_n) = \sum_{i=1}^n V(X_i) + 2 \sum_{1 \leq i < j \leq n} \text{cov}(X_i, X_j). $$</p>
<p>Il en résulte que si $X_1, \ldots, X_n$ sont $n$ variables aléatoires réelles possédant toutes un moment d'ordre 2 et indépendantes deux à deux :
$$V(X_1 + \cdots + X_n) = \sum_{i=1}^{n} V(X_i).
$$</p>
<ul>
<li>Soient $X$ et $Y$ deux variables aléatoires réelles dont la variance existe et est non nulle. On appelle coefficient de corrélation linéaire de $X$ et $Y$ le nombre réel :
$$\rho(X,Y) = \frac{\text{cov}(X,Y)}{\sigma(X)\sigma(Y)}.
$$</li>
</ul>
<h4>Propriétés</h4>
<p>Soient $X$ et $Y$ deux variables aléatoires réelles dont la variance existe et est non nulle.</p>
<ul>
<li>$|\rho(X,Y)| \leq 1$;</li>
<li>$|\rho(X,Y)| = 1$ si et seulement si $Y$ est une fonction quasi affine de $X$, c'est-à-dire s'il existe $a$ et $b$ réels tels que l'événement $(Y = aX + b)$ soit quasi certain.</li>
</ul>
<h2>[S16.10] Variables aléatoires à valeurs dans $\mathbb{N}$</h2>
<ul>
<li>Soit $X$ une variable aléatoire définie sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, à valeurs dans $\mathbb{N}$.<br>
La fonction génératrice de $X$ est la série entière :
$$G_X(t) = \sum_{n=0}^{+\infty} P(X = n)t^n.
$$</li>
<li>La fonction génératrice d'une variable aléatoire à valeurs entières a un rayon de convergence $R_X$ supérieur ou égal à 1.</li>
<li>$G_X$ est de classe $\mathcal{C}^\infty$ sur $]-R_X; R_X[$ et :
$$\forall n \in \mathbb{N}, P(X = n) = \frac{G_X^{(n)}(0)}{n!}.
$$</li>
<li>Pour tout $t \in ]-R_X; R_X[, G_X(t) = E(t^X)$</li>
</ul>

<!-- Image omitted: image0010173 -->
<!-- Page 169 -->
<h3>Fonctions génératrices des lois usuelles</h3>
<ul>
<li>Si $X$ suit la loi binomiale de paramètre $(n, p)$, sa fonction génératrice est la fonction polynomiale définie par :
$$\forall t \in \mathbb{R}, G_X(t) = (pt + q)^n.
$$</li>
<li>Si $X$ suit la loi géométrique de paramètre $p$, le rayon de convergence de sa série génératrice est $\frac{1}{q}$ et :
$$\forall t \in \left]-\frac{1}{q}; \frac{1}{q}\right[, G_X(t) = \frac{pt}{1 - qt}.
$$</li>
<li>Si $X$ suit la loi de Poisson de paramètre $\lambda$, sa série génératrice est de rayon de convergence $+\infty$ et :
$$\forall t \in \mathbb{R}, G_X(t) = e^{\lambda(t-1)}.
$$</li>
</ul>
<h3>Somme de variables aléatoires indépendantes</h3>
<ul>
<li>Soient $X$ et $Y$ deux variables aléatoires à valeurs dans $\mathbb{N}$, et $G_X$, $G_Y$ leurs fonctions génératrices respectives.<br>
Si $X$ et $Y$ sont indépendantes on a :
$$\forall t \in [-1; 1], G_{X+Y}(t) = G_X(t)G_Y(t).
$$</li>
<li>Plus généralement, si $X_1, \ldots, X_n$ sont $n$ variables aléatoires mutuellement indépendantes, on a :
$$\forall t \in [-1; 1], G_{X_1 + \cdots + X_n}(t) = \prod_{i=1}^n G_{X_i}(t).
$$</li>
</ul>
<h3>Fonction génératrice et moments</h3>
<ul>
<li>Soit $X$ une variable aléatoire définie sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, à valeurs dans $\mathbb{N}$, et $G_X$ sa fonction génératrice. On suppose que $R_X > 1$, ce qui implique que $G_X$ est de classe $C^\infty$ au voisinage de 1. Alors :
$$\forall k \in \mathbb{N}^*, G_X^{(k)}(1) = E(X(X-1) \ldots (X-k+1)).
$$
En particulier :
$$E(X) = G_X'(1), E(X(X-1)) = G_X''(1) \text{ d'où } E(X^2) = G_X'(1) + G_X''(1).
$$</li>
<li>Réciproquement, soit $X$ une variable aléatoire définie sur un espace probabilisé $(\Omega, \mathcal{A}, P)$, à valeurs dans $\mathbb{N}$, et $G_X$ sa fonction génératrice.<br>
L'espérance de $X$ existe si et seulement si $G_X$ est dérivable à gauche en 1 et la variance de $X$ existe si et seulement si $G_X$ est deux fois dérivable à gauche en 1.<br>
Dans ce cas on aura : $E(X) = G_X'(1^-)$ et $E(X^2) = G_X'(1^-) + G_X''(1^-)$.</li>
</ul>
<h2>[S16.11] Loi faible des grands nombres</h2>
<p>Soit $(X_i)_{i \in \mathbb{N}}$ une suite de variables aléatoires réelles définies sur un espace probabilisé $(\Omega, \mathcal{A}, P)$ indépendantes, de même loi, possédant une espérance $m = E(X_1)$ et une variance $\sigma^2 = V(X_1)$. On pose : (Note: Correction de E(X) et V(X) à E(X_1) et V(X_1))
$$\overline{X_n} = \frac{\sum_{i=1}^n X_i}{n}.
$$
Alors on a :
$$\forall \epsilon > 0, P\left( \left| \overline{X_n} - m \right| \geq \epsilon \right) \leq \frac{\sigma^2}{n \epsilon^2}.
$$
Par conséquent :
$$\forall \epsilon > 0, \lim_{n \to +\infty} P\left( \left| \overline{X_n} - m \right| \geq \epsilon \right) = 0.
$$</p>

<!-- Image omitted: image0010174 -->
<!-- Page 170 -->
<h1>Thème 17 - Équations différentielles linéaires</h1>
<p>Dans tout ce chapitre, $\mathbb{K}$ désigne $\mathbb{R}$Ou $\mathbb{C}$, $I$ est un intervalle de $\mathbb{R}$ et $E$ est un $\mathbb{K}$-espace vectoriel normé de dimension finie $n \in \mathbb{N}^*$.</p>
<h2>[S17.1] Équations différentielles linéaires, systèmes différentiels</h2>
<ul>
<li>Une équation différentielle (vectorielle) linéaire du premier ordre est une équation de la forme :
$$
x' = a(t)x + b(t) \quad (L)
$$ (Note: Corrigé de a(x) à a(t)x et b à b(t))
où $a$ est une application continue de $I$ dans $\mathcal{L}(E)$ et $b$ une application continue de $I$ dans $E$.<br>
Une solution de cette équation est une application $x$ dérivable de $I$ dans $E$ telle que :
$$
\forall t \in I, x'(t) = a(t)(x(t)) + b(t).
$$
$\checkmark$ Dans cette écriture, $a(t)$ est un endomorphisme de $E$ et $a(t)(x(t))$ représente donc l'image du vecteur $x(t)$ par cet endomorphisme.</li>
<li>Soit $\mathcal{B} = (e_1, \ldots, e_n)$ une base de $E$, et :
<ul>
<li>$X(t) = ^t(x_1(t) \ldots x_n(t))$ la matrice colonne formée des coordonnées du vecteur $x(t)$ dans $\mathcal{B}$, où les $x_i$ sont des applications dérivables de $I$ dans $\mathbb{K}$; (Note: correction de la transposée)</li>
<li>$B(t) = ^t(b_1(t) \ldots b_n(t))$ la matrice colonne formée des coordonnées du vecteur $b(t)$ dans $\mathcal{B}$, où les $b_i$ sont des applications continues de $I$ dans $\mathbb{K}$;</li>
<li>$A(t) = (a_{ij}(t))_{1 \leq i,j \leq n}$ la matrice dans $\mathcal{B}$ de l'application linéaire $a(t)$, où les $a_{ij}$ sont des applications continues de $I$ dans $\mathbb{K}$.</li>
</ul>
L'équation $x' = a(t)x + b(t)$ peut alors s'écrire matriciellement sous la forme :
$$
X' = AX + B \quad \text{ou} : \quad \forall t \in I, X'(t) = A(t)X(t) + B(t) \quad (L)
$$
On obtient alors ce que l'on appelle un système différentiel :
$$
\begin{cases}
x_1'(t) &= a_{11}(t)x_1(t) + \cdots + a_{1n}(t)x_n(t) + b_1(t) \\
x_2'(t) &= a_{21}(t)x_1(t) + \cdots + a_{2n}(t)x_n(t) + b_2(t) \\
&\vdots \\
x_n'(t) &= a_{n1}(t)x_1(t) + \cdots + a_{nn}(t)x_n(t) + b_n(t)
\end{cases}
$$</li>
</ul>
<h2>[S17.2] Théorème de Cauchy-Lipschitz linéaire</h2>
<ul>
<li>Résoudre le problème de Cauchy pour une équation différentielle $x' = a(t)x + b(t)$ sur $I$ consiste à étudier l'existence et l'unicité d'une solution vérifiant une condition initiale du type $x(t_0) = x_0$ avec $t_0 \in I$ et $x_0 \in E$ donnés.</li>
</ul>

<!-- Image omitted: image0010175 -->
<!-- Page 171 -->
<ul>
<li><strong>Théorème de Cauchy-Lipschitz linéaire</strong><br>
Soit $E$ un espace vectoriel de dimension finie, $I$ un intervalle de $\mathbb{R}$, $a$ une application continue de $I$ dans $\mathscr{L}(E)$ et $b$ une application continue de $I$ dans $E$.<br>
Pour tout $(t_0, x_0) \in I \times E$, il existe une et une seule solution sur $I$ de l'équation différentielle $x' = a(t)x + b(t)$ vérifiant la condition initiale $x(t_0) = x_0$.<br>
$\checkmark$ La continuité des applications $a$ et $b$ est indispensable.<br>
Par exemple, l'équation différentielle $x' = f(x)$ avec $f(x) = \begin{cases} 1 & \text{si } x > 0 \\ 0 & \text{si } x \leq 0 \end{cases}$ n'a pas de solution sur $\mathbb{R}$ (en effet, une telle solution ne pourrait être dérivable en 0). (Note: correction de l'exemple)</li>
<li>Matriciellement, ce théorème s'énonce ainsi.<br>
Soient $A: I \rightarrow \mathcal{M}_n(\mathbb{K})$ et $B: I \rightarrow \mathcal{M}_{n,1}(\mathbb{K})$ des applications continues.<br>
Soit $t_0 \in I$, et soit $X_0 \in \mathcal{M}_{n,1}(\mathbb{K})$.<br>
Alors il existe une unique application $X: I \rightarrow \mathcal{M}_{n,1}(\mathbb{K})$ telle que :
$$\forall t \in I, X'(t) = A(t)X(t) + B(t) \quad \text{et} \quad X(t_0) = X_0.$$</li>
</ul>
<h2>[S17.3] Structure de l'ensemble des solutions</h2>
<h3>Solutions d'une équation homogène</h3>
<p>L'équation $(H): x' = a(t)x$ s'appelle l'équation homogène associée à l'équation $(L): x' = a(t)x + b(t)$.<br>
L'ensemble $\mathscr{S}_H$ des solutions de l'équation différentielle homogène $(H)$ est un sous-espace vectoriel de l'espace vectoriel $\mathscr{C}^1(I,E)$ des fonctions de classe $\mathscr{C}^1$ de $I$ dans $E$.<br>
De plus, cet espace vectoriel est de dimension finie, et $\dim \mathscr{S}_H = \dim E$.<br>
$\checkmark$ Ce résultat n'est valable que sur un intervalle.<br>
Exemple : l'espace des solutions sur $\mathbb{R}^*$ de l'équation $x' = 0$ est de dimension 2, puisque les solutions sont les applications telles que $x(t) = C_1$ sur $]-\infty;0[$ et $x(t) = C_2$ sur $]0;+\infty[$.</p>
<h3>Solutions d'une équation avec second membre</h3>
<p>Si $x_L$ est une solution particulière de l'équation $(L): x' = a(t)x + b(t)$ (il en existe, d'après le théorème de Cauchy-Lipschitz), alors l'ensemble des solutions de l'équation $(L)$ est exactement l'ensemble des fonctions somme de $x_L$ et d'une solution quelconque de l'équation homogène $(H)$ :
$$\mathscr{S}_L = x_L + \mathscr{S}_H.$$</p>
<h3>Principe de superposition des solutions</h3>
<p>Soient $b_1: I \rightarrow E$ et $b_2: I \rightarrow E$ deux applications continues, et $\lambda_1, \lambda_2$ dans $\mathbb{K}$.<br>
Soient $x_1$ et $x_2$ deux solutions respectives des équations différentielles
$$(L_1): \quad x' = a(t)x + b_1(t) \quad \text{et} \quad (L_2): \quad x' = a(t)x + b_2(t).$$
Alors $\lambda_1 x_1 + \lambda_2 x_2$ est solution de l'équation différentielle :
$$(L): \quad x' = a(t)x + \lambda_1 b_1(t) + \lambda_2 b_2(t).$$</p>

<!-- Image omitted: image0010176 -->
<!-- Page 172 -->
<h3>Système fondamental de solution, wronskien</h3>
<p>Soit $\mathcal{B}$ une base de $E$, et $(x_1, \ldots, x_n)$ une famille formée de $n$ solutions de l'équation homogène $(H)$.<br>
On appelle matrice wronskienne de cette famille la matrice $\mathcal{W}(t)$ des vecteurs $(x_1(t), \ldots, x_n(t))$ dans la base $\mathcal{B}$ et on appelle wronskien de cette famille, noté $W(t)$, le déterminant de cette matrice :
$$\forall t \in I, W(t) = \det \mathcal{W}(t) = \det_{\mathcal{B}}(x_1(t), \ldots, x_n(t)).
$$
On appelle système fondamental de solutions de l'équation $(H) : x' = a(t)x$ (Note: Correction L à H) toute base de l'espace vectoriel $\mathcal{S}_H$ des solutions de l'équation homogène $(H)$.<br>
Les propositions suivantes sont équivalentes :</p>
<ol type="i">
<li>$(x_1, \ldots, x_n)$ est un système fondamental de solutions de $(H)$,</li>
<li>$\forall t \in I, W(t) \neq 0$,</li>
<li>$\exists t_0 \in I$ tel que $W(t_0) \neq 0$.</li>
</ol>
<p>Si $(x_1, \ldots, x_n)$ est un système fondamental de solutions, toute solution de l'équation homogène $x' = a(t)x$ est donc combinaison linéaire de $(x_1, \ldots, x_n)$ :
$$\mathcal{S}_H = \text{Vect}(x_1, \ldots, x_n) = \left\{ \sum_{k=1}^{n} \lambda_k x_k \mid (\lambda_1, \ldots, \lambda_n) \in \mathbb{K}^n \right\}.
$$
En notant $X(t)$ la matrice colonne des coordonnées de $x(t)$ dans $\mathcal{B}$, la relation $x = \sum_{k=1}^{n} \lambda_k x_k$ s'écrit $X(t) = \mathcal{W}(t)C$Où $\mathcal{W}$ est la matrice wronskienne de $(x_1, \ldots, x_n)$ et $C$ le vecteur colonne $C = ^t(\lambda_1 \ldots \lambda_n)$, de sorte que, matriciellement :
$$\mathcal{S}_H = \left\{ t \mapsto \mathcal{W}(t)C \mid C \in \mathcal{M}_{n,1}(\mathbb{K}) \right\}.
$$</p>
<h2>[S17.4] Méthode de Lagrange, ou de variation des constantes</h2>
<p>On considère ici une équation différentielle linéaire écrite matriciellement dans une base $\mathcal{B}$ de $E$ sous la forme :
$$(L) \quad X'(t) = A(t)X(t) + B(t).
$$
On suppose qu'on connaît un système fondamental de solutions $(x_1, \ldots, x_n)$, et on note $\mathcal{W}$ sa matrice wronskienne dans $\mathcal{B}$.<br>
La méthode de Lagrange consiste à chercher les solutions de $(L)$ sous la forme $X(t) = \mathcal{W}(t)C(t)$Où $C$ est une application de classe $\mathcal{C}^1$ de $I$ dans $\mathcal{M}_{n,1}(\mathbb{K})$.<br>
Plus précisément :
$$X(t) = \mathcal{W}(t)C(t) \text{ est solution de } (L) \iff \mathcal{W}(t)C'(t) = B(t) \iff C'(t) = \mathcal{W}(t)^{-1}B(t).
$$</p>

<!-- Image omitted: image0010177 -->
<!-- Page 173 -->
<p>Si $C(t) = ^t (\lambda_1(t) \ldots \lambda_n(t))$, cela revient à chercher les solutions de $(L)$ sous la forme :
$$x(t) = \sum_{k=1}^{n} \lambda_k(t) x_k(t),
$$
avec la condition : $\sum_{k=1}^{n} \lambda_k'(t) x_k(t) = b(t)$. La résolution de ce système permet alors d'obtenir les fonctions $\lambda_k'$ puis les fonctions $\lambda_k$ (à une constante près).</p>
<h2>[S17.5] Systèmes différentiels linéaires à coefficients constants</h2>
<p>On s'intéresse ici à des systèmes différentiels de la forme : $X'(t) = AX(t) + B(t)$, où $A \in \mathcal{M}_n(\mathbb{K})$ ne dépend pas de $t$.</p>
<h3>Utilisation de l'exponentielle de matrice</h3>
<ul>
<li><strong>Rappels</strong>
<ul>
<li>Nous avons vu en [S11.9] que l'on peut définir l'exponentielle d'une matrice carrée $A$, ou d'un endomorphisme $a$ d'un espace vectoriel de dimension finie, par :
$$e^A = \exp(A) = \sum_{k=0}^{+\infty} \frac{A^k}{k!} \quad \text{ou} \quad e^a = \exp(a) = \sum_{k=0}^{+\infty} \frac{a^k}{k!}.
$$
De plus, si $A$, $B$ sont deux matrices carrées qui commutent, ou si $a$, $b$ sont deux endomorphismes qui commutent, on a :
$$\exp(A + B) = \exp(A) \exp(B) \quad \text{ou} \quad \exp(a + b) = \exp(a) \circ \exp(b).
$$ (Note: Corrigé de + à $\exp(A)\exp(B)$ etc.)</li>
<li>Avec les mêmes notations, en appliquant les résultats de [S12.11] concernant les séries de fonctions de classe $\mathscr{C}^1$, on montre que les applications $e_A : t \in \mathbb{R} \mapsto \exp(tA)$Ou $e_a : t \in \mathbb{R} \mapsto \exp(ta)$ sont de classe $\mathscr{C}^1$ sur $\mathbb{R}$ et que :
$$e_A' = Ae_A = e_A A \quad \text{ou} \quad e_a' = a \circ e_a = e_a \circ a.
$$</li>
</ul>
</li>
<li>L'ensemble des solutions du système homogène $(H) : X'(t) = AX(t)$Où $A \in \mathcal{M}_n(\mathbb{K})$ est constante est l'ensemble :
$$\mathscr{S}_H = \{ t \mapsto e^{tA}C \mid C \in \mathcal{M}_{n,1}(\mathbb{K}) \}.
$$
Autrement dit, un système fondamental de solutions de $(H)$ est formé des vecteurs colonnes de la matrice $e^{tA}$.</li>
<li>Pour résoudre alors l'équation avec second membre $(L) : X'(t) = AX(t) + B(t)$, on utilise la méthode de variation des constantes en cherchant les solutions sous la forme :
$$X(t) = e^{tA}C(t)
$$
qui conduit ici à :
$$C'(t) = e^{-tA}B(t).
$$</li>
</ul>

<!-- Image omitted: image0010178 -->
<!-- Page 174 -->
<h3>Utilisation de la réduction</h3>
<ul>
<li>On suppose que $A$ admet une valeur propre $\lambda$.<br>
Si $V$ est un vecteur propre associé à cette valeur propre, alors la fonction $t \mapsto e^{\lambda t} V$ est solution sur $\mathbb{R}$ de l'équation homogène $X'(t) = A X(t)$.</li>
<li>On suppose ici que $A$ est diagonalisable. Il existe donc une base $(V_1, \ldots, V_n)$ de $\mathcal{M}_{n,1}(\mathbb{K})$ formée de vecteurs propres de $A$, pour les valeurs propres $(\lambda_1, \ldots, \lambda_n)$.<br>
Alors les fonctions $X_i : t \mapsto e^{\lambda_i t} V_i$, pour $i \in 〚 1 ; n 〛$, forment une base de l'espace vectoriel $\mathcal{S}_H$ des solutions de l'équation homogène $X'(t) = A X(t)$.<br>
Toute solution de l'équation homogène est donc de la forme :
$$t \mapsto \sum_{i=1}^n \alpha_i e^{\lambda_i t} V_i \quad \text{avec} \ (\alpha_1, \ldots, \alpha_n) \in \mathbb{K}^n.$$</li>
<li>De façon plus générale, si la matrice $A$ est semblable à une matrice $T$ plus simple (triangulaire supérieure en général), on effectuera un changement de base, ce qui revient à changer de fonctions inconnues, pour obtenir un système différentiel plus facile à résoudre.<br>
Le principe est simple : si $T = P^{-1} A P$, le système $X'(t) = A X(t)$ équivaut à $P^{-1} X'(t) = T P^{-1} X(t)$, donc, en posant $Y(t) = P^{-1} X(t)$, soit $X(t) = P Y(t)$, on obtiendra le nouveau système différentiel $Y'(t) = T Y(t)$ (en effet, $X(t) = P Y(t)$ implique $X'(t) = P Y'(t)$ car la matrice $P$ est constante).<br>
$\checkmark$ Remarquez que dans cette méthode, dans le cas d'une équation homogène, le calcul de la matrice inverse $P^{-1}$ n'est pas utile en pratique puisque la solution finale s'obtient par $X(t) = P Y(t)$.<br>
$\checkmark$ D'après les calculs exposés ci-dessus, la méthode peut aussi s'appliquer dans le cas d'une équation de la forme $X'(t) = A(t) X(t)$ sous réserve que la matrice de passage $P$ ne dépende pas de $t$.</li>
</ul>
<h2>[S17.6] Équations différentielles linéaires scalaires du premier ordre</h2>
<p>Il s'agit d'équations différentielles de la forme :
$$\alpha(t) x' + \beta(t) x = \gamma(t),$$
où les fonctions $\alpha, \beta$ et $\gamma$ sont continues sur un certain intervalle $I$, à valeurs dans $\mathbb{K}$. Les solutions $x$ sont à chercher parmi les applications de $I$ dans $\mathbb{K}$.</p>
<ul>
<li>La première chose à faire est d'examiner si la fonction $\alpha$ peut ou non s'annuler sur $I$. Si oui, on cherche les sous-intervalles de $I$Où $\alpha$ ne s'annule pas, et on résoudra l'équation sur chacun de ces intervalles (et il y aura des constantes d'intégration différentes sur chaque intervalle).</li>
<li>Un tel intervalle $J$ étant choisi, on peut alors diviser l'équation par $\alpha(t)$ et on se ramène ainsi à une équation différentielle sous forme résolue :
$$x' = a(t) x + b(t) \quad (L)$$
On peut remarquer que les solutions d'une telle équation sont nécessairement de classe $\mathcal{C}^1$ sur $J$.</li>
</ul>

<!-- Image omitted: image0010179 -->
<!-- Page 175 -->
<ul>
<li><strong>Intégration de l'équation homogène</strong> $(H)$ : $x' = a(t)x$<br>
Les solutions de l'équation différentielle $x' = a(t)x$ sont les fonctions de la forme :
$$t \mapsto C \exp \left( \int_{t_0}^t a(u) \, du \right),$$
où $C$ est une constante dans $\mathbb{K}$ et où $t \mapsto \int_{t_0}^t a(u) \, du$ est une primitive de la fonction $a$.<br>
$\checkmark$On retrouve ainsi le théorème de Cauchy-Lipschitz : l'ensemble des solutions de $(H)$ sur $J$ est ici un $\mathbb{K}$-espace vectoriel de dimension 1.</li>
<li><strong>Intégration de l'équation avec second membre</strong> $(L)$ : $x' = a(t)x + b(t)$<br>
L'ensemble des solutions de l'équation complète $(L)$ est l'ensemble des fonctions de la forme $x = x_L + y$, où $y$ est solution de l'équation homogène $(H)$ et où $x_L$ est une solution particulière de $(L)$.<br>
Si l'on connaît une solution particulière $x_L$ de $(L)$, la résolution est terminée. Sinon, on peut rechercher une telle solution par la <em>méthode de variation de la constante</em>, expliquée ci-dessous.<br>
Soit $y$ une solution non nulle de $(H)$, de la forme $y(t) = \exp \left( \int_{t_0}^t a(u) \, du \right)$. Une telle solution ne peut s'annuler, et on peut donc chercher les solutions de $(L)$ sous la forme $x(t) = C(t)y(t)$ avec $C$ de classe $\mathcal{C}^1$. Un calcul simple montre alors que $(L)$ est équivalente à l'équation :
$$C'(t)y(t) = b(t) \quad \text{soit} \quad C'(t) = \frac{b(t)}{y(t)}.$$
Cela permet d'obtenir $C(t)$ par une simple primitivation, puis d'en déduire la solution $x(t)$.<br>
$\checkmark$ Il est bon de retenir les formules ci-dessus, en particulier de se rappeler que dans l'équation transformée apparaît seulement $C'(t)$ et non plus $C(t)$. Il est en fait inutile de refaire le calcul de la dérivée de $t \mapsto C(t)y(t)$ lorsqu'on connaît ces formules.</li>
<li><strong>Cas d'une équation à coefficients constants</strong><br>
Il s'agit ici d'une équation de la forme $x' - ax = b(t)$, où $a \in \mathbb{K}$ est une constante. Dans ce cas, l'ensemble des solutions de l'équation homogène $x' - ax = 0$ est l'ensemble des fonctions de la forme $t \mapsto Ce^{at}$.<br>
La méthode de variation de la constante s'applique bien sûr pour la résolution de l'équation complète, mais il y a un cas particulier à retenir. Si le second membre est de la forme $b(t) = e^{mt}P(t)$Où $m \in \mathbb{C}$ et où $P$ est un polynôme, on peut rechercher (avec des coefficients à déterminer) une solution particulière de l'équation $x' - ax = e^{mt}P(t)$ sous la forme :
<ul>
<li>$e^{mt}Q(t)$ avec $Q$ polynôme de même degré que $P$ si $m \neq a$,</li>
<li>$e^{mt}tQ(t)$ avec $Q$ polynôme de même degré que $P$ si $m = a$.</li>
</ul>
</li>
<li>Pour résoudre complètement l'équation initiale $\alpha(t)x' + \beta(t)x = \gamma(t)$ sur tout l'intervalle $I$, il faut ensuite (éventuellement) raccorder les solutions trouvées précédemment sur les intervalles où la fonction $\alpha$ ne s'annule pas.</li>
</ul>

<!-- Image omitted: image0010180 -->
<!-- Page 176 -->
<h2>[S17.7] Équations différentielles linéaires scalaires du second ordre</h2>
<p>Il s'agit d'équations différentielles de la forme :
$$\alpha(t) x'' + \beta(t) x' + \gamma(t) x = \delta(t),
$$
où les fonctions $\alpha, \beta, \gamma$ et $\delta$ sont continues sur un certain intervalle $I$, à valeurs dans $\mathbb{K}$.</p>
<ul>
<li>Comme pour les équations du premier ordre, il faut résoudre une telle équation sur un intervalle $J \subset I$Où la fonction $\alpha$ ne s'annule pas.<br>
Un tel intervalle $J$ étant choisi, on peut alors diviser l'équation par $\alpha(t)$ et on se ramène ainsi à une équation différentielle sous forme résolue :
$$x'' = a(t) x' + b(t) x + c(t) \quad (L)
$$
où $a, b$ et $c$ sont des applications continues de $J$ dans $\mathbb{K}$.<br>
On peut remarquer que les solutions $x$ d'une telle équation sont nécessairement de classe $\mathcal{C}^2$ sur $J$.</li>
<li><strong>Système différentiel associé</strong><br>
L'équation $(L)$ précédente peut s'écrire :
$$\begin{pmatrix}
x'(t) \\
x''(t)
\end{pmatrix} =
\begin{pmatrix}
0 & 1 \\
b(t) & a(t)
\end{pmatrix}
\begin{pmatrix}
x(t) \\
x'(t)
\end{pmatrix} +
\begin{pmatrix}
0 \\
c(t)
\end{pmatrix},
$$
soit :
$$X' = A X + B,
$$
avec : $X(t) = \begin{pmatrix} x(t) \\ x'(t) \end{pmatrix}$, $A(t) = \begin{pmatrix} 0 & 1 \\ b(t) & a(t) \end{pmatrix}$ et $B(t) = \begin{pmatrix} 0 \\ c(t) \end{pmatrix}$.<br>
On est donc ramené à un système différentiel linéaire du premier ordre. Les résultats obtenus pour ce type d'équation permettent alors d'obtenir les résultats qui suivent.</li>
<li>Une famille $(x_1, x_2)$ de solutions de l'équation homogène : $x'' = a(t) x' + b(t) x$ est un système fondamental de solutions si et seulement si le wronskien $W(t) = \begin{vmatrix} x_1(t) & x_2(t) \\ x_1'(t) & x_2'(t) \end{vmatrix}$ est non nul en tout point de $J$ (il suffit qu'il le soit en au moins un point).<br>
Dans ce cas, l'ensemble $\mathscr{S}_H$ des solutions de l'équation homogène est l'espace vectoriel engendré par $x_1$ et $x_2$ soit :
$$\mathscr{S}_H = \{\lambda_1 x_1 + \lambda_2 x_2, (\lambda_1, \lambda_2) \in \mathbb{K}^2\}.
$$</li>
<li>L'ensemble $\mathscr{S}_L$ des solutions de $(L)$ sur $J$ est un espace affine de dimension 2 : $\mathscr{S}_L = x_L + \mathscr{S}_H$, où $x_L$ est une solution particulière de $(L)$.</li>
<li>Le théorème de Cauchy-Lipschitz linéaire s'énonce ici : pour tout $t_0 \in J$ et tout $(x_0, x_0') \in \mathbb{K}^2$, il existe une solution et une seule solution de $(L)$ sur $J$ vérifiant les conditions initiales :
$$x(t_0) = x_0 \quad \text{et} \quad x'(t_0) = x_0'.
$$</li>
</ul>

<!-- Image omitted: image0010181 -->
<!-- Page 177 -->
<ul>
<li><strong>Cas où l'on connaît une solution de l'équation homogène</strong><br>
Dans ce cas, on peut, comme pour les équations du premier ordre, appliquer la méthode de variation de la constante (encore appelée <em>méthode de Lagrange</em>). En voici le principe.<br>
Considérons l'équation différentielle (pas forcément écrite sous forme résolue) :
$$\alpha(t)x'' + \beta(t)x' + \gamma(t)x = \delta(t) \quad (L).
$$
Supposons connue une solution $x_L$ de l'équation *homogène*. (Note: doit être $x_H$) <br>
Sur un intervalle où la fonction $x_H$ ne s'annule pas, on peut chercher une solution quelconque de l'équation $(L)$ sous la forme : $x(t) = y(t)x_H(t)$ (en effet, puisque $x_H$ ne s'annule pas, la fonction $y = \frac{x}{x_H}$ est bien définie et deux fois dérivable).<br>
Quelques calculs montrent alors que $x$ est solution de $(L)$ si et seulement si :
$$\alpha(t)x_H(t)y''(t) + (2\alpha(t)x_H'(t) + \beta(t)x_H(t))y'(t) = \delta(t).
$$
C'est une équation scalaire du premier ordre pour la fonction $y'$. Il ne reste « plus qu'à » la résoudre pour trouver $y'$, d'où l'on déduit $y$ puis $x$.</li>
<li><strong>Cas où l'on connaît deux solutions de l'équation homogène</strong><br>
Considérons l'équation différentielle :
$$x'' = a(t)x' + b(t)x + c(t) \quad (L).
$$
On suppose connu un système fondamental de solutions $(x_1, x_2)$ de l'équation homogène $(H)$ : $x'' = a(t)x' + b(t)x$.<br>
La méthode de variations des constantes consiste ici à chercher les solutions $x$ de $(L)$ sous la forme :
$$x(t) = \lambda_1(t)x_1(t) + \lambda_2(t)x_2(t),
$$
où $\lambda_1, \lambda_2$ sont de classe $\mathscr{C}^1$ et vérifient :
$$\lambda_1'(t) \begin{pmatrix} x_1(t) \\ x_1'(t) \end{pmatrix} + \lambda_2'(t) \begin{pmatrix} x_2(t) \\ x_2'(t) \end{pmatrix} = \begin{pmatrix} 0 \\ c(t) \end{pmatrix},
$$
ce qui amène à résoudre le système :
$$\begin{cases}
\lambda_1' x_1 + \lambda_2' x_2 = 0 \\
\lambda_1' x_1' + \lambda_2' x_2' = c
\end{cases}.
$$
Ce système permet de calculer $\lambda_1'$ et $\lambda_2'$, d'où on déduit par primitivation $\lambda_1$ et $\lambda_2$, et finalement la solution générale de $(L)$ : $x = \lambda_1 x_1 + \lambda_2 x_2$.<br>
$\checkmark$ Il ne sert à rien d'apprendre ces formules par cœur. La meilleure façon de retenir cette méthode consiste à bien comprendre d'où elle provient : il s'agit simplement de la méthode de variation des constantes pour le système différentiel associé à l'équation $(L)$.</li>
</ul>

<!-- Image omitted: image0010182 -->
<!-- Page 178 -->
<h2>[S17.8] Équations différentielles du 2e ordre à coefficients constants</h2>
<p>Soit $(L)$ une équation différentielle de la forme :
$$ x'' + ax' + bx = c(t) \quad \text{avec} \ a, b \in \mathbb{K} \ \text{et} \ c \ \text{continue sur} \ I \ \text{à valeurs dans} \ \mathbb{K}$$
(où $\mathbb{K}$ désigne $\mathbb{R}$ ou $\mathbb{C}$).<br>
L'équation homogène associée est $(H)$ : $x'' + ax' + bx = 0$.</p>
<h3>Résolution de l'équation homogène</h3>
<p>On cherche des solutions de cette équation de la forme $t \mapsto e^{rt}$. Un calcul immédiat montre que cette fonction est solution de $(H)$ si et seulement si $r^2 + ar + b = 0$.<br>
À l'équation $(H)$, on associe donc son <em>équation caractéristique</em> :
$$ (E_c) : r^2 + ar + b = 0 , $$
et on étudie ses solutions. Il faut distinguer deux cas, selon que le corps de base est $\mathbb{R}$Ou $\mathbb{C}$; on notera $\Delta$ le discriminant de l'équation caractéristique.</p>
<h5>Si $\mathbb{K} = \mathbb{C}$</h5>
<ul>
<li>Si $\Delta \neq 0$, $(E_c)$ possède deux racines distinctes $r_1$ et $r_2$; dans ce cas, les fonctions $x_1 : t \mapsto e^{r_1 t}$ et $x_2 : t \mapsto e^{r_2 t}$ sont deux solutions linéairement indépendantes de $(H)$, et $\mathcal{S}_H$ est l'ensemble des applications de la forme :
$$ t \mapsto \lambda_1 e^{r_1 t} + \lambda_2 e^{r_2 t}, (\lambda_1, \lambda_2) \in \mathbb{C}^2 . $$</li>
<li>Si $\Delta = 0$, $(E_c)$ possède une racine double $r$; dans ce cas, les fonctions $x_1 : t \mapsto e^{rt}$ et $x_2 : t \mapsto t e^{rt}$ sont deux solutions linéairement indépendantes de $(H)$, et $\mathcal{S}_H$ est l'ensemble des applications de la forme :
$$ t \mapsto (\lambda_1 t + \lambda_2) e^{rt}, (\lambda_1, \lambda_2) \in \mathbb{C}^2 . $$</li>
</ul>
<h5>Si $\mathbb{K} = \mathbb{R}$</h5>
<ul>
<li>Si $\Delta \geq 0$, on a les mêmes résultats que ci-dessus, mais avec $(\lambda_1, \lambda_2) \in \mathbb{R}^2$.</li>
<li>Si $\Delta < 0$, l'équation $(E_c)$ possède deux racines complexes non réelles conjuguées $\alpha \pm i \beta$; alors les fonctions $x_1 : t \mapsto e^{\alpha t} \cos \beta t$ et $x_2 : t \mapsto e^{\alpha t} \sin \beta t$ sont deux solutions linéairement indépendantes de $(H)$, et $\mathcal{S}_H$ est l'ensemble des applications de la forme :
$$ t \mapsto (\lambda_1 \cos \beta t + \lambda_2 \sin \beta t) e^{\alpha t}, (\lambda_1, \lambda_2) \in \mathbb{R}^2 . $$</li>
</ul>
<h3>Résolution de l'équation avec second membre</h3>
<p>Il y a bien sûr toujours la méthode de variation des constantes, puisque l'on vient de trouver deux solutions linéairement indépendantes de l'équation homogène. Il s'agit de la méthode à préférer mais il y a un cas particulier à connaître. Si le second membre est de la forme $e^{mt} P(t)$, où $P$ est un polynôme et $m \in \mathbb{C}$, on cherchera une solution particulière de $(L)$ sous la forme :
$$ x(t) = t^k e^{mt} Q(t) \ \text{où} \ \begin{cases} k = 0 & \text{si} \ m \ \text{n'est pas racine de} \ (E_c) \\ k = 1 & \text{si} \ m \ \text{est racine simple de} \ (E_c) \\ k = 2 & \text{si} \ m \ \text{est racine double de} \ (E_c) \end{cases}$$
avec $Q$ polynôme de même degré que $P$.</p>

<!-- Image omitted: image0010183 -->
<!-- Page 179 -->
<h1>Thème 18 - Calcul différentiel</h1>
<p>On étudie ici des fonctions définies sur un $\mathbb{R}$-espace vectoriel normé $E$ de dimension $p \in \mathbb{N}^*$, à valeurs dans un $\mathbb{R}$-espace vectoriel normé $F$ de dimension $n \in \mathbb{N}^*$. Quitte à choisir des bases dans ces espaces vectoriels, on pourra se ramener à l'étude de fonctions de $\mathbb{R}^p$ dans $\mathbb{R}^n$.<br>
Les espaces vectoriels considérés étant de dimensions finies, toutes les normes y sont équivalentes. On notera donc $\| \|$ une norme quelconque sur $E$Ou $F$.<br>
Les applications considérées seront toujours définies sur un <em>ouvert</em> de $E$; cela assure que, si $f$ est définie en $a \in U$, alors elle reste définie dans tout un voisinage de $a$.</p>
<h2>[S18.1] Rappels</h2>
<h3>Applications coordonnées</h3>
<p>Si $f$ est une application de $U \subset \mathbb{R}^p$ dans $\mathbb{R}^n$, on a :
$$\forall x \in U, f(x) = (f_1(x), \ldots, f_n(x))$$
où les $f_i$ sont des applications de $U$ dans $\mathbb{R}$ : ce sont les applications coordonnées de $f$.<br>
L'utilisation de ces applications coordonnées permet de ramener l'étude des fonctions de $\mathbb{R}^p$ dans $\mathbb{R}^n$ à celle des fonctions de $\mathbb{R}^p$ dans $\mathbb{R}$.</p>
<h3>Applications partielles</h3>
<p>Soit $f : U \subset \mathbb{R}^p \to \mathbb{R}^n$, et $a = (a_1, \ldots, a_p) \in U$. Pour tout $j \in [1, p]$, la $j$-ème application partielle de $f$ en $a$ est l'application :
$$ f_{a, j} : x \mapsto f(a_1, \ldots, a_{j-1}, x, a_{j+1}, \ldots, a_p). $$
Elle est définie sur un ouvert de $\mathbb{R}$ contenant $a_j$ et à valeurs dans $\mathbb{R}^n$.</p>
<h3>Continuité</h3>
<p>Une application $f : U \subset \mathbb{R}^p \to \mathbb{R}^n$ d'un ouvert $U$ de $\mathbb{R}^p$ à valeurs dans $\mathbb{R}^n$ est dite continue en $a \in U$ si :
$$\forall \epsilon > 0, \exists \alpha > 0, \forall x \in U, \| x - a \| < \alpha \implies \| f(x) - f(a) \| < \epsilon $$
ou, plus simplement :
$$\lim_{x \to a} f(x) = f(a). $$</p>
<ul>
<li>Une combinaison linéaire, une composée, un produit ou un quotient (pour des applications à valeurs dans $\mathbb{R}$) d'applications continues est une application continue là où elle est définie.</li>
<li>Une application $f : U \subset \mathbb{R}^p \to \mathbb{R}^n$ est continue si et seulement si ses applications coordonnées le sont.</li>
<li>Si $f : U \subset \mathbb{R}^p \to \mathbb{R}^n$ est continue en $a \in U$, alors toutes ses applications partielles $f_{a, j}$ en ce point sont continues.<br>
$\checkmark$ La réciproque de cette propriété est fausse : la continuité des seules applications partielles ne suffit pas à assurer la continuité en un point.</li>
</ul>

<!-- Image omitted: image0010184 -->
<!-- Page 180 -->
<h2>[S18.2] Différentiabilité</h2>
<h3>Différentielle</h3>
<p>Soit $f$ une application définie sur un ouvert $U$ de $E$ à valeurs dans $F$. On dit que $f$ est différentiable en un point $a$ de $U$ s'il existe une application linéaire $df_a \in \mathscr{L}(E, F)$ telle que :<br>
pour tout $h \in E$ tel que $a + h \in U$, $f(a + h) = f(a) + df_a(h) + o(\|h\|)$.<br>
$df_a$ s'appelle la différentielle de $f$ en $a$ (ou application linéaire tangente à $f$ en $a$). L'écriture ci-dessus s'appelle un développement limité d'ordre 1 de $f$ en $a$.</p>
<ul>
<li>Si $f$ est différentiable en $a$, alors $f$ est continue en $a$.<br>
$\checkmark$ La réciproque de cette propriété est fausse!</li>
</ul>
<h3>Dérivée selon un vecteur</h3>
<p>Soit $f$ une application définie sur un ouvert $U$ de $E$ à valeurs dans $F$, et $a$ un point de $U$. Soit $\vec{v}$ un vecteur non nul de $E$, et $t$ un réel tel que $a + t \vec{v} \in U$. On dit que $f$ admet une dérivée selon le vecteur $\vec{v}$ au point $a$ si la fonction $t \mapsto f(a + t \vec{v})$ est dérivable en 0.<br>
Le vecteur dérivé correspondant est appelé dérivée de $f$ suivant le vecteur $\vec{v}$, et est noté $\frac{\partial f}{\partial \vec{v}}(a)$Ou $D_{\vec{v}} f(a)$. Ainsi :
$$\frac{\partial f}{\partial \vec{v}}(a) = D_{\vec{v}} f(a) = \lim_{t \to 0} \frac{f(a + t \vec{v}) - f(a)}{t}.
$$</p>
<h3>Théorème</h3>
<p>Soit $f$ une application définie sur un ouvert $U$ de $E$ à valeurs dans $F$, et $a$ un point de $U$. Si $f$ est différentiable en $a$, alors pour tout vecteur $\vec{v}$ de $E$, $f$ admet une dérivée selon $\vec{v}$ et :
$$D_{\vec{v}} f(a) = df_a(\vec{v}).
$$
$\checkmark$ La réciproque est fausse : une fonction peut admettre une dérivée en un point selon n'importe quel vecteur sans pour autant être différentiable en ce point.</p>
<h2>[S18.3] Dérivées partielles</h2>
<ul>
<li>Soit $f$ une application définie sur un ouvert $U$ de $E$ à valeurs dans $F$, et $a$ un point de $U$.<br>
Soit $\mathscr{B}_E = (\vec{e}_1, \ldots, \vec{e}_p)$ une base de $E$; on confondra alors tout vecteur $x \in E$ avec le $p$-uplet de ses coordonnées $(x_1, \ldots, x_p)$, et on écrira indifféremment $f(x)$Ou $f(x_1, \ldots, x_p)$.<br>
On appelle dérivées partielles (premières) de $f$ ses dérivées suivant les vecteurs $\vec{e}_1, \ldots, \vec{e}_p$ (si elles existent).<br>
La $j^e$ dérivée partielle en $a$ est notée $\partial_j f(a)$Ou $\frac{\partial f}{\partial x_j}(a)$ (c'est un vecteur appartenant à $F$). Ainsi :
$$\forall j \in [1, p], \frac{\partial f}{\partial x_j}(a) = D_{\vec{e}_j} f(a) = \lim_{t \to 0} \frac{f(a + t \vec{e}_j) - f(a)}{t}.
$$</li>
</ul>

<!-- Image omitted: image0010185 -->
<!-- Page 181 -->
<p>Si $a = (a_1, \ldots, a_p)$, c'est donné aussi la dérivée en $a_j$ de la $j^e$ application partielle $f_{a, j}$ : (Note: $\alpha$ remplacé par $a$)
$$\frac{\partial f}{\partial x_j}(a) = \lim_{t \to 0} \frac{f(a_1, \ldots, a_{j-1}, a_j + t, a_{j+1}, \ldots, a_p) - f(a_1, \ldots, a_p)}{t}.
$$</p>
<ul>
<li>Si $f$ est différentiable en $a$, alors $f$ admet en ce point des dérivées partielles par rapport à chacune des variables et l'on a :
$$\forall h = (h_1, \ldots, h_p) \in E, \quad \mathrm{d}f_a(h) = \sum_{j=1}^p h_j \frac{\partial f}{\partial x_j}(a).
$$</li>
</ul>
<h2>[S18.4] Matrice jacobienne</h2>
<ul>
<li>Soit $f$ une application définie sur un ouvert $U$ de $E$ à valeurs dans $F$. On choisit dans $E$ une base $\mathcal{B}_E$ et dans $F$ une base $\mathcal{B}_F$.<br>
On notera $f_1, \ldots, f_n$ les applications coordonnées de $f$ dans la base $\mathcal{B}_F$, c'est-à-dire
