<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cours - Groupe Symétrique et Déterminant</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"  crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"  crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"  crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <style>
        body { font-family: sans-serif; line-height: 1.6; padding: 20px; }
        h1, h2, h3 { color: #0056b3; }
        .definition, .theorem, .proposition, .proof, .example, .corollary, .remark {
            margin: 15px 0;
            padding: 15px;
            border-left: 4px solid;
        }
        .definition { border-color: #17a2b8; background-color: #e1f5fe; }
        .theorem { border-color: #28a745; background-color: #e8f5e9; }
        .proposition { border-color: #ffc107; background-color: #fff8e1; }
        .proof { border-color: #6c757d; background-color: #f8f9fa; font-style: italic; }
        .example { border-color: #fd7e14; background-color: #fff3e0; }
        .corollary { border-color: #007bff; background-color: #e7f3ff; }
        .remark { border-color: #adb5bd; background-color: #e9ecef; }
        strong { color: #0056b3; }
        code { background-color: #eee; padding: 2px 4px; border-radius: 3px; }
        .katex-display { overflow-x: auto; overflow-y: hidden; } /* Allow scrolling for wide formulas */
        .video-link { font-style: italic; color: #6c757d; margin-left: 10px; }
    </style>
</head>
<body>

<h1>Groupe Symétrique et Déterminant</h1>

<div class="remark">
    <strong>Notations préliminaires :</strong>
    <ul>
        <li>$\mathbb{K}$ désigne un corps commutatif (souvent $\mathbb{R}$ ou $\mathbb{C}$).</li>
        <li>$n \ge 1$ est un entier.</li>
        <li>$E, E_1, \dots, E_n, F$ sont des $\mathbb{K}$-espaces vectoriels.</li>
        <li>$\{1, \dots, n\}$ désigne l'ensemble des $n$ premiers entiers non nuls.</li>
    </ul>
</div>

<h2>I. Groupe symétrique $\mathfrak{S}_n$</h2>

<div class="definition">
    <strong>Groupe symétrique</strong><br>
    Soit $n \ge 1$. On appelle <strong>groupe symétrique</strong> d'indice $n$, noté $\mathfrak{S}_n$, l'ensemble des bijections de l'ensemble $\{1, \dots, n\}$ sur lui-même.
    $$ \mathfrak{S}_n = \{ \sigma: \{1, \dots, n\} \to \{1, \dots, n\} \mid \sigma \text{ est bijective} \} $$
    Muni de la loi de composition des applications ($\circ$), $(\mathfrak{S}_n, \circ)$ est un groupe. Ses éléments sont appelés des <strong>permutations</strong> de $\{1, \dots, n\}$.
    Le cardinal de $\mathfrak{S}_n$ est $|\mathfrak{S}_n| = n!$.
</div>

<div class="remark">
    <strong>Notation des permutations :</strong>
    Une permutation $\sigma \in \mathfrak{S}_n$ est souvent représentée par un tableau à deux lignes :
    $$ \sigma = \begin{pmatrix} 1 & 2 & \dots & k & \dots & n \\ \sigma(1) & \sigma(2) & \dots & \sigma(k) & \dots & \sigma(n) \end{pmatrix} $$
    La première ligne liste les éléments dans l'ordre naturel, la seconde ligne leurs images par $\sigma$.
</div>

<div class="example">
    Pour $n=3$, $\mathfrak{S}_3$ contient $3! = 6$ éléments. Par exemple :
    <ul>
        <li>L'identité : $id = \begin{pmatrix} 1 & 2 & 3 \\ 1 & 2 & 3 \end{pmatrix}$</li>
        <li>Une transposition : $\tau = \begin{pmatrix} 1 & 2 & 3 \\ 1 & 3 & 2 \end{pmatrix}$ (échange 2 et 3)</li>
        <li>Un cycle de longueur 3 : $c = \begin{pmatrix} 1 & 2 & 3 \\ 2 & 3 & 1 \end{pmatrix}$ ($1 \mapsto 2 \mapsto 3 \mapsto 1$)</li>
    </ul>
</div>

<div class="definition">
    <strong>Cycles et Transpositions</strong><br>
    <ul>
        <li>Une permutation $\sigma \in \mathfrak{S}_n$ est appelée un <strong>cycle de longueur $k$</strong> (ou $k$-cycle) s'il existe $k$ éléments distincts $a_1, \dots, a_k \in \{1, \dots, n\}$ (avec $k \ge 2$) tels que :
            $$ \sigma(a_1) = a_2, \sigma(a_2) = a_3, \dots, \sigma(a_{k-1}) = a_k, \sigma(a_k) = a_1 $$
            et $\sigma(x) = x$ pour tout $x \notin \{a_1, \dots, a_k\}$.
            L'ensemble $\{a_1, \dots, a_k\}$ est appelé le <strong>support</strong> du cycle. On note ce cycle $\sigma = (a_1 \ a_2 \ \dots \ a_k)$.</li>
        <li>Une <strong>transposition</strong> est un cycle de longueur 2, c'est-à-dire une permutation qui échange deux éléments et laisse les autres fixes. Elle est notée $(i \ j)$.</li>
    </ul>
    L'identité est parfois considérée comme un cycle de longueur 1.
</div>

<div class="example">
    Dans $\mathfrak{S}_5$:
    <ul>
        <li>$\sigma = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 3 & 2 & 4 & 1 & 5 \end{pmatrix}$ est le cycle $(1 \ 3 \ 4)$. Son support est $\{1, 3, 4\}$. Sa longueur est 3.</li>
        <li>$\tau = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 1 & 5 & 3 & 4 & 2 \end{pmatrix}$ est la transposition $(2 \ 5)$.</li>
    </ul>
</div>

<div class="theorem">
    <strong>Décomposition en produit de cycles à supports disjoints</strong><br>
    Toute permutation $\sigma \in \mathfrak{S}_n$ (avec $n \ge 2$, et $\sigma \neq id$) se décompose en un produit de cycles à supports disjoints.
    $$ \sigma = c_1 \circ c_2 \circ \dots \circ c_r $$
    Cette décomposition est <strong>unique</strong> à l'ordre près des cycles $c_i$. (Les cycles à supports disjoints commutent).
    <span class="video-link">(Démonstration en vidéo)</span>
</div>

<div class="example">
    Soit $\sigma = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ 3 & 6 & 4 & 1 & 5 & 2 & 7 \end{pmatrix} \in \mathfrak{S}_7$.
    <ul>
        <li>$1 \mapsto 3 \mapsto 4 \mapsto 1$. C'est le cycle $c_1 = (1 \ 3 \ 4)$.</li>
        <li>Parmi les éléments restants $\{2, 5, 6, 7\}$, prenons 2 : $2 \mapsto 6 \mapsto 2$. C'est le cycle $c_2 = (2 \ 6)$.</li>
        <li>Il reste $\{5, 7\}$. $5 \mapsto 5$ et $7 \mapsto 7$. Ce sont des points fixes (cycles de longueur 1).</li>
    </ul>
    La décomposition est $\sigma = (1 \ 3 \ 4) \circ (2 \ 6)$. Les supports $\{1, 3, 4\}$ et $\{2, 6\}$ sont disjoints.
</div>

<div class="theorem">
    <strong>Décomposition en produit de transpositions</strong><br>
    Toute permutation $\sigma \in \mathfrak{S}_n$ (avec $n \ge 2$) peut s'écrire comme un produit de transpositions.
    $$ \sigma = \tau_1 \circ \tau_2 \circ \dots \circ \tau_k $$
    Cette décomposition n'est <strong>pas unique</strong> (ni en nombre de transpositions, ni en les transpositions elles-mêmes). Cependant, la parité du nombre $k$ de transpositions est unique (voir Signature).
</div>

<div class="proof">
    <strong>Idée de la preuve :</strong> Il suffit de montrer qu'un cycle peut se décomposer en transpositions. En effet, $(a_1 \ a_2 \ \dots \ a_k) = (a_1 \ a_k) \circ (a_1 \ a_{k-1}) \circ \dots \circ (a_1 \ a_2)$. Comme toute permutation est produit de cycles (disjoints), elle est donc produit de transpositions.
</div>

<div class="example">
    Le cycle $(1 \ 3 \ 4) \in \mathfrak{S}_4$ peut s'écrire $(1 \ 4) \circ (1 \ 3)$.
    La permutation $\sigma = (1 \ 3 \ 4) \circ (2 \ 6)$ de l'exemple précédent peut s'écrire $\sigma = (1 \ 4) \circ (1 \ 3) \circ (2 \ 6)$. C'est un produit de 3 transpositions.
</div>

<h2>II. Signature d'une permutation</h2>

<div class="remark">
    Dans ce paragraphe, on suppose $n \ge 2$.
</div>

<div class="definition">
    <strong>Inversion et Signature</strong><br>
    Soit $\sigma \in \mathfrak{S}_n$.
    <ul>
        <li>On appelle <strong>inversion</strong> de $\sigma$ toute paire d'entiers $\{i, j\}$ telle que $1 \le i < j \le n$ et $\sigma(i) > \sigma(j)$.</li>
        <li>On note $N(\sigma)$ le nombre total d'inversions de $\sigma$.</li>
        <li>La <strong>signature</strong> de $\sigma$ est le nombre $\varepsilon(\sigma)$ défini par :
            $$ \varepsilon(\sigma) = (-1)^{N(\sigma)} $$
    </ul>
    Une permutation est dite <strong>paire</strong> si $\varepsilon(\sigma) = +1$, et <strong>impaire</strong> si $\varepsilon(\sigma) = -1$.
</div>

<div class="example">
    Soit $\sigma = \begin{pmatrix} 1 & 2 & 3 \\ 3 & 1 & 2 \end{pmatrix} \in \mathfrak{S}_3$.
    Les paires $(i, j)$ avec $i < j$ sont $(1, 2)$, $(1, 3)$, $(2, 3)$.
    <ul>
        <li>$i=1, j=2$: $\sigma(1)=3$, $\sigma(2)=1$. On a $\sigma(1) > \sigma(2)$. C'est une inversion.</li>
        <li>$i=1, j=3$: $\sigma(1)=3$, $\sigma(3)=2$. On a $\sigma(1) > \sigma(3)$. C'est une inversion.</li>
        <li>$i=2, j=3$: $\sigma(2)=1$, $\sigma(3)=2$. On a $\sigma(2) < \sigma(3)$. Ce n'est pas une inversion.</li>
    </ul>
    Il y a $N(\sigma)=2$ inversions. La signature est $\varepsilon(\sigma) = (-1)^2 = +1$. $\sigma$ est paire. (C'est le cycle $(1 \ 3 \ 2)$).
</div>

<div class="proposition">
    <strong>Formule alternative pour la signature</strong><br>
    Pour tout $\sigma \in \mathfrak{S}_n$, on a :
    $$ \varepsilon(\sigma) = \prod_{1 \le i < j \le n} \frac{\sigma(j) - \sigma(i)}{j - i} $$
    (Note : Le produit contient autant de termes $-1$ que d'inversions, les autres termes étant $+1$).
</div>

<div class="theorem">
    <strong>Propriétés fondamentales de la signature</strong><br>
    L'application signature $\varepsilon: (\mathfrak{S}_n, \circ) \to (\{-1, 1\}, \times)$ est un morphisme de groupes. C'est-à-dire :
    <ol>
        <li>Pour toutes permutations $\sigma, \sigma' \in \mathfrak{S}_n$ :
           $$ \varepsilon(\sigma \circ \sigma') = \varepsilon(\sigma) \times \varepsilon(\sigma') $$</li>
        <li>Si $\tau$ est une transposition, alors $\varepsilon(\tau) = -1$.</li>
    </ol>
    De plus, $\varepsilon$ est la seule application de $\mathfrak{S}_n$ dans $\{-1, 1\}$ vérifiant ces deux propriétés.
</div>

<div class="proof">
    <strong>Idée de la preuve :</strong>
    1. La propriété de morphisme peut se prouver en utilisant la formule avec le produit, ou par des considérations sur les inversions.
    2. Pour une transposition $\tau = (a \ b)$ avec $a < b$, on compte les inversions : ce sont les paires $\{a, k\}$ avec $a < k < b$ et $\tau(a) > \tau(k)$, les paires $\{k, b\}$ avec $a < k < b$ et $\tau(k) > \tau(b)$, et la paire $\{a, b\}$ où $\tau(a)=b > \tau(b)=a$. Le nombre total d'inversions est $2(b-a-1) + 1$, qui est impair. Donc $\varepsilon(\tau) = -1$.
    L'unicité découle du fait que les transpositions engendrent $\mathfrak{S}_n$.
</div>

<div class="corollary">
    <strong>Signature d'un cycle</strong><br>
    Si $\sigma$ est un cycle de longueur $k$ (un $k$-cycle), alors sa signature est :
    $$ \varepsilon(\sigma) = (-1)^{k-1} $$
</div>

<div class="proof">
    <strong>Preuve :</strong> Un $k$-cycle $(a_1 \ \dots \ a_k)$ peut s'écrire comme un produit de $k-1$ transpositions : $(a_1 \ a_k) \circ \dots \circ (a_1 \ a_2)$.
    En utilisant le théorème précédent : $\varepsilon(\sigma) = \varepsilon((a_1 \ a_k)) \times \dots \times \varepsilon((a_1 \ a_2)) = (-1)^{k-1}$.
</div>

<div class="corollary">
    <strong>Parité du nombre de transpositions</strong><br>
    Si une permutation $\sigma$ peut s'écrire comme un produit de $k$ transpositions et aussi comme un produit de $k'$ transpositions, alors $k$ et $k'$ ont la même parité. Cette parité est donnée par la signature : $\sigma$ est paire si $k$ est pair, impaire si $k$ est impair.
</div>

<h2>III. Applications multilinéaires et formes $n$-linéaires alternées</h2>

<div class="remark">
    Dans cette section, $E, E_1, \dots, E_n, F$ sont des $\mathbb{K}$-espaces vectoriels.
</div>

<div class="definition">
    <strong>Application multilinéaire</strong><br>
    Une application $f: E_1 \times \dots \times E_n \to F$ est dite <strong>multilinéaire</strong> (ou $n$-linéaire) si elle est linéaire par rapport à chacune de ses variables. C'est-à-dire, pour tout $i \in \{1, \dots, n\}$ et pour tous vecteurs $u_1, \dots, u_{i-1}, u_{i+1}, \dots, u_n$ fixés, l'application partielle :
    $$ x \in E_i \mapsto f(u_1, \dots, u_{i-1}, x, u_{i+1}, \dots, u_n) \in F $$
    est une application linéaire de $E_i$ dans $F$.
</div>

<div class="definition">
    <strong>Forme multilinéaire</strong><br>
    Une <strong>forme multilinéaire</strong> (ou forme $n$-linéaire) sur $E_1 \times \dots \times E_n$ est une application multilinéaire à valeurs dans le corps $\mathbb{K}$ (c'est-à-dire $F=\mathbb{K}$). Si $E_1=\dots=E_n=E$, on parle de forme $n$-linéaire sur $E$.
</div>

<div class="definition">
    <strong>Forme $n$-linéaire alternée</strong><br>
    Une forme $n$-linéaire $f: E^n \to \mathbb{K}$ sur $E$ est dite <strong>alternée</strong> si elle s'annule dès que deux de ses arguments sont égaux :
    $$ \forall i \neq j, \quad u_i = u_j \implies f(u_1, \dots, u_n) = 0 $$
</div>

<div class="proposition">
    <strong>Propriété d'antisymétrie</strong><br>
    Si $f$ est une forme $n$-linéaire alternée sur $E$, alors elle est <strong>antisymétrique</strong>, c'est-à-dire que l'échange de deux arguments multiplie la valeur par $-1$. Pour $i < j$ :
    $$ f(u_1, \dots, u_i, \dots, u_j, \dots, u_n) = - f(u_1, \dots, u_j, \dots, u_i, \dots, u_n) $$
</div>

<div class="proof">
    <strong>Preuve :</strong> Considérons $f(\dots, u_i+u_j, \dots, u_i+u_j, \dots)$. Comme $f$ est alternée, cette valeur est 0.
    Par bilinéarité (en positions $i$ et $j$), on a :
    $0 = f(\dots, u_i, \dots, u_i, \dots) + f(\dots, u_i, \dots, u_j, \dots) + f(\dots, u_j, \dots, u_i, \dots) + f(\dots, u_j, \dots, u_j, \dots)$.
    Les premier et dernier termes sont nuls car $f$ est alternée. Il reste :
    $0 = f(\dots, u_i, \dots, u_j, \dots) + f(\dots, u_j, \dots, u_i, \dots)$, d'où le résultat.
</div>

<div class="proposition">
    <strong>Effet d'une permutation sur une forme alternée</strong><br>
    Soit $f$ une forme $n$-linéaire alternée sur $E$. Pour toute permutation $\sigma \in \mathfrak{S}_n$ et tous vecteurs $u_1, \dots, u_n \in E$, on a :
    $$ f(u_{\sigma(1)}, \dots, u_{\sigma(n)}) = \varepsilon(\sigma) f(u_1, \dots, u_n) $$
</div>

<div class="proof">
    <strong>Idée de la preuve :</strong> On sait que toute permutation $\sigma$ est un produit de $k$ transpositions, et $\varepsilon(\sigma) = (-1)^k$. Chaque transposition échange deux arguments, ce qui multiplie la valeur de $f$ par $-1$. Appliquer $k$ transpositions multiplie donc la valeur par $(-1)^k = \varepsilon(\sigma)$.
</div>

<div class="proposition">
    <strong>Formes alternées et familles liées</strong><br>
    Si $f$ est une forme $n$-linéaire alternée sur $E$ et si la famille de vecteurs $(u_1, \dots, u_n)$ est liée, alors $f(u_1, \dots, u_n) = 0$.
</div>

<div class="proof">
    <strong>Preuve :</strong> Si la famille est liée, l'un des vecteurs, disons $u_k$, est combinaison linéaire des autres : $u_k = \sum_{j \neq k} \lambda_j u_j$.
    Par linéarité par rapport à la $k$-ième variable :
    $f(u_1, \dots, u_n) = f(u_1, \dots, \sum_{j \neq k} \lambda_j u_j, \dots, u_n) = \sum_{j \neq k} \lambda_j f(u_1, \dots, u_j \text{ (en pos k)}, \dots, u_n)$.
    Dans chaque terme $f(u_1, \dots, u_j \text{ (en pos k)}, \dots, u_n)$, le vecteur $u_j$ apparaît deux fois (en position $j$ et en position $k$). Comme $f$ est alternée, chacun de ces termes est nul. Donc $f(u_1, \dots, u_n) = 0$.
</div>

<h2>IV. Déterminant dans une base</h2>

<div class="remark">
    Désormais, $E$ désigne un $\mathbb{K}$-espace vectoriel de <strong>dimension finie $n$</strong>.
</div>

<div class="theorem">
    <strong>Existence et Unicité du Déterminant dans une base</strong><br>
    Soit $\mathcal{B} = (e_1, \dots, e_n)$ une base de $E$. Il existe une et une seule forme $n$-linéaire alternée $f$ sur $E$ telle que $f(e_1, \dots, e_n) = 1$.
    Cette forme unique est appelée <strong>déterminant dans la base $\mathcal{B}$</strong> et est notée $\det_{\mathcal{B}}$.

    De plus, l'ensemble des formes $n$-linéaires alternées sur $E$ est un $\mathbb{K}$-espace vectoriel de dimension 1. Toute forme $n$-linéaire alternée $g$ sur $E$ est proportionnelle à $\det_{\mathcal{B}}$ : il existe un unique scalaire $\lambda \in \mathbb{K}$ tel que $g = \lambda \det_{\mathcal{B}}$. (On a $\lambda = g(e_1, \dots, e_n)$).
    <span class="video-link">(Démonstration des résultats suivants en vidéo)</span>
</div>

<div class="proposition">
    <strong>Expression du déterminant en coordonnées</strong><br>
    Soit $\mathcal{B} = (e_1, \dots, e_n)$ une base de $E$. Soit $(x_1, \dots, x_n)$ une famille de $n$ vecteurs de $E$. Pour chaque $j \in \{1, \dots, n\}$, on décompose $x_j$ dans la base $\mathcal{B}$ :
    $$ x_j = \sum_{i=1}^n a_{i,j} e_i $$
    Alors le déterminant de la famille $(x_1, \dots, x_n)$ dans la base $\mathcal{B}$ est donné par :
    $$ \det_{\mathcal{B}}(x_1, \dots, x_n) = \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) a_{\sigma(1), 1} a_{\sigma(2), 2} \dots a_{\sigma(n), n} $$
</div>

<div class="proof">
    <strong>Idée de la preuve :</strong>
    On utilise la $n$-linéarité de $\det_{\mathcal{B}}$ :
    $\det_{\mathcal{B}}(x_1, \dots, x_n) = \det_{\mathcal{B}}(\sum_{i_1=1}^n a_{i_1,1} e_{i_1}, \dots, \sum_{i_n=1}^n a_{i_n,n} e_{i_n})$
    $= \sum_{i_1, \dots, i_n} a_{i_1, 1} \dots a_{i_n, n} \det_{\mathcal{B}}(e_{i_1}, \dots, e_{i_n})$.
    Comme $\det_{\mathcal{B}}$ est alternée, $\det_{\mathcal{B}}(e_{i_1}, \dots, e_{i_n})$ est nul si deux indices sont égaux. Il ne reste que les termes où $(i_1, \dots, i_n)$ est une permutation $\sigma$ de $(1, \dots, n)$.
    Dans ce cas, $\det_{\mathcal{B}}(e_{\sigma(1)}, \dots, e_{\sigma(n)}) = \varepsilon(\sigma) \det_{\mathcal{B}}(e_1, \dots, e_n) = \varepsilon(\sigma)$.
    D'où la formule.
</div>

<div class="example">
    <strong>Cas $n=2$:</strong> Soit $\mathcal{B}=(e_1, e_2)$. Soit $u = x_1 e_1 + y_1 e_2$ et $v = x_2 e_1 + y_2 e_2$.
    La matrice des coordonnées est $A = \begin{pmatrix} x_1 & x_2 \\ y_1 & y_2 \end{pmatrix}$. Les coefficients sont $a_{1,1}=x_1, a_{2,1}=y_1, a_{1,2}=x_2, a_{2,2}=y_2$.
    Les permutations dans $\mathfrak{S}_2$ sont $id = \begin{pmatrix} 1 & 2 \\ 1 & 2 \end{pmatrix}$ ($\varepsilon(id)=1$) et $\sigma = \begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix}$ ($\varepsilon(\sigma)=-1$).
    La formule donne :
    $\det_{\mathcal{B}}(u, v) = \varepsilon(id) a_{id(1), 1} a_{id(2), 2} + \varepsilon(\sigma) a_{\sigma(1), 1} a_{\sigma(2), 2}$
    $= (+1) a_{1,1} a_{2,2} + (-1) a_{2,1} a_{1,2}$
    $= a_{1,1} a_{2,2} - a_{2,1} a_{1,2} = x_1 y_2 - y_1 x_2$.
    C'est le déterminant bien connu de la matrice $\begin{pmatrix} x_1 & x_2 \\ y_1 & y_2 \end{pmatrix}$.
    <span class="video-link">(Interprétation géométrique du déterminant de deux vecteurs - aire orientée)</span>
</div>

<div class="proposition">
    <strong>Changement de base pour le déterminant de vecteurs</strong><br>
    Soient $\mathcal{B}_1$ et $\mathcal{B}_2$ deux bases de $E$. Alors pour toute famille de vecteurs $(x_1, \dots, x_n)$ de $E$, on a :
    $$ \det_{\mathcal{B}_2}(x_1, \dots, x_n) = \det_{\mathcal{B}_2}(\mathcal{B}_1) \times \det_{\mathcal{B}_1}(x_1, \dots, x_n) $$
    où $\det_{\mathcal{B}_2}(\mathcal{B}_1)$ désigne $\det_{\mathcal{B}_2}(e_1, \dots, e_n)$ si $\mathcal{B}_1=(e_1, \dots, e_n)$.
    (Note: $\det_{\mathcal{B}_2}(\mathcal{B}_1)$ est le déterminant de la matrice de passage $P_{\mathcal{B}_2 \to \mathcal{B}_1}$).
</div>

<div class="corollary">
    <strong>Caractérisation des bases</strong><br>
    Une famille $(x_1, \dots, x_n)$ de $n$ vecteurs de $E$ est une base de $E$ si et seulement si son déterminant dans une base quelconque $\mathcal{B}$ est non nul :
    $$ (x_1, \dots, x_n) \text{ base de } E \iff \det_{\mathcal{B}}(x_1, \dots, x_n) \neq 0 $$
</div>

<div class="proof">
    <strong>Preuve :</strong> $(x_1, \dots, x_n)$ est une base ssi elle est libre (car $\dim E = n$). D'après une proposition précédente, si la famille est liée, son déterminant est nul. Réciproquement, si $\det_{\mathcal{B}}(x_1, \dots, x_n) = 0$, supposons par l'absurde que la famille est libre (donc une base). Alors la forme $\det_{(x_1, \dots, x_n)}$ existe et $\det_{\mathcal{B}} = \lambda \det_{(x_1, \dots, x_n)}$ avec $\lambda = \det_{\mathcal{B}}(x_1, \dots, x_n) = 0$. Donc $\det_{\mathcal{B}}$ serait la forme nulle, ce qui est absurde car $\det_{\mathcal{B}}(\mathcal{B}) = 1$. Donc la famille est liée.
</div>

<h2>V. Déterminant d'un endomorphisme</h2>

<div class="theorem">
    <strong>Définition du déterminant d'un endomorphisme</strong><br>
    Soit $f \in \mathcal{L}(E)$ un endomorphisme de $E$. Soit $\mathcal{B} = (e_1, \dots, e_n)$ une base de $E$. Le scalaire $\det_{\mathcal{B}}(f(e_1), \dots, f(e_n))$ ne dépend pas du choix de la base $\mathcal{B}$.
    Cette valeur commune est appelée le <strong>déterminant de l'endomorphisme $f$</strong> et est notée $\det(f)$.
    $$ \det(f) = \det_{\mathcal{B}}(f(e_1), \dots, f(e_n)) \quad (\text{pour n'importe quelle base } \mathcal{B}) $$
</div>

<div class="proof">
    <strong>Idée de la preuve :</strong> Soient $\mathcal{B}_1$ et $\mathcal{B}_2$ deux bases. On veut montrer $\det_{\mathcal{B}_1}(f(\mathcal{B}_1)) = \det_{\mathcal{B}_2}(f(\mathcal{B}_2))$.
    On utilise la formule de changement de base : $\det_{\mathcal{B}_2}(f(\mathcal{B}_1)) = \det_{\mathcal{B}_2}(\mathcal{B}_1) \det_{\mathcal{B}_1}(f(\mathcal{B}_1))$.
    On définit $g: E^n \to \mathbb{K}$ par $g(x_1, \dots, x_n) = \det_{\mathcal{B}_2}(f(x_1), \dots, f(x_n))$. $g$ est une forme $n$-linéaire alternée (car $f$ est linéaire et $\det_{\mathcal{B}_2}$ est $n$-linéaire alternée).
    Donc $g = \lambda \det_{\mathcal{B}_1}$, où $\lambda = g(\mathcal{B}_1) = \det_{\mathcal{B}_2}(f(\mathcal{B}_1))$.
    Ainsi, $\det_{\mathcal{B}_2}(f(x_1, \dots, x_n)) = \det_{\mathcal{B}_2}(f(\mathcal{B}_1)) \det_{\mathcal{B}_1}(x_1, \dots, x_n)$.
    En appliquant avec $(x_1, \dots, x_n) = \mathcal{B}_2$, on obtient :
    $\det_{\mathcal{B}_2}(f(\mathcal{B}_2)) = \det_{\mathcal{B}_2}(f(\mathcal{B}_1)) \det_{\mathcal{B}_1}(\mathcal{B}_2)$.
    Or, on sait aussi que $\det_{\mathcal{B}_2}(\mathcal{B}_1) \det_{\mathcal{B}_1}(\mathcal{B}_2) = \det_{\mathcal{B}_2}(\mathcal{B}_1 \text{ via } \mathcal{B}_1) = \det_{\mathcal{B}_2}(\mathcal{B}_2) = 1$.
    Donc $\det_{\mathcal{B}_1}(\mathcal{B}_2) = (\det_{\mathcal{B}_2}(\mathcal{B}_1))^{-1}$.
    En substituant dans l'équation précédente : $\det_{\mathcal{B}_2}(f(\mathcal{B}_2)) = \det_{\mathcal{B}_2}(f(\mathcal{B}_1)) (\det_{\mathcal{B}_2}(\mathcal{B}_1))^{-1}$.
    Cela ne semble pas donner le résultat directement. Revoyons l'approche.
    Soit $A = \text{Mat}_{\mathcal{B}_1}(f)$ et $A' = \text{Mat}_{\mathcal{B}_2}(f)$. Soit $P = P_{\mathcal{B}_1 \to \mathcal{B}_2}$. On a $A' = P^{-1}AP$. On verra que $\det(A') = \det(A)$.
    $\det_{\mathcal{B}_1}(f(\mathcal{B}_1)) = \det(A)$ et $\det_{\mathcal{B}_2}(f(\mathcal{B}_2)) = \det(A')$. Il faut définir $\det(A)$.
</div>

<div class="proposition">
    <strong>Propriétés du déterminant d'endomorphismes</strong><br>
    Soient $f, g \in \mathcal{L}(E)$, $\lambda \in \mathbb{K}$.
    <ul>
        <li>$\det(f \circ g) = \det(f) \det(g)$</li>
        <li>$\det(\text{Id}_E) = 1$</li>
        <li>$f$ est un automorphisme de $E$ (c-à-d $f \in GL(E)$) si et seulement si $\det(f) \neq 0$.
           Dans ce cas, $\det(f^{-1}) = (\det f)^{-1}$.</li>
        <li>$\det(\lambda f) = \lambda^n \det(f)$</li>
    </ul>
</div>

<h2>VI. Déterminant d'une matrice carrée</h2>

<div class="definition">
    <strong>Déterminant d'une matrice carrée</strong><br>
    Soit $A = (a_{i,j}) \in \mathcal{M}_n(\mathbb{K})$. On appelle <strong>déterminant</strong> de $A$, noté $\det(A)$ ou $|A|$, le déterminant de la famille de ses vecteurs colonnes $(C_1, \dots, C_n)$ dans la base canonique $\mathcal{B}_c$ de $\mathbb{K}^n$.
    $$ \det(A) = \det_{\mathcal{B}_c}(C_1, \dots, C_n) $$
    D'après la formule en coordonnées, cela donne :
    $$ \det(A) = \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) a_{\sigma(1), 1} a_{\sigma(2), 2} \dots a_{\sigma(n), n} $$
    On note aussi :
    $$ \det(A) = \begin{vmatrix} a_{1,1} & a_{1,2} & \dots & a_{1,n} \\ a_{2,1} & a_{2,2} & \dots & a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2} & \dots & a_{n,n} \end{vmatrix} $$
</div>

<div class="proposition">
    <strong>Lien avec le déterminant d'endomorphisme</strong><br>
    1. Si $f \in \mathcal{L}(E)$ et si $A = \text{Mat}_{\mathcal{B}}(f)$ est la matrice de $f$ dans une base quelconque $\mathcal{B}$ de $E$, alors $\det(f) = \det(A)$.
    2. Si $u_A: \mathbb{K}^n \to \mathbb{K}^n$ est l'endomorphisme canoniquement associé à $A$ ($u_A(X) = AX$), alors $\det(u_A) = \det(A)$.
</div>

<div class="proposition">
    <strong>Propriétés du déterminant matriciel</strong><br>
    Soient $A, B \in \mathcal{M}_n(\mathbb{K})$, $\lambda \in \mathbb{K}$.
    <ul>
        <li>$\det(AB) = \det(A) \det(B)$</li>
        <li>$\det(I_n) = 1$</li>
        <li>$A$ est inversible si et seulement si $\det(A) \neq 0$.
           Dans ce cas, $\det(A^{-1}) = (\det A)^{-1}$.</li>
        <li>$\det(A^T) = \det(A)$ (où $A^T$ est la transposée de $A$).</li>
        <li>$\det(\lambda A) = \lambda^n \det(A)$</li>
        <li>Si $A, A' \in \mathcal{M}_n(\mathbb{K})$ sont semblables ($A' = P^{-1}AP$), alors $\det(A') = \det(A)$.</li>
    </ul>
</div>

<div class="proof">
    <strong>Preuve (sélection) :</strong>
    - $\det(AB) = \det(f_A \circ f_B) = \det(f_A) \det(f_B) = \det(A) \det(B)$.
    - $A$ inversible $\iff f_A$ automorphisme $\iff \det(f_A) \neq 0 \iff \det(A) \neq 0$.
    - $\det(A^T)$: La formule $\sum \varepsilon(\sigma) a_{1,\sigma(1)} \dots a_{n,\sigma(n)}$ (équivalente par $\sigma \mapsto \sigma^{-1}$) montre que $\det(A)$ est aussi le déterminant de ses lignes. $\det(A^T)$ utilise les colonnes de $A^T$, qui sont les lignes de $A$. D'où l'égalité.
    - $\det(P^{-1}AP) = \det(P^{-1})\det(A)\det(P) = (\det P)^{-1} \det(A) \det(P) = \det(A)$.
</div>

<h2>VII. Calcul pratique des déterminants</h2>

<div class="proposition">
    <strong>Effet des opérations élémentaires</strong><br>
    Soit $A \in \mathcal{M}_n(\mathbb{K})$.
    <ul>
        <li><strong>Sur les colonnes :</strong>
            <ul>
                <li>Ajouter à une colonne une combinaison linéaire des autres colonnes ($C_i \leftarrow C_i + \sum_{j \neq i} \lambda_j C_j$) ne change pas le déterminant. (Cas simple : $C_i \leftarrow C_i + \lambda C_j$ pour $j \neq i$).</li>
                <li>Échanger deux colonnes ($C_i \leftrightarrow C_j$) multiplie le déterminant par $-1$.</li>
                <li>Multiplier une colonne par un scalaire $\lambda$ ($C_i \leftarrow \lambda C_i$) multiplie le déterminant par $\lambda$.</li>
            </ul>
        </li>
        <li><strong>Sur les lignes :</strong> Les mêmes propriétés sont valables en remplaçant "colonne(s)" par "ligne(s)" (grâce à $\det(A^T)=\det(A)$).</li>
    </ul>
    En conséquence:
    <ul>
        <li>Si une colonne (ou ligne) est nulle, $\det(A)=0$.</li>
        <li>Si deux colonnes (ou lignes) sont identiques ou proportionnelles, $\det(A)=0$.</li>
        <li>$\det(A)$ est une fonction $n$-linéaire alternée des colonnes de $A$, et aussi des lignes de $A$.</li>
    </ul>
</div>

<div class="remark">
    Ces propriétés sont la base de la méthode du pivot de Gauss pour calculer un déterminant : on transforme la matrice en une matrice triangulaire (plus simple) par des opérations élémentaires, en suivant l'évolution du déterminant.
</div>

<div class="proposition">
    <strong>Déterminants de matrices triangulaires</strong><br>
    Si $A = (a_{i,j})$ est une matrice triangulaire (supérieure ou inférieure), son déterminant est le produit de ses coefficients diagonaux :
    $$ \det(A) = a_{1,1} a_{2,2} \dots a_{n,n} = \prod_{i=1}^n a_{i,i} $$
</div>

<div class="proof">
    <strong>Preuve (cas triangulaire supérieure) :</strong> $a_{i,j}=0$ si $i>j$. Dans la formule $\sum \varepsilon(\sigma) a_{\sigma(1), 1} \dots a_{\sigma(n), n}$, un terme est non nul seulement si $a_{\sigma(i), i} \neq 0$ pour tout $i$. Cela impose $\sigma(i) \le i$ pour tout $i$. La seule permutation vérifiant ceci est l'identité ($\sigma(1)=1 \implies \sigma(2)\le 2$, si $\sigma(2)=1$ impossible, donc $\sigma(2)=2$, etc.). Le seul terme potentiellement non nul est pour $\sigma=id$ (avec $\varepsilon(id)=1$), qui vaut $a_{1,1} \dots a_{n,n}$.
</div>

<div class="proposition">
    <strong>Déterminant par blocs</strong><br>
    Si $A$ est une matrice carrée triangulaire par blocs, par exemple :
    $$ A = \begin{pmatrix} B & D \\ 0 & C \end{pmatrix} \quad \text{ou} \quad A = \begin{pmatrix} B & 0 \\ D & C \end{pmatrix} $$
    où $B$ et $C$ sont des matrices carrées (pas nécessairement de même taille), alors :
    $$ \det(A) = \det(B) \det(C) $$
</div>

<div class="definition">
    <strong>Mineur et Cofacteur</strong><br>
    Soit $A = (a_{i,j}) \in \mathcal{M}_n(\mathbb{K})$. Pour $i, j \in \{1, \dots, n\}$ :
    <ul>
        <li>Le <strong>mineur</strong> d'indice $(i, j)$ est le déterminant de la matrice carrée d'ordre $n-1$ obtenue en supprimant la $i$-ème ligne et la $j$-ème colonne de $A$. On le note $\Delta_{i,j}$.</li>
        <li>Le <strong>cofacteur</strong> d'indice $(i, j)$ est le scalaire $C_{i,j}$ défini par :
            $$ C_{i,j} = (-1)^{i+j} \Delta_{i,j} $$
        </li>
    </ul>
</div>

<div class="theorem">
    <strong>Développement par rapport à une ligne ou une colonne</strong><br>
    Soit $A = (a_{i,j}) \in \mathcal{M}_n(\mathbb{K})$.
    <ul>
        <li><strong>Développement par rapport à la $i$-ème ligne</strong> (pour $i$ fixé) :
           $$ \det(A) = \sum_{j=1}^n a_{i,j} C_{i,j} = \sum_{j=1}^n (-1)^{i+j} a_{i,j} \Delta_{i,j} $$</li>
        <li><strong>Développement par rapport à la $j$-ème colonne</strong> (pour $j$ fixé) :
           $$ \det(A) = \sum_{i=1}^n a_{i,j} C_{i,j} = \sum_{i=1}^n (-1)^{i+j} a_{i,j} \Delta_{i,j} $$</li>
    </ul>
</div>

<div class="example">
    Calcul de $\det(A)$ pour $A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix}$. Développons par rapport à la première ligne ($i=1$) :
    $\det(A) = a_{1,1} C_{1,1} + a_{1,2} C_{1,2} + a_{1,3} C_{1,3}$
    $= 1 \cdot (-1)^{1+1} \begin{vmatrix} 5 & 6 \\ 8 & 9 \end{vmatrix} + 2 \cdot (-1)^{1+2} \begin{vmatrix} 4 & 6 \\ 7 & 9 \end{vmatrix} + 3 \cdot (-1)^{1+3} \begin{vmatrix} 4 & 5 \\ 7 & 8 \end{vmatrix}$
    $= 1 \cdot (5 \times 9 - 8 \times 6) - 2 \cdot (4 \times 9 - 7 \times 6) + 3 \cdot (4 \times 8 - 7 \times 5)$
    $= 1 \cdot (45 - 48) - 2 \cdot (36 - 42) + 3 \cdot (32 - 35)$
    $= 1 \cdot (-3) - 2 \cdot (-6) + 3 \cdot (-3)$
    $= -3 + 12 - 9 = 0$.
    (Le déterminant est nul, ce qui est normal car $C_3 = 2C_2 - C_1$).
</div>

<div class="theorem">
    <strong>Déterminant de Vandermonde</strong><br>
    Soient $a_1, \dots, a_n \in \mathbb{K}$. Le déterminant de la matrice de Vandermonde associée est :
    $$ V(a_1, \dots, a_n) = \begin{vmatrix} 1 & 1 & \dots & 1 \\ a_1 & a_2 & \dots & a_n \\ a_1^2 & a_2^2 & \dots & a_n^2 \\ \vdots & \vdots & \ddots & \vdots \\ a_1^{n-1} & a_2^{n-1} & \dots & a_n^{n-1} \end{vmatrix} = \prod_{1 \le i < j \le n} (a_j - a_i) $$
    <span class="video-link">(Démonstration en vidéo)</span>
</div>

<div class="corollary">
    Le déterminant de Vandermonde est non nul si et seulement si les scalaires $a_1, \dots, a_n$ sont deux à deux distincts.
</div>

<h2>VIII. Formules de Cramer, Comatrice</h2>

<div class="definition">
    <strong>Comatrice</strong><br>
    Soit $A \in \mathcal{M}_n(\mathbb{K})$. La <strong>comatrice</strong> de $A$, notée $\text{Com}(A)$ ou $\text{comat}(A)$, est la matrice dont les coefficients sont les cofacteurs $C_{i,j}$ de $A$:
    $$ \text{Com}(A) = (C_{i,j})_{1 \le i, j \le n} = \begin{pmatrix} C_{1,1} & C_{1,2} & \dots & C_{1,n} \\ C_{2,1} & C_{2,2} & \dots & C_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ C_{n,1} & C_{n,2} & \dots & C_{n,n} \end{pmatrix} $$
</div>

<div class="remark">
    Attention : la comatrice est parfois définie comme la transposée de cette matrice. La formule suivante clarifie la relation.
</div>

<div class="theorem">
    <strong>Formule de la comatrice (ou Formule de Cramer/Laplace)</strong><br>
    Soit $A \in \mathcal{M}_n(\mathbb{K})$. Alors :
    $$ A \, (\text{Com}(A))^T = (\text{Com}(A))^T A = \det(A) \, I_n $$
    où $(\text{Com}(A))^T$ est la transposée de la comatrice, parfois appelée matrice adjointe de $A$.
</div>

<div class="proof">
    <strong>Idée de la preuve :</strong> Le coefficient $(i, k)$ du produit $A (\text{Com}(A))^T$ est $\sum_{j=1}^n a_{i,j} ((\text{Com}(A))^T)_{j,k} = \sum_{j=1}^n a_{i,j} C_{k,j}$.
    - Si $k=i$, $\sum_{j=1}^n a_{i,j} C_{i,j}$ est le développement de $\det(A)$ par rapport à la ligne $i$. Le résultat est $\det(A)$.
    - Si $k \neq i$, $\sum_{j=1}^n a_{i,j} C_{k,j}$ est le développement par rapport à la ligne $k$ du déterminant d'une matrice obtenue en remplaçant la ligne $k$ de $A$ par la ligne $i$. Cette matrice a deux lignes identiques (la ligne $i$), son déterminant est donc 0.
    Ainsi, les coefficients diagonaux de $A (\text{Com}(A))^T$ valent $\det(A)$ et les coefficients hors diagonale sont nuls. D'où $A (\text{Com}(A))^T = \det(A) I_n$. La preuve pour $(\text{Com}(A))^T A$ est similaire en utilisant le développement par colonnes.
</div>

<div class="corollary">
    <strong>Expression de l'inverse</strong><br>
    Si $A \in \mathcal{M}_n(\mathbb{K})$ est inversible (c'est-à-dire $\det(A) \neq 0$), alors son inverse est donné par :
    $$ A^{-1} = \frac{1}{\det(A)} (\text{Com}(A))^T $$
</div>

<div class="corollary">
    <strong>Systèmes de Cramer</strong><br>
    Soit $AX = B$ un système linéaire de $n$ équations à $n$ inconnues, où $A \in GL_n(\mathbb{K})$ (matrice inversible), $X = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}$ le vecteur des inconnues, et $B = \begin{pmatrix} b_1 \\ \vdots \\ b_n \end{pmatrix}$ le second membre.
    L'unique solution $X = A^{-1}B$ a ses composantes données par les <strong>formules de Cramer</strong> :
    $$ x_i = \frac{\det(A_i)}{\det(A)} $$
    où $A_i$ est la matrice obtenue en remplaçant la $i$-ème colonne de $A$ par le vecteur $B$.
</div>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            // customised options
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
        });
    });
</script>

</body>
</html>
