<!DOCTYPE html>

<html lang="fr">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Cours : Compléments d'algèbre linéaire</title>
<script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            tags: 'ams', // Automatic equation numbering
            macros: {
              K: '\\mathbb{K}',
              C: '\\mathbb{C}',
              N: '\\mathbb{N}',
              R: '\\mathbb{R}',
              Mnk: '{\\mathcal{M}_{n,k}(\\K)}',
              Mnm: '{\\mathcal{M}_{n,m}(\\K)}',
              Mpq: '{\\mathcal{M}_{p,q}(\\K)}',
              Mn: '{\\mathcal{M}_{n}(\\K)}',
              Mp: '{\\mathcal{M}_{p}(\\K)}',
              Mq: '{\\mathcal{M}_{q}(\\K)}',
              Mr: '{\\mathcal{M}_{r}(\\K)}',
              LE: '{\\mathcal{L}(E)}',
              LEF: '{\\mathcal{L}(E, F)}',
              Id: '\\text{Id}',
              dim: '\\text{dim}',
              Vect: '\\text{Vect}',
              Ker: '\\text{Ker}',
              Im: '\\text{Im}',
              det: '\\text{det}',
              Tr: '\\text{Tr}'
            }
          },
          svg: {
            fontCache: 'global'
          }
        };
        </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" type="text/javascript">
</script>
<style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f8f9fa; /* Gris très clair */
        }
        h1, h2, h3 {
            color: #0056b3; /* Bleu foncé */
            border-bottom: 2px solid #0056b3;
            padding-bottom: 5px;
            margin-top: 30px;
        }
        h1 { font-size: 2.2em; }
        h2 { font-size: 1.8em; }
        h3 { font-size: 1.4em; font-style: italic; border-bottom: none; color: #007bff;} /* Bleu */

        .definition, .theorem, .proposition, .corollary, .example, .proof, .remark {
            margin-top: 15px;
            margin-bottom: 15px;
            padding: 15px;
            border-left: 5px solid #17a2b8; /* Cyan */
            background-color: #e3f2fd; /* Bleu très clair */
            border-radius: 0 5px 5px 0;
        }
        .definition strong, .theorem strong, .proposition strong, .corollary strong, .remark strong {
            color: #0056b3; /* Bleu foncé */
            font-weight: bold;
        }
        .proof {
            border-left-color: #28a745; /* Vert */
            background-color: #e8f5e9; /* Vert très clair */
        }
        .proof strong {
            color: #155724; /* Vert foncé */
            font-weight: bold;
        }
        .example {
            border-left-color: #ffc107; /* Jaune */
            background-color: #fff8e1; /* Jaune très clair */
        }
         .example strong {
            color: #856404; /* Jaune foncé */
            font-weight: bold;
        }
         .remark {
            border-left-color: #6c757d; /* Gris */
            background-color: #f8f9fa; /* Gris très clair */
        }
         .remark strong {
            color: #343a40; /* Gris foncé */
            font-weight: bold;
        }
        code {
            background-color: #e9ecef; /* Gris clair */
            padding: 2px 5px;
            border-radius: 4px;
            font-family: monospace;
        }
        ul {
            margin-left: 20px;
            list-style-type: square;
            padding-left: 20px;
        }
         ul ul {
             list-style-type: circle;
             margin-top: 5px;
         }
        li {
            margin-bottom: 8px;
        }
        .MJX-TeXAtom-ORD { }
    </style>
</head>
<body>
<h1>Compléments d'algèbre linéaire</h1>
<p>Dans ce chapitre, $\K$ désigne un corps ($\mathbb{Q}$, $\R$ ou $\C$) et $E$ est un $\K$-espace vectoriel.</p>
<h2>1. Somme de sous-espaces vectoriels</h2>
<div class="definition" id="maths02_algèbreLinéaireCompléments-1">
<strong>Définition (Somme de deux sous-espaces)</strong><br/>
        Soient $F$ et $G$ deux sous-espaces vectoriels (sev) de $E$. On appelle <strong>somme</strong> de $F$ et $G$ le sous-espace vectoriel de $E$, noté $F+G$, défini par :
        $$ F+G = \{ x+y \mid x \in F, y \in G \} $$
        C'est le plus petit sev de $E$ contenant $F \cup G$.
    </div>
<div class="definition" id="maths02_algèbreLinéaireCompléments-2">
<strong>Définition (Somme directe de deux sous-espaces)</strong><br/>
        La somme $F+G$ est dite <strong>directe</strong> si tout vecteur $z \in F+G$ s'écrit de manière <strong>unique</strong> comme somme d'un vecteur de $F$ et d'un vecteur de $G$.
        <br/> Autrement dit : $\forall z \in F+G, \exists! (x, y) \in F \times G, z = x+y$.
        <br/> On note alors cette somme $F \oplus G$.
    </div>
<div class="proposition" id="maths02_algèbreLinéaireCompléments-3">
<strong>Proposition (Caractérisation de la somme directe)</strong><br/>
        Soient $F$ et $G$ deux sev de $E$. Les assertions suivantes sont équivalentes :
        <ol>
<li>La somme $F+G$ est directe ($F \oplus G$).</li>
<li>L'intersection de $F$ et $G$ est réduite au vecteur nul : $F \cap G = \{0_E\}$.</li>
<li>Pour tout $x \in F$ et $y \in G$, si $x+y = 0_E$, alors $x=0_E$ et $y=0_E$.</li>
</ol>
</div>
<div class="proof" id="maths02_algèbreLinéaireCompléments-4">
<strong>Preuve :</strong><br/>
        (1 $\implies$ 3) : Soit $z = 0_E \in F \oplus G$. La décomposition $z = x+y$ avec $x \in F, y \in G$ doit être unique. Or, $0_E = 0_E + 0_E$ est une telle décomposition (avec $0_E \in F$ et $0_E \in G$). Par unicité, si $x+y = 0_E$, on doit avoir $x=0_E$ et $y=0_E$.
        <br/>(3 $\implies$ 2) : Soit $z \in F \cap G$. Alors $z \in F$ et $z \in G$. On peut écrire $z + (-z) = 0_E$. Comme $z \in F$ et $-z \in G$ (car $G$ est un sev), l'hypothèse (3) implique $z=0_E$ et $-z=0_E$. Donc $F \cap G = \{0_E\}$.
        <br/>(2 $\implies$ 1) : Supposons $F \cap G = \{0_E\}$. Soit $z \in F+G$. Supposons qu'il existe deux décompositions : $z = x_1 + y_1 = x_2 + y_2$ avec $x_1, x_2 \in F$ et $y_1, y_2 \in G$.
        Alors $x_1 - x_2 = y_2 - y_1$.
        Le membre de gauche $x_1 - x_2$ appartient à $F$ (car $F$ est un sev).
        Le membre de droite $y_2 - y_1$ appartient à $G$ (car $G$ est un sev).
        Donc $x_1 - x_2 \in F \cap G$. Comme $F \cap G = \{0_E\}$, on a $x_1 - x_2 = 0_E$, soit $x_1 = x_2$.
        De même, $y_2 - y_1 = 0_E$, soit $y_1 = y_2$.
        La décomposition est unique, la somme est directe. $\Box$
    </div>
<div class="definition" id="maths02_algèbreLinéaireCompléments-5">
<strong>Définition (Sous-espaces supplémentaires)</strong><br/>
        Deux sous-espaces vectoriels $F$ et $G$ de $E$ sont dits <strong>supplémentaires</strong> dans $E$ si leur somme est directe et égale à $E$ tout entier :
        $$ F \oplus G = E $$
        Cela équivaut à dire que tout vecteur $x \in E$ s'écrit de manière unique $x = y+z$ avec $y \in F$ et $z \in G$.
        Cela équivaut aussi à $F \cap G = \{0_E\}$ et $F + G = E$.
    </div>
<div class="remark" id="maths02_algèbreLinéaireCompléments-6">
        En dimension finie, si $F$ et $G$ sont deux sev de $E$ avec $\dim(E) = n$, alors $F$ et $G$ sont supplémentaires si et seulement si $F \cap G = \{0_E\}$ et $\dim(F) + \dim(G) = n$. (Utilise la formule de Grassmann : $\dim(F+G) = \dim(F) + \dim(G) - \dim(F \cap G)$).
    </div>
<div class="definition" id="maths02_algèbreLinéaireCompléments-7">
<strong>Définition (Somme de plusieurs sous-espaces)</strong><br/>
        Soient $F_1, \dots, F_r$ ($r \ge 2$) des sous-espaces vectoriels de $E$. On définit leur <strong>somme</strong> par :
        $$ \sum_{i=1}^r F_i = F_1 + \dots + F_r = \{ x_1 + \dots + x_r \mid x_i \in F_i \text{ pour } i=1, \dots, r \} $$
        C'est le plus petit sev de $E$ contenant $F_1 \cup \dots \cup F_r$.
    </div>
<div class="definition" id="maths02_algèbreLinéaireCompléments-8">
<strong>Définition (Somme directe de plusieurs sous-espaces)</strong><br/>
        La somme $F_1 + \dots + F_r$ est dite <strong>directe</strong> si la décomposition de tout vecteur de la somme est unique. Autrement dit :
        $$ \forall z \in \sum_{i=1}^r F_i, \exists! (x_1, \dots, x_r) \in F_1 \times \dots \times F_r, z = x_1 + \dots + x_r $$
        Ceci est équivalent à la condition suivante :
        $$ \forall (x_1, \dots, x_r) \in F_1 \times \dots \times F_r, \left( \sum_{i=1}^r x_i = 0_E \implies x_1 = x_2 = \dots = x_r = 0_E \right) $$
        On note alors la somme $\bigoplus_{i=1}^r F_i = F_1 \oplus \dots \oplus F_r$.
    </div>
<div class="remark" id="maths02_algèbreLinéaireCompléments-9">
<strong>Attention :</strong> Pour $r \ge 3$, la condition $F_i \cap F_j = \{0_E\}$ pour tous $i \neq j$ n'est <strong>pas suffisante</strong> pour garantir que la somme est directe.
        <br/>
<strong>Exemple :</strong> Dans $E = \R^2$, soient $F_1 = \Vect((1,0))$, $F_2 = \Vect((0,1))$, $F_3 = \Vect((1,1))$.
        On a $F_1 \cap F_2 = \{0\}$, $F_1 \cap F_3 = \{0\}$, $F_2 \cap F_3 = \{0\}$.
        Pourtant, la somme $F_1+F_2+F_3 = \R^2$ n'est pas directe. Par exemple, le vecteur nul $0_E = (0,0)$ a plusieurs décompositions :
        $0_E = 0_{F_1} + 0_{F_2} + 0_{F_3}$.
        Mais aussi $0_E = (1,0) + (0,1) + (-1,-1)$, où $(1,0) \in F_1$, $(0,1) \in F_2$, et $(-1,-1) \in F_3$. Les vecteurs ne sont pas tous nuls.
        <br/>
        Une caractérisation (moins utilisée en pratique) est que la somme $\sum F_i$ est directe si et seulement si pour tout $k \in \{1, \dots, r-1\}$, $ (\sum_{i=1}^k F_i) \cap F_{k+1} = \{0_E\}$.
    </div>
<div class="definition" id="maths02_algèbreLinéaireCompléments-10">
<strong>Définition (Base adaptée à une somme directe)</strong><br/>
        Soient $F_1, \dots, F_r$ des sous-espaces vectoriels de $E$ de dimensions finies. On suppose que leur somme $S = F_1 \oplus \dots \oplus F_r$ est directe.
        Soit $\mathcal{B}_i$ une base de $F_i$ pour chaque $i=1, \dots, r$.
        Alors la famille $\mathcal{B}$ obtenue par <strong>concaténation</strong> des familles $\mathcal{B}_1, \dots, \mathcal{B}_r$ est une base de la somme directe $S = \bigoplus_{i=1}^r F_i$.
        On appelle $\mathcal{B}$ une <strong>base adaptée</strong> à la décomposition en somme directe.
        <br/> En particulier, si $E = F_1 \oplus \dots \oplus F_r$, alors $\mathcal{B}$ est une base de $E$.
    </div>
<div class="proof" id="maths02_algèbreLinéaireCompléments-11">
<strong>Preuve (Idée) :</strong><br/>
        Soit $S = \bigoplus_{i=1}^r F_i$ et $\mathcal{B} = (\mathcal{B}_1, \dots, \mathcal{B}_r)$.
        - <strong>Famille génératrice de S :</strong> Tout $x \in S$ s'écrit $x = x_1 + \dots + x_r$ avec $x_i \in F_i$. Chaque $x_i$ est une combinaison linéaire des vecteurs de $\mathcal{B}_i$. Donc $x$ est une combinaison linéaire des vecteurs de $\mathcal{B}$. $\mathcal{B}$ engendre $S$.
        - <strong>Famille libre :</strong> Supposons une combinaison linéaire nulle des vecteurs de $\mathcal{B}$ :
        $$ \sum_{i=1}^r \underbrace{\left( \sum_{b \in \mathcal{B}_i} \lambda_b b \right)}_{= x_i \in F_i} = 0_E $$
        Comme la somme des $F_i$ est directe, cette égalité implique que chaque terme $x_i = \sum_{b \in \mathcal{B}_i} \lambda_b b$ est nul.
        Or, $\mathcal{B}_i$ est une base de $F_i$, donc c'est une famille libre. La combinaison linéaire nulle $x_i=0_E$ implique que tous les coefficients $\lambda_b$ correspondants (pour $b \in \mathcal{B}_i$) sont nuls.
        Ceci étant vrai pour chaque $i$, tous les coefficients de la combinaison linéaire initiale sont nuls. La famille $\mathcal{B}$ est libre.
        $\mathcal{B}$ est une famille libre et génératrice de $S$, c'est donc une base de $S$. $\Box$
    </div>
<div class="theorem" id="maths02_algèbreLinéaireCompléments-12">
<strong>Théorème (Dimension d'une somme de sous-espaces)</strong><br/>
        Si $F_1, \dots, F_r$ sont des sous-espaces vectoriels de dimension finie de $E$, alors leur somme $S = \sum_{i=1}^r F_i$ est de dimension finie et :
        $$ \dim\left( \sum_{i=1}^r F_i \right) \le \sum_{i=1}^r \dim(F_i) $$
        Il y a égalité si et seulement si la somme est directe :
        $$ \dim\left( \bigoplus_{i=1}^r F_i \right) = \sum_{i=1}^r \dim(F_i) $$
    </div>
<div class="remark" id="maths02_algèbreLinéaireCompléments-13">
        Pour $r=2$, ce théorème est cohérent avec la formule de Grassmann : $\dim(F_1+F_2) = \dim(F_1) + \dim(F_2) - \dim(F_1 \cap F_2)$. L'égalité $\dim(F_1+F_2) = \dim(F_1) + \dim(F_2)$ a lieu si et seulement si $\dim(F_1 \cap F_2) = 0$, c'est-à-dire $F_1 \cap F_2 = \{0_E\}$, ce qui caractérise la somme directe.
    </div>
<div class="definition" id="maths02_algèbreLinéaireCompléments-14">
<strong>Définition (Projecteurs associés à une somme directe)</strong><br/>
        Soient $F_1, \dots, F_r$ des sous-espaces vectoriels de $E$ tels que $E = F_1 \oplus \dots \oplus F_r$.
        Pour chaque $i \in \{1, \dots, r\}$, on peut définir l'application $p_i : E \to E$ qui à un vecteur $x \in E$ associe son unique composante $x_i$ dans $F_i$ selon la décomposition $x = x_1 + \dots + x_r$ ($x_k \in F_k$).
        L'application $p_i$ est un endomorphisme de $E$. C'est le <strong>projecteur</strong> sur $F_i$ parallèlement à la somme directe des autres sous-espaces $G_i = \bigoplus_{j \neq i} F_j$.
        La famille $(p_1, \dots, p_r)$ est appelée la <strong>famille de projecteurs</strong> associée à la décomposition en somme directe $E = F_1 \oplus \dots \oplus F_r$.
    </div>
<div class="proposition" id="maths02_algèbreLinéaireCompléments-15">
<strong>Propriétés des projecteurs associés :</strong><br/>
        La famille de projecteurs $(p_1, \dots, p_r)$ associée à $E = \bigoplus_{i=1}^r F_i$ vérifie :
        <ol>
<li>Chaque $p_i$ est un projecteur : $p_i \circ p_i = p_i$.</li>
<li>$\sum_{i=1}^r p_i = \Id_E$.</li>
<li>$p_i \circ p_j = 0$ (l'endomorphisme nul) si $i \neq j$.</li>
</ol>
        Réciproquement, si $(p_1, \dots, p_r)$ est une famille d'endomorphismes de $E$ vérifiant les conditions 2 et 3, alors chaque $p_i$ est un projecteur, les sous-espaces $F_i = \Im(p_i)$ sont en somme directe et $E = \bigoplus_{i=1}^r F_i$.
    </div>
<div class="proof" id="maths02_algèbreLinéaireCompléments-16">
<strong>Preuve (des propriétés directes) :</strong><br/>
        Soit $x \in E$, décomposé en $x = x_1 + \dots + x_r$ avec $x_k \in F_k$. Par définition, $p_i(x) = x_i$.
        1. $p_i(p_i(x)) = p_i(x_i)$. Comme $x_i \in F_i$, sa décomposition est $x_i = 0 + \dots + x_i + \dots + 0$. Donc $p_i(x_i) = x_i$. Ainsi $p_i(p_i(x)) = p_i(x)$ pour tout $x$, donc $p_i \circ p_i = p_i$.
        2. $\sum_{i=1}^r p_i(x) = \sum_{i=1}^r x_i = x = \Id_E(x)$. Donc $\sum p_i = \Id_E$.
        3. Si $i \neq j$, $p_j(x) = x_j \in F_j$. La décomposition de $x_j$ est $0 + \dots + x_j + \dots + 0$. La composante sur $F_i$ est $0_E$. Donc $p_i(x_j) = 0_E$. Ainsi $p_i(p_j(x)) = 0_E$ pour tout $x$, donc $p_i \circ p_j = 0$. $\Box$
    </div>
<div class="theorem" id="maths02_algèbreLinéaireCompléments-17">
<strong>Théorème (Reconstruction d'applications linéaires)</strong><br/>
        Soient $E, F$ deux $\K$-espaces vectoriels. Soit $E = F_1 \oplus \dots \oplus F_r$ une décomposition de $E$ en somme directe de sous-espaces $F_i$.
        Pour chaque $i \in \{1, \dots, r\}$, soit $u_i \in \mathcal{L}(F_i, F)$ une application linéaire définie sur $F_i$.
        Alors il existe une unique application linéaire $u \in \mathcal{L}(E, F)$ telle que la restriction de $u$ à chaque $F_i$ coïncide avec $u_i$ :
        $$ \forall i \in \{1, \dots, r\}, u|_{F_i} = u_i $$
        Cette application $u$ est définie pour $x = x_1 + \dots + x_r$ (décomposition unique) par $u(x) = u_1(x_1) + \dots + u_r(x_r)$.
    </div>
<div class="remark" id="maths02_algèbreLinéaireCompléments-18">
    Ce théorème est fondamental pour étudier des endomorphismes $u \in \mathcal{L}(E)$ en les "restreignant" à des sous-espaces stables $F_i$ tels que $E = \bigoplus F_i$. Si on connait $u_i = u|_{F_i}$ (qui va de $F_i$ dans $F_i$), on reconstruit $u$.
    </div>
<h2>2. Matrices définies par blocs</h2>
<div class="definition" id="maths02_algèbreLinéaireCompléments-19">
<strong>Définition (Matrices par blocs 2x2)</strong><br/>
        Soient $A \in \mathcal{M}_{n,q}(\K)$, $B \in \mathcal{M}_{p,q}(\K)$, $C \in \mathcal{M}_{n,r}(\K)$ et $D \in \mathcal{M}_{p,r}(\K)$. On peut former une matrice $M \in \mathcal{M}_{n+p, q+r}(\K)$ par blocs :
        $$ M = \begin{pmatrix} A &amp; C \\ B &amp; D \end{pmatrix} $$
        Les lignes $1$ à $n$ et colonnes $1$ à $q$ forment le bloc $A$.
        Les lignes $1$ à $n$ et colonnes $q+1$ à $q+r$ forment le bloc $C$.
        Les lignes $n+1$ à $n+p$ et colonnes $1$ à $q$ forment le bloc $B$.
        Les lignes $n+1$ à $n+p$ et colonnes $q+1$ à $q+r$ forment le bloc $D$.
    </div>
<div class="proposition" id="maths02_algèbreLinéaireCompléments-20">
<strong>Opérations par blocs :</strong><br/>
        On peut effectuer les opérations matricielles (combinaison linéaire, produit, transposition) en traitant les blocs comme des "coefficients", à condition que les tailles des blocs soient compatibles.
        <ul>
<li><strong>Combinaison linéaire :</strong>
               $\lambda \begin{pmatrix} A &amp; C \\ B &amp; D \end{pmatrix} + \mu \begin{pmatrix} A' &amp; C' \\ B' &amp; D' \end{pmatrix} = \begin{pmatrix} \lambda A + \mu A' &amp; \lambda C + \mu C' \\ \lambda B + \mu B' &amp; \lambda D + \mu D' \end{pmatrix}$ (si les blocs correspondants ont même taille).
            </li>
<li><strong>Produit :</strong> Si $M = \begin{pmatrix} A &amp; C \\ B &amp; D \end{pmatrix}$ et $N = \begin{pmatrix} E &amp; G \\ F &amp; H \end{pmatrix}$ avec des tailles de blocs compatibles pour le produit (le nombre de colonnes des blocs de $M$ correspond au nombre de lignes des blocs de $N$), alors :
               $$ MN = \begin{pmatrix} A &amp; C \\ B &amp; D \end{pmatrix} \begin{pmatrix} E &amp; G \\ F &amp; H \end{pmatrix} = \begin{pmatrix} AE + CF &amp; AG + CH \\ BE + DF &amp; BG + DH \end{pmatrix} $$
            </li>
<li><strong>Transposition :</strong>
                $$ M^T = \begin{pmatrix} A &amp; C \\ B &amp; D \end{pmatrix}^T = \begin{pmatrix} A^T &amp; B^T \\ C^T &amp; D^T \end{pmatrix} $$
                (Attention : les blocs sont transposés, et leur position change comme pour la transposition d'une matrice 2x2).
            </li>
</ul>
</div>
<div class="definition" id="maths02_algèbreLinéaireCompléments-21">
<strong>Définition (Matrices par blocs générales)</strong><br/>
        Plus généralement, on peut partitionner les lignes et les colonnes d'une matrice $M$ pour l'écrire sous la forme :
        $$ M = \begin{pmatrix}
        A_{1,1} &amp; A_{1,2} &amp; \dots &amp; A_{1,p} \\
        A_{2,1} &amp; A_{2,2} &amp; \dots &amp; A_{2,p} \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        A_{r,1} &amp; A_{r,2} &amp; \dots &amp; A_{r,p}
        \end{pmatrix} $$
        où chaque $A_{i,j}$ est une matrice (un bloc) de taille $n_i \times q_j$, avec $\sum n_i$ le nombre total de lignes et $\sum q_j$ le nombre total de colonnes de $M$. Les opérations se généralisent.
    </div>
<div class="remark" id="maths02_algèbreLinéaireCompléments-22">
<strong>Lien avec les sommes directes :</strong><br/>
        Les matrices par blocs sont particulièrement utiles en lien avec les bases adaptées.
        Soit $u \in \mathcal{L}(E)$ et une décomposition $E = F_1 \oplus \dots \oplus F_r$. Soit $\mathcal{B} = (\mathcal{B}_1, \dots, \mathcal{B}_r)$ une base adaptée.
        La matrice de $u$ dans la base $\mathcal{B}$ s'écrit naturellement comme une matrice par blocs $M = (A_{i,j})_{1 \le i,j \le r}$, où le bloc $A_{i,j}$ est la matrice de l'application $u_{i,j} : F_j \to F_i$ définie par $u_{i,j} = p_i \circ u|_{F_j}$ dans les bases $\mathcal{B}_j$ (départ) et $\mathcal{B}_i$ (arrivée).
        <ul>
<li>Si tous les $F_i$ sont <strong>stables</strong> par $u$ (i.e., $u(F_i) \subseteq F_i$), alors $u_{i,j} = 0$ pour $i \neq j$. La matrice $M$ est <strong>diagonale par blocs</strong> :
            $$ M = \begin{pmatrix} A_{1,1} &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; A_{2,2} &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; A_{r,r} \end{pmatrix} $$
            où $A_{i,i}$ est la matrice de $u|_{F_i}$ dans la base $\mathcal{B}_i$.
            </li>
</ul>
</div>
<div class="definition" id="maths02_algèbreLinéaireCompléments-23">
<strong>Définition (Matrices triangulaires/diagonales par blocs)</strong><br/>
        Soit $M = (A_{i,j})$ une matrice carrée décomposée en blocs carrés sur la diagonale ($A_{i,i}$ est carrée).
        <ul>
<li>$M$ est <strong>triangulaire supérieure par blocs</strong> si $A_{i,j} = 0$ (la matrice nulle) pour tout $i &gt; j$.
            $$ M = \begin{pmatrix}
            A_{1,1} &amp; A_{1,2} &amp; \dots &amp; A_{1,r} \\
            0 &amp; A_{2,2} &amp; \dots &amp; A_{2,r} \\
            \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
            0 &amp; \dots &amp; 0 &amp; A_{r,r}
            \end{pmatrix} $$
            </li>
<li>$M$ est <strong>triangulaire inférieure par blocs</strong> si $A_{i,j} = 0$ pour tout $i &lt; j$.</li>
<li>$M$ est <strong>diagonale par blocs</strong> si $A_{i,j} = 0$ pour tout $i \neq j$.</li>
</ul>
</div>
<div class="theorem" id="maths02_algèbreLinéaireCompléments-24">
<strong>Théorème (Déterminant d'une matrice triangulaire par blocs)</strong><br/>
        Soit $M$ une matrice carrée écrite par blocs telle que les blocs diagonaux $A_{1,1}, \dots, A_{r,r}$ soient carrés. Si $M$ est triangulaire par blocs (supérieure ou inférieure), alors son déterminant est le produit des déterminants des blocs diagonaux :
        $$ \det(M) = \det(A_{1,1}) \times \det(A_{2,2}) \times \dots \times \det(A_{r,r}) $$
        En particulier, pour une matrice $2 \times 2$ par blocs :
        $$ \det \begin{pmatrix} A &amp; C \\ 0 &amp; D \end{pmatrix} = \det(A) \det(D) \quad \text{et} \quad \det \begin{pmatrix} A &amp; 0 \\ B &amp; D \end{pmatrix} = \det(A) \det(D) $$
        (où $A$ et $D$ sont des blocs carrés).
    </div>
<div class="proof" id="maths02_algèbreLinéaireCompléments-25">
<strong>Preuve (Idée pour le cas 2x2 supérieur) :</strong><br/>
        On peut montrer ce résultat en utilisant la définition du déterminant avec les permutations ou par récurrence en développant par rapport à des colonnes (ou lignes) des blocs nuls. Une autre approche (si $A$ est inversible) utilise la factorisation :
        $$ \begin{pmatrix} A &amp; C \\ 0 &amp; D \end{pmatrix} = \begin{pmatrix} A &amp; 0 \\ 0 &amp; I \end{pmatrix} \begin{pmatrix} I &amp; A^{-1}C \\ 0 &amp; D \end{pmatrix} $$
        Le déterminant de la première matrice est $\det(A)$. Le déterminant de la seconde est $\det(D)$ (peut se voir par développement). Le résultat suit par multiplicativité du déterminant. Le cas où $A$ n'est pas inversible demande un argument de densité ou de travail avec des matrices de cofacteurs. $\Box$
    </div>
<div class="remark" id="maths02_algèbreLinéaireCompléments-26">
<strong>Attention :</strong> En général, pour une matrice $M = \begin{pmatrix} A &amp; C \\ B &amp; D \end{pmatrix}$ avec $A, B, C, D$ carrés de même taille, on n'a <strong>pas</strong> $\det(M) = \det(A)\det(D) - \det(B)\det(C)$.
        <br/> Il existe une formule si $A$ est inversible : $\det(M) = \det(A) \det(D - B A^{-1} C)$.
        Et si $D$ est inversible : $\det(M) = \det(D) \det(A - C D^{-1} B)$.
        Si $A, B, C, D$ commutent deux à deux, alors $\det(M) = \det(AD - BC)$. Mais ces formules sont plus rarement utilisées en Spé.
    </div>
</body>
</html>
